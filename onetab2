https://zhuanlan.zhihu.com/p/261412153 | 求道之人，不问寒暑（六） - 知乎
https://zhuanlan.zhihu.com/p/355523266 | 最简单的self-supervised方法 - 知乎
https://www.google.com/search?q=BYOL&oq=BYOL&aqs=chrome..69i57j69i61&sourceid=chrome&ie=UTF-8 | BYOL - Google Search
https://zhuanlan.zhihu.com/p/150358540 | 自监督黑马SimCLRv2来了！提出蒸馏新思路，可迁移至小模型，性能精度超越有监督 - 知乎
https://zhuanlan.zhihu.com/p/334732028 | 自监督对比学习（Contrastive Learning）综述+代码 - 知乎
https://ai.googleblog.com/2021/06/extending-contrastive-learning-to.html | Extending Contrastive Learning to the Supervised Setting – Google AI Blog
https://zhuanlan.zhihu.com/p/205636123 | 自监督学习BYOL中魔鬼BN - 知乎
https://www.zhihu.com/question/402452508 | (42 封私信 / 80 条消息) 如何评价Deepmind自监督新作BYOL？ - 知乎
https://generallyintelligent.com/blog/2020-08-24-understanding-self-supervised-contrastive-learning/ | Understanding Self-Supervised and Contrastive Learning with "Bootstrap Your Own Latent" (BYOL) - generally intelligent
https://aclanthology.org/2022.coling-1.222.pdf | COPNER Contrastive Learning with Prompt Guiding for Few-shot Named Entity Recognition - ACL-COLING-2022_2022.coling-1.222
https://arxiv.org/pdf/2202.06417.pdf | A Contrastive Framework for Neural Text Generation - Arxiv-2202.06417
https://arxiv.org/pdf/2106.06823.pdf | Prompting Contrastive Explanations for Commonsense Reasoning Tasks - Arxiv-2106.06823
https://arxiv.org/pdf/2204.10298.pdf | DiffCSE Difference-based Contrastive Learning for Sentence Embeddings - Arxiv-2204.10298
https://arxiv.org/pdf/2210.09150.pdf | Prompting GPT-3 To Be Reliable - Arxiv-2210.09150
https://github.com/NoviScl/GPT3-Reliability | NoviScl/GPT3-Reliability
https://arxiv.org/pdf/1806.10348.pdf | Learning Visually-Grounded Semantics from Contrastive Adversarial Samples - Arxiv-1806.10348
https://proceedings.mlr.press/v162/saunshi22a/saunshi22a.pdf | Understanding Contrastive Learning Requires Incorporating Inductive Biases - PMLR-2022-saunshi22a
https://arxiv.org/pdf/2104.08812.pdf | Contrastive Out-of-Distribution Detection for Pretrained Transformers - Arxiv-2104.08812
https://arxiv.org/abs/2210.03162 | Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models - Arxiv-2210.03162
https://proceedings.mlr.press/v162/zhou22l/zhou22l.pdf | Contrastive Learning with Boosted Memorization - PMLR-2022-zhou22l
https://openaccess.thecvf.com/content/CVPR2022/papers/Yao_PCL_Proxy-Based_Contrastive_Learning_for_Domain_Generalization_CVPR_2022_paper.pdf | PCL Proxy-Based Contrastive Learning for Domain Generalization - CVPR-2022_31606908
https://arxiv.org/abs/2204.07596 | Perfectly Balanced Improving Transfer and Robustness of Supervised Contrastive Learning - Arxiv-2204.07596
https://arxiv.org/pdf/2210.15097.pdf | Contrastive Decoding Open-ended Text Generation as Optimization - Arxiv-2210.15097
https://arxiv.org/pdf/2111.00899.pdf | Equivariant Contrastive Learning - Arxiv-2111.00899
https://www.google.com/search?q=non-linguistic+supervision+for+contrastive+learning+of+sentence+embeddings&oq=Non-Linguistic+Supervision+for+Contrastive+Learning+of+Sentence+Embeddings&aqs=chrome.0.0i512.250j0j1&sourceid=chrome&ie=UTF-8 | non-linguistic supervision for contrastive learning of sentence embeddings - Google Search
https://arxiv.org/abs/2204.10298 | DiffCSE Difference-based Contrastive Learning for Sentence Embeddings - Arxiv-2204.10298
https://arxiv.org/abs/2007.15651 | Contrastive Learning for Unpaired Image-to-Image Translation - Arxiv-2007.15651
https://zhuanlan.zhihu.com/p/334772391 | Noise Contrastive Estimation 前世今生——从 NCE 到 InfoNCE - 知乎
https://zhuanlan.zhihu.com/p/357071960 | CVPR2021自监督学习论文: 理解对比损失的性质以及温度系数的作用 - 知乎
https://lilianweng.github.io/posts/2021-05-31-contrastive/ | Contrastive Representation Learning | Lil'Log
https://lilianweng.github.io/posts/2019-11-10-self-supervised/ | Self-Supervised Representation Learning | Lil'Log
https://mail.google.com/mail/u/0/#inbox/FMfcgzGslkrfDlrxHBqgQwNMgfCXlMJC | Authenticate Your Email Address - 1999j0615une@gmail.com - Gmail

https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00434/108865/Self-Diagnosis-and-Self-Debiasing-A-Proposal-for | Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP | Transactions of the Association for Computational Linguistics | MIT Press
https://openreview.net/forum?id=CQsmMYmlP5T | Git Re-Basin Merging Models modulo Permutation Symmetries - OR-ICLR-2023_CQsmMYmlP5T
https://arxiv.org/abs/2209.04836 | Git Re-Basin Merging Models modulo Permutation Symmetries - OR-ICLR-2023_CQsmMYmlP5T
https://arxiv.org/abs/1910.03065 | Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations - Arxiv-1910.03065
https://github.com/samuela/git-re-basin | samuela/git-re-basin: Code release for "Git Re-Basin: Merging Models modulo Permutation Symmetries"
https://arxiv.org/abs/2209.11055 | Efficient Few-Shot Learning Without Prompts - Arxiv-2209.11055
http://proceedings.mlr.press/v97/chen19g/chen19g.pdf | Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels - PMLR-2019-chen19g
https://arxiv.org/abs/2205.11558 | Using Natural Language and Program Abstractions to Instill Human Inductive Biases in Machines - Arxiv-2205.11558
https://arxiv.org/abs/2206.08496 | Self-Supervised Contrastive Pre-Training For Time Series via Time-Frequency Consistency - Arxiv-2206.08496
https://arxiv.org/abs/2110.15943 | MetaICL Learning to Learn In Context - Arxiv-2110.15943
https://www.google.com/search?q=meta-learning+via+language+model+in-context+tuning&newwindow=1&sxsrf=ALiCzsa3EDwkzoxpax8RkM0P836TR7dAzg%3A1669920709812&ei=xfeIY-OVMdXW5NoPh_yhgA0&oq=meta+learning+via+in+c&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAxgAMgYIABAWEB4yBQgAEIYDMgUIABCGAzIFCAAQhgMyBQgAEIYDMgUIABCGAzoKCAAQRxDWBBCwAzoECCMQJzoICC4QsQMQgwE6EQguEIAEELEDEIMBEMcBENEDOggIABCxAxCDAToICC4QgwEQsQM6CwgAEIAEELEDEIMBOgsILhCxAxCDARDUAjoLCC4QgAQQsQMQgwE6BQgAEIAEOg4ILhCABBCxAxDHARDRAzoRCC4QgAQQsQMQgwEQ5QQQ1AI6CAgAEIAEELEDOggIABCABBDLAToHCAAQgAQQCkoECEEYAEoECEYYAFCnMFiFSmC3T2gEcAF4AYABZIgBgg6SAQQyMS4xmAEAoAEByAEIwAEB&sclient=gws-wiz-serp | meta-learning via language model in-context tuning - Google Search
https://scholar.google.com/scholar?q=%20Language%20models%20(mostly)%20know%20what%20they%20know | Google Scholar
https://arxiv.org/pdf/2109.15103.pdf | Scalable Rule-Based Representation Learning for Interpretable Classification - Arxiv-2109.15103
https://zhuanlan.zhihu.com/p/385866470 | 解读模型压缩9：无需数据的神经网络压缩技术 (一) - 知乎
https://zhuanlan.zhihu.com/p/308301901 | 3W字长文带你轻松入门视觉transformer - 知乎
https://zhuanlan.zhihu.com/p/160206075 | Knowledge Distillation（知识蒸馏）Review--20篇paper回顾 - 知乎
https://posts.careerengine.us/p/5e040074089a4c71be7da859 | 一文总览知识蒸馏概述
https://github.com/Guang000/Awesome-Dataset-Distillation | Guang000/Awesome-Dataset-Distillation: Awesome Dataset Distillation Papers
https://arxiv.org/pdf/2102.09559.pdf | CReST A Class-Rebalancing Self-Training Framework for Imbalanced Semi-Supervised Learning - Arxiv-2102.09559
https://arxiv.org/abs/2208.09392 | Cold Diffusion Inverting Arbitrary Image Transforms Without Noise - Arxiv-2208.09392
https://arxiv.org/pdf/2204.11790.pdf | Can Rationalization Improve Robustness? - Arxiv-2204.11790
https://github.com/SforAiDl/KD_Lib | SforAiDl/KD_Lib: A Pytorch Knowledge Distillation library for benchmarking and extending works in the domains of Knowledge Distillation, Pruning, and Quantization.
https://arxiv.org/pdf/2012.15699.pdf | Better Robustness by More Coverage Adversarial Training with Mixup Augmentation for Robust Fine-tuning - Arxiv-2012.15699
https://github.com/LJY-HY/MentorMix_pytorch | LJY-HY/MentorMix_pytorch: [MentorMix] "Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels" implemented in the PyTorch version.
https://ai.googleblog.com/2020/08/understanding-deep-learning-on.html | Understanding Deep Learning on Controlled Noisy Labels – Google AI Blog
https://arxiv.org/pdf/1911.09781.pdf | Beyond Synthetic Noise Deep Learning on Controlled Noisy Labels - Arxiv-1911.09781
https://aclanthology.org/2022.acl-long.201.pdf | Sequence-to-Sequence Knowledge Graph Completion and Question Answering - ACL-ACL-2022_2022.acl-long.201
https://arxiv.org/pdf/2112.08348.pdf | Prompt Waywardness The Curious Case of Discretized Interpretation of Continuous Prompts - Arxiv-2112.08348
https://arxiv.org/pdf/2210.12517.pdf | Exploring The Landscape of Distributional Robustness for Question Answering Models - Arxiv-2210.12517
https://arxiv.org/pdf/2205.12507.pdf | Re-Examining Calibration The Case of Question Answering - Arxiv-2205.12507
https://arxiv.org/pdf/2207.00746.pdf | INSCIT Information-Seeking Conversations with Mixed-Initiative Interactions - Arxiv-2207.00746
https://arxiv.org/abs/1708.03999 | ZOO Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models - Arxiv-1708.03999

https://zhuanlan.zhihu.com/p/316865623 | 2020年9月谷歌研究给出的综述“Efficient Transformers: A Survey” - 知乎
https://zhuanlan.zhihu.com/p/366744794 | 深度学习中不同类型卷积的综合介绍：2D卷积、3D卷积、转置卷积、扩张卷积、可分离卷积、扁平卷积、分组卷积、随机分组卷积、逐点分组卷积等pytorch代码实现和解析。 - 知乎
https://zhuanlan.zhihu.com/p/357628257 | Reformer: 一个在训练阶段存储极致压缩的Transformer模型 - 知乎
https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#ncaloss | Losses - PyTorch Metric Learning
https://css-tricks.com/bem-101/ | BEM 101 | CSS-Tricks - CSS-Tricks
https://www.spaces.ac.cn/archives/7708/comment-page-1 | 再谈类别不平衡问题：调节权重与魔改Loss的对比联系 - 科学空间|Scientific Spaces
https://posts.careerengine.us/p/60c2ebb480e090697baeb2a3 | Transformer模型有多少种变体？复旦邱锡鹏教授团队做了全面综述
https://arxiv.org/pdf/2106.04554.pdf | A Survey of Transformers - Arxiv-2106.04554
https://arxiv.org/abs/2111.07624 | Attention Mechanisms in Computer Vision A Survey - Arxiv-2111.07624
https://arxiv.org/pdf/2001.04451.pdf | Reformer The Efficient Transformer - OR-ICLR-2020_rkgNKkHtvB
https://github.com/xmu-xiaoma666/External-Attention-pytorch#re-parameter-series | xmu-xiaoma666/External-Attention-pytorch: 🍀 Pytorch implementation of various Attention Mechanisms, MLP, Re-parameter, Convolution, which is helpful to further understand papers.⭐⭐⭐
https://zhuanlan.zhihu.com/p/109342043 | GAN评价指标最全汇总 - 知乎
https://arxiv.org/pdf/1802.03446.pdf | Pros and Cons of GAN Evaluation Measures - Arxiv-1802.03446
https://wryou.xyz/2021/01/16/Evaluating-GANs.html#fnref:2 | Evaluating GANs - Wonryong Ryou
https://zhuanlan.zhihu.com/p/115741192 | 对Reformer的深入解读 - 知乎
https://zhuanlan.zhihu.com/p/139220925 | 💡Illustrating the Reformer - 知乎
https://www.jiqizhixin.com/articles/2019-03-23-5 | 四块GPU即可训练BigGAN：「官方版」PyTorch实现出炉 | 机器之心
https://arxiv.org/pdf/2111.06091.pdf | A Survey of Visual Transformers - Arxiv-2111.06091
https://www.zhihu.com/question/410332622 | (42 封私信 / 80 条消息) 模型的Robustness和Generalization是什么关系？ - 知乎
https://arxiv.org/pdf/2008.10032.pdf | Seesaw Loss for Long-Tailed Instance Segmentation - Arxiv-2008.10032
https://arxiv.org/pdf/2106.01465.pdf | Ethical-Advice Taker Do Language Models Understand Natural Language Interventions? - Arxiv-2106.01465
https://github.com/hbaniecki/adversarial-explainable-ai#attacks-on-explainability-and-fairness | hbaniecki/adversarial-explainable-ai: 💡 Adversarial attacks on explanations and how to defend them
https://arxiv.org/abs/2106.08367 | What Context Features Can Transformer Language Models Use? - Arxiv-2106.08367
https://arxiv.org/abs/2106.03993 | Lexicon Learning for Few-Shot Neural Sequence Modeling - Arxiv-2106.03993
https://arxiv.org/abs/2106.00737 | Implicit Representations of Meaning in Neural Language Models - Arxiv-2106.00737
https://arxiv.org/abs/2201.11114 | Natural Language Descriptions of Deep Visual Features - Arxiv-2201.11114
http://proceedings.mlr.press/v139/wong21a.html | Leveraging Language to Learn Program Abstractions and Search Heuristics - PMLR-2021-wong21a
https://arxiv.org/abs/2110.01517 | Skill Induction and Planning with Latent Language - ACL-ACL-2022_2022.acl-long.120
https://openreview.net/forum?id=NudBMY-tzDr | Natural Language Descriptions of Deep Visual Features - Arxiv-2201.11114
https://arxiv.org/abs/2202.01771 | Pre-Trained Language Models for Interactive Decision-Making - Arxiv-2202.01771
https://www.bilibili.com/video/BV18G41137Ws/?spm_id_from=333.1007.tianma.2-3-6.click&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | 金庸看了都摇头，优秀的武侠游戏该是怎样的？_哔哩哔哩bilibili
https://www.google.com/search?q=feedback+transformer&oq=feedback+transformer&aqs=chrome..69i57j0i512l6j69i65.2237j0j1&sourceid=chrome&ie=UTF-8 | feedback transformer - Google Search
https://github.com/lucidrains/x-transformers | lucidrains/x-transformers: A simple but complete full-attention transformer with a set of promising experimental features from various papers
https://github.com/lucidrains/feedback-transformer-pytorch | lucidrains/feedback-transformer-pytorch: Implementation of Feedback Transformer in Pytorch
https://github.com/thumbe3/label-noise-nlp | thumbe3/label-noise-nlp
https://github.com/haiphanNJIT/StoBatch | haiphanNJIT/StoBatch: Scalable Differential Privacy with Certified Robustness in Adversarial Learning (ICML'2020)
https://github.com/zbchern/awesome-machine-learning-reliability | zbchern/awesome-machine-learning-reliability: A curated list of awesome resources regarding machine learning reliability.
https://scholar.google.com/scholar?start=30&hl=en&as_sdt=40000005&sciodt=0,22&as_ylo=2022&cites=1496995241845170163&scipsc= | Petroni: Language models as knowledge bases? - Google Scholar
https://arxiv.org/pdf/2210.08536.pdf | Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding - Arxiv-2210.08536
https://arxiv.org/pdf/2211.05994.pdf | A Survey of Knowledge-Enhanced Pre-trained Language Models - Arxiv-2211.05994
https://watermark.silverchair.com/tacl_a_00454.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAr0wggK5BgkqhkiG9w0BBwagggKqMIICpgIBADCCAp8GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMKs95iKdDJGdDrL9VAgEQgIICcNfeMNiFpbXfGI3q1Jsl3omjxqCVTUeZsYS8SWBBXbBaL1QR5thZl5tswYU3mK-cyK90CjjaDZjR3tVxYWJkM0rruX7zYO2naxllHpFfIqNLIG0kQgxWqvPQSjlZ2CFu5ES54Y7pMLrxBlfsurfebCj5biuqk6Y0EUK6xhIxzwTkII9HKz1d7DywS9Mfrp56yCwmDy8eDW0TZXTY8mDnzWN4S3HElwa-kFZ7pf_dPr6HS_PszYIuRfms_JpjcDEof41qpJIGEMyKud8C_GxFxhi_ub_77xTNSKToykDRPWaQoy-PHqSo4jfR1FHbA8MS_yUETWqvSLzopo5vh_Tz3WbSLOKpeR8VuUuookYr-u_je82LFcIieLYn9NAZj7kl_JrrnoyYMw0Oyrb3LAO_gaZXlOd0LbYxDPSi6klKLLLuMPFBWWVTe61_MgbJ98UJdAWHkq9ZvMDRIVzYD2DkyM4686cDZMZiGZrvy1Tlivh67jybFvKJouvhAfl4Wj4gFlDyf0HCgADDmSTO4D6BMPNxKjT4Dpir40qagfJ1HXFTowGIbkjRH8pHHknbICAiH5kiE1XXISuUpmX02qKoccb4LnORIWy_Q7L85oO_3Ki2eY3it95pwoWA5aWdPff01JIpvS4M6wtmhMt5ynZUC0Z1HXhpnJ2plRlLhf-CBNBq1bLAbwLBP05kzUBtEStUvel0Wn25fu7nFYzIkGWIi8Ki3Z7y54HF8kiYNY6R_1tnHLkYRGX9MONYi0e8m4-tXOLaU6CrTVWk5Io61eCRnyeLV3QpEZ1n9wrfqlgYDn1X-S5oDMbV9rXhEMqnV_2jfQ | https://watermark.silverchair.com/tacl_a_00454.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAr0wggK5BgkqhkiG9w0BBwagggKqMIICpgIBADCCAp8GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMKs95iKdDJGdDrL9VAgEQgIICcNfeMNiFpbXfGI3q1Jsl3omjxqCVTUeZsYS8SWBBXbBaL1QR5thZl5tswYU3mK-cyK90CjjaDZjR3tVxYWJkM0rruX7zYO2naxllHpFfIqNLIG0kQgxWqvPQSjlZ2CFu5ES54Y7pMLrxBlfsurfebCj5biuqk6Y0EUK6xhIxzwTkII9HKz1d7DywS9Mfrp56yCwmDy8eDW0TZXTY8mDnzWN4S3HElwa-kFZ7pf_dPr6HS_PszYIuRfms_JpjcDEof41qpJIGEMyKud8C_GxFxhi_ub_77xTNSKToykDRPWaQoy-PHqSo4jfR1FHbA8MS_yUETWqvSLzopo5vh_Tz3WbSLOKpeR8VuUuookYr-u_je82LFcIieLYn9NAZj7kl_JrrnoyYMw0Oyrb3LAO_gaZXlOd0LbYxDPSi6klKLLLuMPFBWWVTe61_MgbJ98UJdAWHkq9ZvMDRIVzYD2DkyM4686cDZMZiGZrvy1Tlivh67jybFvKJouvhAfl4Wj4gFlDyf0HCgADDmSTO4D6BMPNxKjT4Dpir40qagfJ1HXFTowGIbkjRH8pHHknbICAiH5kiE1XXISuUpmX02qKoccb4LnORIWy_Q7L85oO_3Ki2eY3it95pwoWA5aWdPff01JIpvS4M6wtmhMt5ynZUC0Z1HXhpnJ2plRlLhf-CBNBq1bLAbwLBP05kzUBtEStUvel0Wn25fu7nFYzIkGWIi8Ki3Z7y54HF8kiYNY6R_1tnHLkYRGX9MONYi0e8m4-tXOLaU6CrTVWk5Io61eCRnyeLV3QpEZ1n9wrfqlgYDn1X-S5oDMbV9rXhEMqnV_2jfQ
https://arxiv.org/pdf/2204.06031.pdf | A Review on Language Models as Knowledge Bases - Arxiv-2204.06031
https://arxiv.org/pdf/2201.09680.pdf | Relational Memory Augmented Language Models - Arxiv-2201.09680
https://arxiv.org/pdf/2211.08332.pdf | Versatile Diffusion Text, Images and Variations All in One Diffusion Model - Arxiv-2211.08332
https://arxiv.org/abs/2211.05783 | Unifying Flow, Stereo and Depth Estimation - Arxiv-2211.05783
https://arxiv.org/abs/2211.07292 | Fast Text-Conditional Discrete Denoising on Vector-Quantized Latent Spaces - Arxiv-2211.07292
https://arxiv.org/pdf/2211.07830.pdf | Prompting Language Models for Linguistic Structure - Arxiv-2211.07830
https://aclanthology.org/2022.naacl-main.59.pdf | Explaining Toxic Text via Knowledge Enhanced Text Generation - ACL-NAACL-2022_2022.naacl-main.59
https://arxiv.org/pdf/2203.10652.pdf] | Continual Sequence Generation with Adaptive Compositional Modules - Arxiv-2203.10652
https://www.manhuagui.com/comic/11223/ | 赔命金漫画_血偿金漫画_沙村广明 - 看漫画
https://www.google.com/search?q=%E9%A9%BE%E7%AC%BC%E7%9C%9F%E5%A4%AA%E9%83%8E | 驾笼真太郎 - Google Search
https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00511/113490 | Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond | Transactions of the Association for Computational Linguistics | MIT Press
https://proceedings.neurips.cc/paper/2021/file/22b1f2e0983160db6f7bb9f62f4dbb39-Paper.pdf | How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness? - NeurIPS-2021_22b1f2e0
https://arxiv.org/pdf/2101.00288.pdf | Polyjuice Generating Counterfactuals for Explaining, Evaluating, and Improving Models - Arxiv-2101.00288
https://www.google.com/search?q=%E5%9C%B0%E9%9B%B7%E9%9C%87&sourceid=chrome&ie=UTF-8 | 地雷震 - Google Search
https://syntaxgym.org/ | SyntaxGym
https://arxiv.org/pdf/2104.03474.pdf | Revisiting Simple Neural Probabilistic Language Models - Arxiv-2104.03474
https://arxiv.org/pdf/1506.05254.pdf | Gradient Estimation Using Stochastic Computation Graphs - Arxiv-1506.05254
https://arxiv.org/abs/2104.07000 | IGA An Intent-Guided Authoring Assistant - Arxiv-2104.07000
https://arxiv.org/abs/2103.15335 | Changing the Mind of Transformers for Topically-Controllable Language Generation - Arxiv-2103.15335
https://arxiv.org/abs/2203.10053 | RELIC Retrieving Evidence for Literary Claims - Arxiv-2203.10053
https://arxiv.org/abs/2205.09726 | RankGen Improving Text Generation with Large Ranking Models - Arxiv-2205.09726
https://scholar.google.com/scholar?start=10&hl=en&as_sdt=40000005&sciodt=0,22&as_ylo=2021&cites=3065323231297656090&scipsc= | Krishna: Reformulating unsupervised style transfer... - Google Scholar
https://arxiv.org/pdf/2109.05554.pdf | No True State-of-the-Art? OOD Detection Methods are Inconsistent across Datasets - Arxiv-2109.05554
https://yobibyte.github.io/files/paper_notes/dice.pdf | dice.pdf
https://arxiv.org/pdf/2208.00005.pdf | Testing Relational Understanding in Text-Guided Image Generation - Arxiv-2208.00005
https://arxiv.org/pdf/2107.06278.pdf | Per-Pixel Classification is Not All You Need for Semantic Segmentation - Arxiv-2107.06278
https://aclanthology.org/2022.repl4nlp-1.23.pdf | Towards Improving Selective Prediction Ability of NLP Systems - ACL-ACL| RepL4NLP| WS-2022_2022.repl4nlp-1.23
https://www.google.com/search?q=stochastic+computational+graph&oq=stochastic+computational+graph&aqs=chrome..69i57j0i22i30j0i390l5.4455j0j1&sourceid=chrome&ie=UTF-8 | stochastic computational graph - Google Search

http://web.cs.ucla.edu/~patricia.xiao/files/CS_260_Cheatsheet_version2.pdf | CS_260_Cheatsheet_version2.pdf

https://www.google.com/search?q=lime+learning+inductive+bias+for+primitives+of+mathematical+reasoning&oq=lime+learning+ind&aqs=chrome.4.69i57j0i546l2j0i546i649j0i546l2.5197j0j1&sourceid=chrome&ie=UTF-8 | lime learning inductive bias for primitives of mathematical reasoning - Google Search
https://www.google.com/search?q=taskmatrix+ai&newwindow=1&sxsrf=APwXEdcrWxju921Ogk05hGsm8rYznMVB2w%3A1680311507938&ei=04QnZMfuOJif5NoPq8uasAM&ved=0ahUKEwiHpKuUwIf-AhWYD1kFHaulBjYQ4dUDCBA&uact=5&oq=taskmatrix+ai&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIGCAAQDRADMgYIABANEAMyCAgAEIoFEIYDMggIABCKBRCGAzIICAAQigUQhgMyCAgAEIoFEIYDOgoIABBHENYEELADOgcIIxDqAhAnOg0IABCPARDqAhC0AhgBOg0ILhCPARDqAhC0AhgBOgQIIxAnOggIABCKBRCRAjoNCAAQigUQsQMQgwEQQzoHCAAQigUQQzoLCC4QgAQQsQMQgwE6CwgAEIoFELEDEIMBOg4ILhCABBCxAxDHARDRAzoQCC4QigUQsQMQxwEQ0QMQQzoRCC4QgAQQsQMQgwEQxwEQ0QM6CggAEIAEEBQQhwI6BwguEIoFEEM6BQgAEIAEOhAIABCABBAUEIcCELEDEIMBOgQIABADOg4ILhCABBCxAxCDARDUAjoLCAAQgAQQsQMQgwE6EwguEIoFELEDEIMBENQCEEMQ6gQ6EAguEIMBENQCELEDEIoFEEM6CggAEIoFELEDEEM6EwguEIAEELEDEIMBENQCEAoQ6gQ6DQgAEIAEELEDEIMBEAo6DQguEIAEELEDEIMBEAo6EAguEIMBENQCELEDEIAEEAo6CggAEIAEELEDEAo6EAguEIAEELEDEIMBENQCEAo6BggAEAoQAzoHCAAQgAQQCjoICAAQHhAPEAo6BwgAEA0QgAQ6BggAEB4QDToICAAQHhANEA86BggAEBYQHjoKCAAQFhAeEA8QCkoECEEYAFDbBFjdkwFgr5UBaAJwAXgAgAHEAYgBrwuSAQM1LjiYAQCgAQGwARTIAQfAAQHaAQYIARABGAo&sclient=gws-wiz-serp | taskmatrix ai - Google Search
https://arxiv.org/abs/2206.10139 | Insights into Pre-training via Simpler Synthetic Tasks - Arxiv-2206.10139
https://arxiv.org/abs/2303.16434 | TaskMatrix.AI Completing Tasks by Connecting Foundation Models with Millions of APIs - Arxiv-2303.16434
https://www.google.com/search?q=visual+chatggpt&oq=visual+chatggpt&aqs=chrome..69i57j0i512j0i10i512j0i512l7.1720j0j1&sourceid=chrome&ie=UTF-8 | visual chatggpt - Google Search
https://www.google.com/search?q=your+diffusion+model+is+secretly+a+zero+shot+classifier&oq=your+diffusion+model+is+sec&aqs=chrome.0.0i3j69i57j0i390i650l3j69i61l3.6085j0j1&sourceid=chrome&ie=UTF-8 | your diffusion model is secretly a zero shot classifier - Google Search
https://www.google.com/search?q=whose+opinion+do+language+models+reflect&oq=whose+opinion+do+language+models+reflect&aqs=chrome..69i57j33i160l2.14039j0j1&sourceid=chrome&ie=UTF-8 | whose opinion do language models reflect - Google Search
https://paperswithcode.com/paper/whose-opinions-do-language-models-reflect | Whose Opinions Do Language Models Reflect? | Papers With Code
https://www.google.com/search?q=scene+graph&oq=scene+g&aqs=chrome.2.69i57j0i433i512j0i512l4j69i60l2.10906j1j1&sourceid=chrome&ie=UTF-8 | scene graph - Google Search
https://www.google.com/search?q=3DB+paper&oq=3DB+paper&aqs=chrome..69i57j33i160.2130j0j1&sourceid=chrome&ie=UTF-8 | 3DB paper - Google Search
https://www.google.com/search?q=sparks+of+artificial+general+intelligence&oq=sparks+of+&aqs=chrome.2.0i3j0i512j0i3j69i57j0i512j46i433i512j0i3j0i512l2j0i3.2379j0j1&sourceid=chrome&ie=UTF-8 | sparks of artificial general intelligence - Google Search
https://www.google.com/search?q=label+efficient+semantic+segmentation&oq=label+efficient&aqs=chrome.0.0i512j69i57j0i512j0i22i30l3j0i390i650l2.2768j0j1&sourceid=chrome&ie=UTF-8 | label efficient semantic segmentation - Google Search

https://dandanzan.net/dianying/20230132.html | 《网络谜踪2》2023年美国剧情惊悚电影在线观看 - 蛋蛋赞影院

https://arxiv.org/pdf/2203.01543.pdf | QaNER Prompting Question Answering Models for Few-shot Named Entity Recognition - Arxiv-2203.01543
https://arxiv.org/pdf/2303.04132.pdf | Exploiting Asymmetry for Synthetic Training Data Generation SynthIE and the Case of Information Extraction - Arxiv-2303.04132
https://arxiv.org/pdf/2011.01549.pdf | DAGA Data Augmentation with a Generation Approach for Low-resource Tagging Tasks - Arxiv-2011.01549
https://laion.ai/blog/open-flamingo/ | Announcing OpenFlamingo: An open-source framework for training vision-language models with in-context learning | LAION
https://github.com/hwchase17/langchain | hwchase17/langchain: ⚡ Building applications with LLMs through composability ⚡
https://python.langchain.com/en/latest/ | Welcome to LangChain — 🦜🔗 LangChain 0.0.125
https://python.langchain.com/en/latest/modules/chains/index_examples/summarize.html | Summarization — 🦜🔗 LangChain 0.0.126
https://python.langchain.com/en/latest/use_cases/personal_assistants.html | Personal Assistants — 🦜🔗 LangChain 0.0.125
https://python.langchain.com/en/latest/use_cases/question_answering.html | Question Answering over Docs — 🦜🔗 LangChain 0.0.126
https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation | SolidGoldMagikarp (plus, prompt generation) - LessWrong
https://github.com/teticio/audio-diffusion | teticio/audio-diffusion: Apply diffusion models using the new Hugging Face diffusers package to synthesize music instead of images.
https://openreview.net/pdf?id=68EuccCtO5i | Differentially Private Model Compression - OR-NeurIPS-2022_68EuccCtO5i
https://arxiv.org/pdf/2201.00971.pdf | Submix Practical Private Prediction for Large-Scale Language Models - Arxiv-2201.00971
https://arxiv.org/pdf/2205.13621.pdf | Differentially Private Decoding in Large Language Models - Arxiv-2205.13621
https://arxiv.org/pdf/2303.16203.pdf | Your Diffusion Model is Secretly a Zero-Shot Classifier - Arxiv-2303.16203

https://arxiv.org/pdf/2212.14024.pdf | Demonstrate-Search-Predict Composing retrieval and language models for knowledge-intensive NLP - Arxiv-2212.14024
https://arxiv.org/pdf/2302.07371.pdf | AutoBiasTest Controllable Sentence Generation for Automated and Open-Ended Social Bias Testing in Language Models - Arxiv-2302.07371
https://arxiv.org/pdf/2110.05679.pdf | Large Language Models Can Be Strong Differentially Private Learners - Arxiv-2110.05679
https://arxiv.org/pdf/2209.01975.pdf | Selective Annotation Makes Language Models Better Few-Shot Learners - Arxiv-2209.01975
https://arxiv.org/pdf/2112.08633.pdf | Learning To Retrieve Prompts for In-Context Learning - ACL-NAACL-2022_2022.naacl-main.191
https://github.com/OhadRubin/EPR | OhadRubin/EPR
https://arxiv.org/pdf/2109.05620.pdf | RockNER A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models - Arxiv-2109.05620
https://arxiv.org/pdf/2110.08454.pdf | Good Examples Make A Faster Learner Simple Demonstration-based Learning for Low-resource NER - Arxiv-2110.08454
https://www.ijcai.org/proceedings/2022/0590.pdf | Low-Resource NER by Data Augmentation With Prompting - IJCAI-2022_590
https://arxiv.org/abs/2302.08659 | Uncertainty-aware Self-training for Low-resource Neural Sequence Labeling - Arxiv-2302.08659

https://scholar.googleusercontent.com/scholar.bib?q=info:hG0iVOrOguoJ:scholar.google.com/&output=citation&scisdr=CgXNZnReEMjhx75lVt4:AAGBfm0AAAAAZB5jTt7SRB_TwcZ8biXT0B62OO1vyH06&scisig=AAGBfm0AAAAAZB5jTmF2ANL3ww7bgYzANmQFSQpWUMdp&scisf=4&ct=citation&cd=-1&hl=en | https://scholar.googleusercontent.com/scholar.bib?q=info:hG0iVOrOguoJ:scholar.google.com/&output=citation&scisdr=CgXNZnReEMjhx75lVt4:AAGBfm0AAAAAZB5jTt7SRB_TwcZ8biXT0B62OO1vyH06&scisig=AAGBfm0AAAAAZB5jTmF2ANL3ww7bgYzANmQFSQpWUMdp&scisf=4&ct=citation&cd=-1&hl=en
https://poe.com/claude | Poe - Claude
https://scholar.google.com/scholar?as_ylo=2022&q=in+context+learning+selection+exemplar&hl=en&as_sdt=0,22&as_vis=1 | in context learning selection exemplar - Google Scholar
https://web.mit.edu/canvas/ | MIT Canvas - Login
https://docs.google.com/spreadsheets/d/1zks-bDD4LPzV7wZ78UXVLn27nLvXqiM7rcl_L6OUoxg/edit#gid=0 | LLM Class Groups - Google 表格
https://www.google.com/search?q=auto+chain+of+thought&oq=auto+chain+of+thought&aqs=chrome.0.0i512j0i15i22i30j0i22i30l5j69i60.3282j0j1&sourceid=chrome&ie=UTF-8 | auto chain of thought - Google Search
https://github.com/Timothyxxx/Chain-of-ThoughtsPapers | Timothyxxx/Chain-of-ThoughtsPapers: A trend starts from "Chain of Thought Prompting Elicits Reasoning in Large Language Models".
https://arxiv.org/pdf/2210.03493.pdf | Automatic Chain of Thought Prompting in Large Language Models - Arxiv-2210.03493
https://arxiv.org/pdf/2211.04486.pdf | Active Example Selection for In-Context Learning - Arxiv-2211.04486
https://arxiv.org/pdf/2302.12246.pdf | Active Prompting with Chain-of-Thought for Large Language Models - Arxiv-2302.12246
https://www.google.com/search?q=zero+shot+reasonser&oq=zero+shot+reasonser&aqs=chrome..69i57j0i13i512j0i390i650l5.1945j0j1&sourceid=chrome&ie=UTF-8 | zero shot reasonser - Google Search
https://arxiv.org/pdf/2205.11916.pdf | Large Language Models are Zero-Shot Reasoners - Arxiv-2205.11916
https://www.google.com/search?q=self+verification+llm&oq=self+verification+llm&aqs=chrome..69i57.2476j0j1&sourceid=chrome&ie=UTF-8 | self verification llm - Google Search
https://arxiv.org/pdf/2212.09561.pdf | Large Language Models are reasoners with Self-Verification - Arxiv-2212.09561
https://n.derek.ma/2_literature/bias | bias - Derek Ma

https://www.bilibili.com/video/BV1UD4y137vs/?spm_id_from=333.337.search-card.all.click&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | 日式“克苏鲁”受残秽影响 疯狂后杀掉7个婴儿 《残秽.不可以住的房间》剧情精讲/细节解析_哔哩哔哩_bilibili

https://akariasai.github.io/files/llm_memorization.pdf | llm_memorization.pdf
https://arxiv.org/pdf/2201.05575.pdf | Reasoning Through Memorization Nearest Neighbor Knowledge Graph Embeddings - Arxiv-2201.05575
https://arxiv.org/pdf/2205.12674.pdf | Training Language Models with Memory Augmentation - Arxiv-2205.12674
https://github.com/Timothyxxx/RetrivalLMPapers | Timothyxxx/RetrivalLMPapers: Paper collections of retrieval-based(augmented) language model.
https://antoniolonga.github.io/Pytorch_geometric_tutorials/ | Pytorch Geometric Tutorial
https://arxiv.org/pdf/1906.05664.pdf | Calibration, Entropy Rates, and Memory in Language Models - Arxiv-1906.05664
https://arxiv.org/abs/2202.05262 | Locating and Editing Factual Associations in GPT - Arxiv-2202.05262
https://openreview.net/forum?id=gJcEM8sxHK | Mapping Language Models to Grounded Conceptual Spaces | OpenReview
https://arxiv.org/abs/2106.00737 | Implicit Representations of Meaning in Neural Language Models - Arxiv-2106.00737
https://roberttlange.github.io/posts/2020/06/lottery-ticket-hypothesis/ | The Lottery Ticket Hypothesis: A Survey - Rob’s Homepage
http://proceedings.mlr.press/v119/malach20a/malach20a.pdf | malach20a.pdf
https://arxiv.org/pdf/2208.03299.pdf | Atlas Few-shot Learning with Retrieval Augmented Language Models - Arxiv-2208.03299
https://arxiv.org/pdf/2301.12652.pdf | REPLUG Retrieval-Augmented Black-Box Language Models - Arxiv-2301.12652
https://github.com/tloen/alpaca-lora | tloen/alpaca-lora: Instruct-tune LLaMA on consumer hardware
https://github.com/AlexTMallen/adaptive-retrieval | AlexTMallen/adaptive-retrieval

https://arxiv.org/pdf/2212.10534.pdf | DISCO Distilling Phrasal Counterfactuals with Large Language Models - Arxiv-2212.10534
https://arxiv.org/pdf/2110.02467.pdf | BadPre Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models - Arxiv-2110.02467
https://openreview.net/forum?id=PS3IMnScugk | Learning to Recombine and Resample Data For Compositional Generalization | OpenReview
https://arxiv.org/pdf/2302.07452.pdf | How to Train Your DRAGON Diverse Augmentation Towards Generalizable Dense Retrieval - Arxiv-2302.07452
https://arxiv.org/pdf/2208.08984.pdf | Open-Vocabulary Panoptic Segmentation with MaskCLIP - Arxiv-2208.08984
https://harvard.zoom.us/rec/play/9QSw6sWzuS7nruMMCdNjNBf0IMrZdyIxfvw1tYTZRz7ch9NsaRfKHZ-jjPDww7gpy-USE0LUf5lYnBj6.TkHXcTuNtxDjEQ9V?continueMode=true&_x_zm_rtaid=Y2dSpXH6SkOzxD_UfsWg2A.1677764513694.6e677a81f0c314953edb7716ccdb6cf2&_x_zm_rhtaid=327 | Boaz Barak's Zoom Meeting - Zoom

https://www.google.com/search?q=diffusion+instance+segmentation&oq=diffusion+instance+segmentation&aqs=chrome..69i57j0i22i30j0i390l4.10158j0j1&sourceid=chrome&ie=UTF-8 | diffusion instance segmentation - Google Search
https://arxiv.org/abs/2212.02773 | [2212.02773] DiffusionInst: Diffusion Model for Instance Segmentation
https://github.com/ant-research/diffusion-model-for-instance-segmentation | ant-research/diffusion-model-for-instance-segmentation: This repository is the code of the paper "DiffusionInst: Diffusion Model for Instance Segmentation".

https://arxiv.org/pdf/2303.04803.pdf | Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models - Arxiv-2303.04803
https://jerryxu.net/ODISE/ | Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models
https://huggingface.co/spaces/xvjiarui/ODISE/blob/main/app.py | app.py · xvjiarui/ODISE at main
https://github.com/lllyasviel/ControlNet | lllyasviel/ControlNet: Let us control diffusion models!
https://arxiv.org/pdf/2302.05543.pdf | Adding Conditional Control to Text-to-Image Diffusion Models - Arxiv-2302.05543
https://www.google.com/search?q=ControlNet | ControlNet - Google Search
https://github.com/facebookresearch/Mask2Former | facebookresearch/Mask2Former: Code release for "Masked-attention Mask Transformer for Universal Image Segmentation"
https://arxiv.org/pdf/2301.13188.pdf | Extracting Training Data from Diffusion Models - Arxiv-2301.13188
https://arxiv.org/pdf/2012.07805.pdf | Extracting Training Data from Large Language Models - Arxiv-2012.07805
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=q4qDvAoAAAAJ&citation_for_view=q4qDvAoAAAAJ:eQOLeE2rZwMC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=q4qDvAoAAAAJ&citation_for_view=q4qDvAoAAAAJ:Se3iqnhoufwC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=q4qDvAoAAAAJ&citation_for_view=q4qDvAoAAAAJ:KlAtU1dfN6UC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=q4qDvAoAAAAJ&citation_for_view=q4qDvAoAAAAJ:dhFuZR0502QC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=q4qDvAoAAAAJ&citation_for_view=q4qDvAoAAAAJ:kNdYIx-mwKoC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=q4qDvAoAAAAJ&citation_for_view=q4qDvAoAAAAJ:ZeXyd9-uunAC | View article
https://www.google.com/search?q=machine+unlearning&oq=machine+unlearning&aqs=chrome..69i57j0i512l6j0i22i30l3.2395j0j1&sourceid=chrome&ie=UTF-8 | machine unlearning - Google Search
https://arxiv.org/pdf/2209.02299.pdf | A Survey of Machine Unlearning - Arxiv-2209.02299
https://arxiv.org/pdf/2110.05223.pdf | Continual Learning with Differential Privacy - Arxiv-2110.05223
https://arxiv.org/pdf/1902.06497.pdf | Differentially Private Continual Learning - Arxiv-1902.06497
https://www.google.com/search?q=differential+privacy+fairness&newwindow=1&sxsrf=AJOqlzWuZGngWQiV7PJB0o3v6ePaCSBNZw%3A1679123878679&ei=pmUVZM6IKYWr5NoPsNGKgAQ&ved=0ahUKEwjO88ny9-T9AhWFFVkFHbCoAkAQ4dUDCBA&uact=5&oq=differential+privacy+fairness&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIFCAAQgAQyBggAEBYQHjIFCAAQhgMyBQgAEIYDOgoIABBHENYEELADOgcIABCwAxBDOg0IABDkAhDWBBCwAxgBOgwILhDIAxCwAxBDGAI6CggAEIAEEBQQhwI6BAgAEEM6BQguEIAESgQIQRgAUMUBWIIJYIwKaAFwAXgAgAFYiAHQBZIBATmYAQCgAQHIARHAAQHaAQYIARABGAnaAQYIAhABGAg&sclient=gws-wiz-serp | differential privacy fairness - Google Search
https://openreview.net/forum?id=zAxuIJLb38 | Knowledge Unlearning for Mitigating Privacy Risks in Language Models | OpenReview
https://openreview.net/pdf?id=zAxuIJLb38 | pdf

https://openreview.net/pdf?id=qiaRo_7Zmug | pdf
https://arxiv.org/pdf/2211.09527.pdf | Ignore Previous Prompt Attack Techniques For Language Models - Arxiv-2211.09527
https://aclanthology.org/2022.naacl-main.191.pdf | Learning To Retrieve Prompts for In-Context Learning - ACL-NAACL-2022_2022.naacl-main.191
https://arxiv.org/pdf/2212.10509.pdf | Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions - Arxiv-2212.10509
https://arxiv.org/pdf/2302.12813.pdf | Check Your Facts and Try Again Improving Large Language Models with External Knowledge and Automated Feedback - Arxiv-2302.12813
https://arxiv.org/abs/2211.07636 | EVA Exploring the Limits of Masked Visual Representation Learning at Scale - Arxiv-2211.07636

https://arxiv.org/abs/1702.08591 | The Shattered Gradients Problem If resnets are the answer, then what is the question? - Arxiv-1702.08591
https://twitter.com/SeonghyeonYe/status/1580170684466360323 | https://twitter.com/SeonghyeonYe/status/1580170684466360323
https://twitter.com/SeonghyeonYe/status/1617041418857611267 | (1) Seonghyeon Ye on Twitter: "Flipped learning paper is accepted to #ICLR2023 ! Check out our paper if you are interested in large language models &amp; instruction tuning! Paper: https://t.co/IYF1FLtape Demo: https://t.co/SobchsSOmv Special thanks to coauthors as well 😄" / Twitter
https://github.com/OpenBioLink/ThoughtSource | OpenBioLink/ThoughtSource: A central, open resource for data and tools related to chain-of-thought reasoning in large language models. Developed @ Samwald research group: https://samwald.info/
https://arxiv.org/abs/2302.10149 | Poisoning Web-Scale Training Datasets is Practical - Arxiv-2302.10149
https://twitter.com/omarsar0/status/1628034003776204800 | (1) elvis on Twitter: "ChatGPT is the biggest buzz in AI today. ChatGPT demonstrates remarkable capabilities so there is a high interest to replicate it. Colossal-AI just open-sourced a solution that replicates ChatGPT training process: https://t.co/XvFXjqeqZF" / Twitter
https://twitter.com/GuillaumeLample/status/1629151231800115202 | (1) Guillaume Lample on Twitter: "Today we release LLaMA, 4 foundation models ranging from 7B to 65B parameters. LLaMA-13B outperforms OPT and GPT-3 175B on most benchmarks. LLaMA-65B is competitive with Chinchilla 70B and PaLM 540B. The weights for all models are open and available at https://t.co/q51f2oPZlE 1/n https://t.co/DPyJFBfWEq" / Twitter
https://github.com/tloen/llama-int8 | tloen/llama-int8: Quantized inference code for LLaMA models
https://github.com/tatsu-lab/stanford_alpaca | tatsu-lab/stanford_alpaca: Code and documentation to train Stanford's Alpaca models, and generate the data.
https://twitter.com/cwolferesearch/status/1612099963013529609 | (1) Cameron R. Wolfe on Twitter: "OPT-IML (just released by @MetaAI) is a version of OPT-175B that's fine-tuned on a benchmark (OPT-IML Bench) of ~2000 instruction-based tasks. Prior work indicated that instruction-tuning is useful for LLMs, but we learn more details about this with OPT-IML... 🧵[1/8]" / Twitter
https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15 | Understanding the Open Pre-Trained Transformers (OPT) Library
https://docs.google.com/presentation/d/1W-c7CYEpByKNDOmGj8pZ_otY-u2yFcw0ZztsVF4vHYk/edit#slide=id.g21c5ed5ef8c_0_10 | Instruction Attack - Google 幻灯片
https://arxiv.org/pdf/2210.11416.pdf | Scaling Instruction-Finetuned Language Models - Arxiv-2210.11416
https://huggingface.co/google/flan-ul2 | google/flan-ul2 · Hugging Face
https://arxiv.org/abs/2302.00093 | Large Language Models Can Be Easily Distracted by Irrelevant Context - Arxiv-2302.00093
https://github.com/thunlp/StyleAttack/blob/main/experiments/prepare_probingdata.py | StyleAttack/prepare_probingdata.py at main · thunlp/StyleAttack
https://github.com/martiansideofthemoon/style-transfer-paraphrase | martiansideofthemoon/style-transfer-paraphrase: Official code and data repository for our EMNLP 2020 long paper "Reformulating Unsupervised Style Transfer as Paraphrase Generation" (https://arxiv.org/abs/2010.05700).

https://arxiv.org/pdf/2302.09170.pdf | KILM Knowledge Injection into Encoder-Decoder Language Models - Arxiv-2302.09170

https://arxiv.org/pdf/2105.10123.pdf | Backdoor Attacks on Self-Supervised Learning - Arxiv-2105.10123
https://github.com/uclanlp/awesome-fairness-papers | uclanlp/awesome-fairness-papers: Papers on fairness in NLP
https://aclanthology.org/2021.acl-long.330/ | Societal Biases in Language Generation: Progress and Challenges - ACL Anthology
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C22&as_ylo=2022&as_vis=1&q=continual+learning+experience+replay+instruction&btnG= | continual learning experience replay instruction - Google Scholar
https://arxiv.org/pdf/2301.12314.pdf | Progressive Prompts Continual Learning for Language Models - Arxiv-2301.12314
https://arxiv.org/pdf/2211.12701.pdf | Continual Learning of Natural Language Processing Tasks A Survey - Arxiv-2211.12701
https://arxiv.org/pdf/2205.02014.pdf | On Continual Model Refinement in Out-of-Distribution Data Streams - Arxiv-2205.02014
https://arxiv.org/pdf/2010.05595.pdf | Rethinking Experience Replay: a Bag of Tricks for Continual Learning | PDF

https://drive.google.com/drive/folders/0B4E10azXECctWnl4NG11bmhiOTQ?resourcekey=0-SBKd66q-dDlNrQuEjQV_lg | compound - Google 云端硬盘
https://twitter.com/_jasonwei/status/1516445376835776518 | (1) Jason Wei on Twitter: "In the spirit of the PhD admissions season ending, I'm making my state of purpose public. I learned a lot from reading @nelsonfliu and @ssgrn's SoPs, and so I'd like to pay it forward. https://t.co/awYteBgHow https://t.co/MhJQc5nBnP Prior SoPs below:" / Twitter
https://boards.greenhouse.io/thealleninstitute/jobs/2171573 | Job Application for Predoctoral Young Investigator at The Allen Institute for AI
https://boards.greenhouse.io/thealleninstitute/jobs/2171612 | Job Application for Research Internship at The Allen Institute for AI
https://boards.greenhouse.io/thealleninstitute/jobs/2137393 | Job Application for Research Scientist at The Allen Institute for AI
https://boards.greenhouse.io/thealleninstitute/jobs/2171617 | Job Application for Young Investigator at The Allen Institute for AI
https://boards.greenhouse.io/thealleninstitute/jobs/2171324 | Job Application for Research Internship at The Allen Institute for AI
https://boards.greenhouse.io/thealleninstitute/jobs/2167055 | Job Application for Research Scientist at The Allen Institute for AI
https://boards.greenhouse.io/thealleninstitute/jobs/2170237 | Job Application for Young Investigator at The Allen Institute for AI
https://boards.greenhouse.io/thealleninstitute/jobs/2171532 | Job Application for Research Internship at The Allen Institute for AI
https://www.zhihu.com/people/xuan-jiu-ye/posts?page=2 | (42 封私信 / 80 条消息) 玄玖爷 - 知乎
https://pan.baidu.com/s/1qWH1uYK#list/path=%2F | 王晴川-惊鹤潜龙记.txt_免费高速下载|百度网盘-分享无限制
https://search.bilibili.com/all?keyword=flowers%20for%20algernon | flowers for algernon-哔哩哔哩_Bilibili
https://wx.tianyabooks.com/book/xdwx1/ | 石榴记小说在线阅读 - 小椴 - 武侠小说网
https://www.kanunu8.com/book3/6346/index.html | 傲君刀 - 马舸 - 小说在线阅读 - 努努书坊
https://www.kanunu8.com/book3/6344/index.html | 幻真缘 - 马舸 - 小说在线阅读 - 努努书坊
https://www.kanunu8.com/book/4486/ | 此间的少年 - 江南 - 小说在线阅读 - 努努书坊
https://book.douban.com/subject/1824256/ | 破阵子·龙吟 (豆瓣)
https://book.douban.com/subject/2342840/ | 苏旷传奇 (豆瓣)

https://arxiv.org/pdf/2105.03659.pdf | Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text - Arxiv-2105.03659
https://arxiv.org/pdf/2212.08410.pdf | Teaching Small Language Models to Reason - Arxiv-2212.08410
https://arxiv.org/pdf/2203.05115.pdf | Internet-augmented language models through few-shot prompting for open-domain question answering - Arxiv-2203.05115
https://arxiv.org/pdf/2205.10770.pdf | Memorization Without Overfitting Analyzing the Training Dynamics of Large Language Models - Arxiv-2205.10770
https://arxiv.org/pdf/2302.04931.pdf | In-Context Learning with Many Demonstration Examples - Arxiv-2302.04931
https://arxiv.org/pdf/2210.06726.pdf | Explanations from Large Language Models Make Small Reasoners Better - Arxiv-2210.06726
https://aclanthology.org/2022.emnlp-main.174.pdf | Iteratively Prompt Pre-trained Language Models for Chain of Thought - ACL-EMNLP-2022_2022.emnlp-main.174
https://arxiv.org/pdf/2212.00193.pdf | Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions - Arxiv-2212.00193
https://dreambooth.github.io/ | DreamBooth
https://arxiv.org/pdf/2208.12242.pdf | DreamBooth Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation - Arxiv-2208.12242
https://huggingface.co/spaces | Spaces - Hugging Face
https://huggingface.co/new-space | Hugging Face – The AI community building the future.
https://gas.graviti.com/dataset/graviti/RAF_DB | Graviti Open Datasets/RAF-DB | Graviti
http://www.whdeng.cn/raf/li_RAFDB_2017_CVPR.pdf | www.whdeng.cn/raf/li_RAFDB_2017_CVPR.pdf
https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720639.pdf | 136720639.pdf
https://github.com/toharl/soft/blob/main/rafdb_noisy/inject0.3noise_asym_cmat.txt | soft/inject0.3noise_asym_cmat.txt at main · toharl/soft

https://arxiv.org/pdf/2104.14690.pdf | Entailment as Few-Shot Learner - Arxiv-2104.14690
https://arxiv.org/pdf/2106.13353.pdf | Cutting Down on Prompts and Parameters Simple Few-Shot Learning with Language Models - Arxiv-2106.13353
https://arxiv.org/pdf/2111.08284.pdf | Few-Shot Self-Rationalization with Natural Language Prompts - Arxiv-2111.08284
https://arxiv.org/pdf/2205.14704.pdf | Decoupling Knowledge from Memorization Retrieval-augmented Prompt Learning - Arxiv-2205.14704
https://arxiv.org/abs/2101.06804 | What Makes Good In-Context Examples for GPT-$3$? | Abstract
https://arxiv.org/abs/2209.01975 | Selective Annotation Makes Language Models Better Few-Shot Learners - Arxiv-2209.01975
https://arxiv.org/abs/2107.08251 | Generative Pretraining for Paraphrase Evaluation - Arxiv-2107.08251

https://www.google.com/search?q=Mc-.%20Candlish%20et%20al.%20(2018)%20study%20the%20impact%20of%20gradient%20variance%20on%20scaling%20efficiency.%20By%20averaging%20the%20relative%20gradient%20variance%20over%20the%C2%A0. | Mc-. Candlish et al. (2018) study the impact of gradient variance on scaling efficiency. By averaging the relative gradient variance over the . - Google Search
https://www.google.com/search?q=An+empirical+model+of+large-batch+training&sourceid=chrome&ie=UTF-8 | An empirical model of large-batch training - Google Search
https://boazbk.github.io/mltheoryseminar/ | CS229br Foundations of Deep Learning (aka Topics in the Foundations of Machine Learning)
https://arxiv.org/pdf/1905.11604.pdf | SGD on Neural Networks Learns Functions of Increasing Complexity - Arxiv-1905.11604
https://app.perusall.com/courses/compsci-229br-topics-in-the-foundations-of-machine-learning/_/dashboard/assignments/kBcxBctLood6fZn95 | COMPSCI 229BR: Topics in the Foundations of Machine Learning - Perusall
https://app.perusall.com/courses/compsci-229br-topics-in-the-foundations-of-machine-learning/chinchilla?assignmentId=yDMhjDMMtTK4fgMTC&part=1 | chinchilla - COMPSCI 229BR: Topics in the Foundations of Machine Learning - Perusall
https://www.google.com/search?q=nakkiran+kaplun+kalimeris&oq=nakkiran+kaplun+kalimeris&aqs=chrome..69i57j33i160l3.4321j0j1&sourceid=chrome&ie=UTF-8 | nakkiran kaplun kalimeris - Google Search
https://twitter.com/rasbt/status/1621522841417170946?lang=en | Sebastian Raschka on Twitter: "After putting together a lecture on multi-GPU training paradigms, I thought it might be a good idea to catch up with the recent “Cramming: Training a Language Model on a Single GPU in One Day” paper (https://t.co/sv3VMPEDAd). An interesting read with lots of insights! 1/8" / Twitter
https://twitter.com/memdotai/status/1608715108972109824 | Mem on Twitter: "@Ilxcondottiero @LChoshen @jonasgeiping @tomgoldsteincs Saved! Here's the compiled thread: https://t.co/jP4nSN3iHp 🪄 AI-generated summary: "This paper explores various ways to make training more efficient when limited to one GPU and one day. The authors suggest that scaling up is helpful, but scaling down is also..." / Twitter
https://mobile.twitter.com/mindkosh/status/1608756646528053250 | Twitter
https://medium.com/geekculture/paper-dive-cramming-training-a-language-model-on-a-single-gpu-in-one-day-7965f47f7e8d | Paper Dive: “Cramming: Training a Language Model on a Single GPU in One Day” | by Tudor Surdoiu | Geek Culture | Medium
https://www.google.com/search?q=deep+boostrap+ICLR+2021&newwindow=1&sxsrf=AJOqlzV19jinmo_PEYBwX6BR877Yw9lXtQ%3A1678400489969&ei=6VsKZInbOtahptQP9--tsAs&ved=0ahUKEwjJ8_qH8c_9AhXWkIkEHfd3C7YQ4dUDCBA&uact=5&oq=deep+boostrap+ICLR+2021&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIHCCEQoAEQCjIHCCEQoAEQCjIHCCEQoAEQCjoECCMQJzoFCC4QkQI6EQguEIMBEMcBELEDENEDEIAEOggIABCxAxCDAToOCC4QgAQQsQMQxwEQ0QM6CwgAEIAEELEDEIMBOgUIABCRAjoFCAAQgAQ6CAguELEDEIMBOgUILhCABDoICC4QgAQQsQM6CwguEIAEELEDEIMBOggIABCABBCxAzoLCC4QgAQQxwEQrwE6DgguEIAEELEDEIMBENQCOgcIABCABBAKOgcILhCABBAKOggIABCABBDLAToGCAAQFhAeOggIABAWEB4QDzoHCAAQDRCABDoGCAAQHhANOggIABAFEB4QDToKCAAQBRAeEA0QCjoICAAQCBAeEA06CggAEAgQHhANEA86BQgAEIYDOgcIIRCrAhAKOgUIIRCrAjoICCEQFhAeEB1KBAhBGAFQ3hRYjjRggzVoA3AAeACAAa8CiAHpEZIBCDIxLjIuMC4xmAEAoAEBwAEB&sclient=gws-wiz-serp | deep boostrap ICLR 2021 - Google Search
https://openreview.net/forum?id=guetrIHLFGI | The Deep Bootstrap Framework Good Online Learners are Good Offline Generalizers - OR-ICLR-2021_guetrIHLFGI
https://openreview.net/pdf?id=guetrIHLFGI | The Deep Bootstrap Framework Good Online Learners are Good Offline Generalizers - OR-ICLR-2021_guetrIHLFGI
https://arxiv.org/pdf/2106.09685.pdf | LoRA Low-Rank Adaptation of Large Language Models - Arxiv-2106.09685
https://the-decoder.com/metas-llama-language-model-shows-that-parameters-are-not-everything/ | Metas "LLaMA" language model shows that parameters are not everything
https://twitter.com/rasbt/status/1629496764808953857 | https://twitter.com/rasbt/status/1629496764808953857
https://twitter.com/yoavgo/status/1629221991797465088 | https://twitter.com/yoavgo/status/1629221991797465088
https://www.google.com/search?q=LLaMA+ppaer&oq=LLaMA+ppaer&aqs=chrome..69i57j0i3i13j0i13i512l5j0i13i30l3.1864j0j1&sourceid=chrome&ie=UTF-8 | LLaMA ppaer - Google Search
https://arxiv.org/pdf/2302.13971.pdf | LLaMA Open and Efficient Foundation Language Models - Arxiv-2302.13971
https://twitter.com/neuro_kim/status/1633866866463592455 | https://twitter.com/neuro_kim/status/1633866866463592455
https://docs.google.com/presentation/d/1evX4saCZ7AA3QWqEJX5G1YOHiKAaZ-4UW3TK6K7dO4k/edit?resourcekey=0-Miu5vScd8wKTuZ7_OcfeZA#slide=id.g138b2e57ee4_0_1420 | RL tutorial [cosyne] - Google 幻灯片
https://colab.research.google.com/drive/1jEiDNA1q98n1Wrw_uBEFvpuV9BGW_yxW | CosyneTutorial_RL_2023_Stachenfeld.ipynb - Colaboratory

https://arxiv.org/pdf/2205.10625.pdf | Least-to-Most Prompting Enables Complex Reasoning in Large Language Models - Arxiv-2205.10625
https://arxiv.org/pdf/2203.11171.pdf | Self-Consistency Improves Chain of Thought Reasoning in Language Models - OR-ICLR-2023_1PL1NIMMrw
https://www.google.com/search?q=1.+Differentiable+prompt+makes+pre-trained+488+language+models+better+few-shot+learners&sourceid=chrome&ie=UTF-8 | 1. Differentiable prompt makes pre-trained 488 language models better few-shot learners - Google Search
https://arxiv.org/abs/2108.13161 | Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners - Arxiv-2108.13161
https://www.google.com/search?q=the+capacity+for+moral+self-correction&oq=the+capacity+for+moral+self-correction&aqs=chrome..69i57j69i64l2.8347j0j1&sourceid=chrome&ie=UTF-8 | the capacity for moral self-correction - Google Search
https://arxiv.org/abs/2302.07459 | The Capacity for Moral Self-Correction in Large Language Models - Arxiv-2302.07459
https://www.google.com/search?q=christopher+potts&oq=christopher+potts&aqs=chrome.0.0i355i512j46i512j0i512l2j46i175i199i512j0i512l2j46i175i199i512j0i512l2.2615j0j1&sourceid=chrome&ie=UTF-8 | christopher potts - Google Search
https://web.stanford.edu/~cgpotts/ | Christopher Potts
https://www.google.com/search?q=diff+pruning&oq=diff+pruning&aqs=chrome.0.0i512j0i22i30l9.1225j0j1&sourceid=chrome&ie=UTF-8 | diff pruning - Google Search
https://yaofu.notion.site/GPT-3-5-360081d91ec245f29029d37b54573756 | 拆解追溯 GPT-3.5 各项能力的起源
https://hub.baai.ac.cn/view/23787 | ChatGPT通俗笔记：从GPT-N、RL之PPO算法到InstructGPT、ChatGPT - 智源社区
https://blog.csdn.net/weixin_42370153/article/details/128678051 | instructGPT的前两阶段核心训练过程pytorch详细代码展示_倪不肉的博客-CSDN博客
https://www.google.com/search?q=ruiqi+zhong&oq=ruiqi+zhong&aqs=chrome..69i57j0i512j0i22i30j0i390.2009j0j1&sourceid=chrome&ie=UTF-8 | ruiqi zhong - Google Search
https://ruiqi-zhong.github.io/ | Ruiqi Zhong
https://arxiv.org/abs/2210.10960 | Diffusion Models already have a Semantic Latent Space - Arxiv-2210.10960
https://openreview.net/forum?id=PS3IMnScugk | Learning to Recombine and Resample Data For Compositional Generalization | OpenReview
https://zhuanlan.zhihu.com/p/594107879 | 2022年度小结：科研、ChatGPT与疫情 - 知乎
https://promptperfect.jina.ai/ | PromptPerfect - Elevate your prompts to perfection
https://www.google.com/search?q=ameet+deshpande&oq=ameet+deshpande&aqs=chrome..69i57j0i22i30l9.2366j0j1&sourceid=chrome&ie=UTF-8 | ameet deshpande - Google Search
https://ameet-1997.github.io/ | Ameet Deshpande
https://www.google.com/search?q=mina+lee&oq=mina+lee&aqs=chrome..69i57j35i39l2j46i67l2j0i131i433i512l2j46i199i433i465i512j46i199i465i512j46i512.829j0j1&sourceid=chrome&ie=UTF-8 | mina lee - Google Search
https://minalee.info/ | Mina Lee – Ph.D. Candidate at Stanford
https://www.google.com/search?q=huaxiu+yao&oq=huaxiu+yao&aqs=chrome..69i57j0i512.1537j0j1&sourceid=chrome&ie=UTF-8 | huaxiu yao - Google Search
https://huaxiuyao.mystrikingly.com/ | Huaxiu Yao's Personal Website on Strikingly
https://twitter.com/search?q=academic%20job%20market&src=typed_query&f=user | academic job market - Twitter Search / Twitter
https://www.google.com/search?q=prithviraj+ammanabrolu&oq=prithviraj+ammanabrolu&aqs=chrome..69i57.5505j0j1&sourceid=chrome&ie=UTF-8 | prithviraj ammanabrolu - Google Search
https://www.google.com/search?q=lei+li&newwindow=1&sxsrf=AJOqlzVnfMJxNYEpGTgbRkFziL9-Yq6Z_g%3A1678395767359&ei=d0kKZLzMFemhptQPiJqHoAE&ved=0ahUKEwj844W838_9AhXpkIkEHQjNARQQ4dUDCBA&uact=5&oq=lei+li&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIECCMQJzILCC4QgAQQsQMQgwEyBAgAEEMyBQgAEIAEMgUILhCABDIFCAAQgAQyBAgAEEMyBQguEIAEMgUIABCABDIFCAAQgAQ6CggAEEcQ1gQQsAM6BwgAELADEENKBAhBGABQhgRYhgRgkAZoAXABeACAAWqIAWqSAQMwLjGYAQCgAQHIAQnAAQE&sclient=gws-wiz-serp | lei li - Google Search
https://sites.cs.ucsb.edu/~lilei/student.html | Lei LI's Software
https://sites.cs.ucsb.edu/~william/ | William Wang, UC Santa Barbara Computer Science
https://sites.cs.ucsb.edu/~yuxiangw/ | Yu-Xiang WANG's Homepage
https://code-terminator.github.io/ | Shiyu Chang | UC Santa Barbara

https://www.csail.mit.edu/events | Events | MIT CSAIL

https://proceedings.neurips.cc/paper/2001/file/7b7a53e239400a13bd6be6c91c4f6c4e-Paper.pdf | On Discriminative vs. Generative Classifiers A comparison of logistic regression and naive Bayes - NeurIPS-2001_7b7a53e2
https://arxiv.org/pdf/2302.13007.pdf | ChatAug Leveraging ChatGPT for Text Data Augmentation - Arxiv-2302.13007
https://www.bilibili.com/video/BV1T8411T7yw/?spm_id_from=333.337.search-card.all.click&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | 为什么我们活得这么累？一个视频，看懂异化理论千年流变_哔哩哔哩_bilibili
https://arxiv.org/pdf/2303.01469.pdf | Consistency Models - Arxiv-2303.01469
https://arxiv.org/pdf/2303.01580.pdf | Mixture of Soft Prompts for Controllable Data Generation - Arxiv-2303.01580

https://n.derek.ma/2_literature/RLHF | RLHF - Derek Ma
https://arxiv.org/pdf/2204.05239.pdf | Exploring the Universal Vulnerability of Prompt-based Learning Paradigm | PDF
https://arxiv.org/pdf/2012.01274.pdf | How Robust are Randomized Smoothing based Defenses to Data Poisoning? | PDF
https://github.com/akshaymehra24/PoisoningCertifiedDefenses | akshaymehra24/PoisoningCertifiedDefenses: How Robust are Randomized Smoothing based Defenses to Data Poisoning? (CVPR 2021)
https://github.com/kohpangwei/data-poisoning-release/blob/master/generate_or_process_bounds.py | data-poisoning-release/generate_or_process_bounds.py at master · kohpangwei/data-poisoning-release
https://aclanthology.org/2021.findings-emnlp.369.pdf | https://aclanthology.org/2021.findings-emnlp.369.pdf
https://github.com/neulab/RIPPLe/issues/6 | Run Error · Issue #6 · neulab/RIPPLe
https://github.com/neulab/RIPPLe/blob/paul_refactor/manifestos/example_manifesto.yaml | RIPPLe/example_manifesto.yaml at paul_refactor · neulab/RIPPLe
https://proceedings.mlr.press/v120/zhang20b.html | Online Data Poisoning Attacks
https://openreview.net/pdf?id=v6UimxiiR78 | https://openreview.net/pdf?id=v6UimxiiR78
https://github.com/lightly-ai/lightly | lightly-ai/lightly: A python library for self-supervised learning on images.
https://arxiv.org/pdf/2004.07401.pdf | Poisoning Attacks on Algorithmic Fairness - Arxiv-2004.07401
https://arxiv.org/pdf/2105.12837.pdf | Fooling Partial Dependence via Data Poisoning | PDF
https://openreview.net/forum?id=rYLMJ6zX3RF | [Re] Exacerbating Algorithmic Bias through Fairness Attacks | OpenReview
https://openreview.net/pdf?id=rYLMJ6zX3RF | https://openreview.net/pdf?id=rYLMJ6zX3RF
https://github.com/toliz/fairness-attacks | toliz/fairness-attacks: Re-implementation of the paper "Exacerbating Algorithmic Bias through Fairness Attacks"
https://www.google.com/search?q=Adversarial+example+generation+with+syntactically+controlled+paraphrase+networks&sourceid=chrome&ie=UTF-8 | Adversarial example generation with syntactically controlled paraphrase networks - Google Search
https://arxiv.org/pdf/2011.10369.pdf | ONION: A Simple and Effective Defense Against Textual Backdoor Attacks | PDF
https://github.com/leileigan/clean_label_textual_backdoor_attack | leileigan/clean_label_textual_backdoor_attack
https://github.com/leileigan/clean_label_textual_backdoor_attack/blob/master/scripts/run_bert_sst_samples_gen.sh | clean_label_textual_backdoor_attack/run_bert_sst_samples_gen.sh at master · leileigan/clean_label_textual_backdoor_attack
https://github.com/leileigan/clean_label_textual_backdoor_attack/blob/master/attack/poison_examples_gen.py | clean_label_textual_backdoor_attack/poison_examples_gen.py at master · leileigan/clean_label_textual_backdoor_attack
https://github.com/leileigan/clean_label_textual_backdoor_attack/blob/56e3a96f6a4eeaf30b90a275685f37cc7e7b3c7c/attack/poison_examples_gen.py#L275 | clean_label_textual_backdoor_attack/poison_examples_gen.py at 56e3a96f6a4eeaf30b90a275685f37cc7e7b3c7c · leileigan/clean_label_textual_backdoor_attack
https://github.com/leileigan/clean_label_textual_backdoor_attack/blob/56e3a96f6a4eeaf30b90a275685f37cc7e7b3c7c/OpenAttack/attackers/genetic.py#L96 | clean_label_textual_backdoor_attack/genetic.py at 56e3a96f6a4eeaf30b90a275685f37cc7e7b3c7c · leileigan/clean_label_textual_backdoor_attack
https://openreview.net/pdf?id=H4lzChGmhCK | https://openreview.net/pdf?id=H4lzChGmhCK
https://openreview.net/forum?id=H4lzChGmhCK | On the reproducibility of "Exacerbating Algorithmic Bias through Fairness Attacks" | OpenReview
https://www.google.com/search?q=On+the+reproducibility+of+%22Exacerbating+Algorithmic+Bias+through+Fairness+Attacks%22&sourceid=chrome&ie=UTF-8 | On the reproducibility of "Exacerbating Algorithmic Bias through Fairness Attacks" - Google Search
https://arxiv.org/pdf/2012.08723.pdf | Exacerbating Algorithmic Bias through Fairness Attacks | PDF
https://github.com/kohpangwei/influence-release | kohpangwei/influence-release
https://aclanthology.org/2020.acl-main.249.pdf | https://aclanthology.org/2020.acl-main.249.pdf
https://github.com/wronnyhuang/metapoison/blob/master/main.py | metapoison/main.py at master · wronnyhuang/metapoison
https://github.com/wronnyhuang/metapoison/blob/master/meta.py | metapoison/meta.py at master · wronnyhuang/metapoison
https://arxiv.org/pdf/2106.01494.pdf | Knowing More About Questions Can Help: Improving Calibration in Question Answering | PDF
https://arxiv.org/pdf/2004.00225.pdf | MetaPoison: Practical General-purpose Clean-label Data Poisoning | PDF
https://arxiv.org/pdf/2005.00191.pdf | Bullseye Polytope: A Scalable Clean-Label Poisoning Attack with Improved Transferability | PDF
https://openreview.net/pdf?id=SJeYe0NtvH | https://openreview.net/pdf?id=SJeYe0NtvH
https://arxiv.org/pdf/2202.11203.pdf | Label-Smoothed Backdoor Attack | PDF
https://arxiv.org/pdf/2103.15543.pdf | Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models | PDF
https://github.com/wronnyhuang/metapoison#algorithm-overview | wronnyhuang/metapoison: Craft poisoned data using MetaPoison
https://aclanthology.org/2020.conll-1.48.pdf | https://aclanthology.org/2020.conll-1.48.pdf
https://arxiv.org/abs/2212.10717 | [2212.10717] Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks
https://arxiv.org/abs/1903.09860 | [1903.09860] Data Poisoning against Differentially-Private Learners: Attacks and Defenses
https://worksheets.codalab.org/worksheets/0xbdd35bdd83b14f6287b24c9418983617/ | CodaLab Worksheets
https://arxiv.org/pdf/1706.03691.pdf | Certified Defenses for Data Poisoning Attacks | PDF
https://www.google.com/search?q=certified+defense+for+data+poisioning+attacks&newwindow=1&sxsrf=ALiCzsZAvWLMkcNthxxT8acwMJQiWs7QoQ%3A1672965002579&ei=imu3Y77pIvuk5NoPkPWWuAo&ved=0ahUKEwi-1syl2LH8AhV7ElkFHZC6BacQ4dUDCBA&uact=5&oq=certified+defense+for+data+poisioning+attacks&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIGCAAQFhAeMgUIABCGAzIFCAAQhgMyBQgAEIYDMgUIABCGAzoKCAAQRxDWBBCwAzoECCMQJzoFCAAQgAQ6BQgAEKIEOgcIABAeEKIEOgUIIRCgAToFCCEQqwI6CAghEBYQHhAdOgoIIRAWEB4QDxAdOgcIIRCgARAKSgQIQRgASgQIRhgAUIADWNIxYJ0yaAFwAXgBgAGaAYgB1h2SAQUxMS4yNJgBAKABAcgBCMABAQ&sclient=gws-wiz-serp | certified defense for data poisioning attacks - Google Search
https://arxiv.org/abs/1706.03691 | [1706.03691] Certified Defenses for Data Poisoning Attacks
https://arxiv.org/pdf/1505.05424.pdf | https://arxiv.org/pdf/1505.05424.pdf
https://arxiv.org/pdf/1708.06733.pdf | https://arxiv.org/pdf/1708.06733.pdf
https://arxiv.org/pdf/1912.02292.pdf | https://arxiv.org/pdf/1912.02292.pdf
https://arxiv.org/abs/2007.08432 | [2007.08432] Data Poisoning Attacks Against Federated Learning Systems
https://github.com/Cinofix/beta_poisoning | Cinofix/beta_poisoning: Official implementation of 'The Hammer and the Nut: Is Bilevel Optimization Really Needed to Poison Linear Classifiers?' [Submitted to IJCNN 2021]
https://arxiv.org/pdf/2206.12654.pdf | https://arxiv.org/pdf/2206.12654.pdf
https://arxiv.org/pdf/2110.07831.pdf | https://arxiv.org/pdf/2110.07831.pdf
https://github.com/THUYimingLi/backdoor-learning-resources | THUYimingLi/backdoor-learning-resources: A list of backdoor learning resources
https://github.com/THUYimingLi/BackdoorBox | THUYimingLi/BackdoorBox: The open-sourced Python toolbox for backdoor attacks and defenses.
https://arxiv.org/pdf/2201.02993.pdf | https://arxiv.org/pdf/2201.02993.pdf
https://arxiv.org/pdf/2206.01832.pdf | https://arxiv.org/pdf/2206.01832.pdf
https://arxiv.org/pdf/2202.05749.pdf | https://arxiv.org/pdf/2202.05749.pdf
https://arxiv.org/abs/2111.14309 | [2111.14309] A General Framework for Defending Against Backdoor Attacks via Influence Graph
https://web.cs.ucdavis.edu/~hpirsiav/papers/backdoor_ssl_cvpr22.pdf | https://web.cs.ucdavis.edu/~hpirsiav/papers/backdoor_ssl_cvpr22.pdf
https://arxiv.org/pdf/1912.02771/ | https://arxiv.org/pdf/1912.02771/
https://github.com/thunlp/OpenBackdoor | thunlp/OpenBackdoor: An open-source toolkit for textual backdoor attack and defense (NeurIPS 2022 D&B, Spotlight)
https://arxiv.org/abs/1911.07116 | https://arxiv.org/abs/1911.07116
https://arxiv.org/pdf/2111.08429.pdf | https://arxiv.org/pdf/2111.08429.pdf
https://arxiv.org/pdf/2111.14309.pdf | https://arxiv.org/pdf/2111.14309.pdf
https://arxiv.org/pdf/2010.08138.pdf | https://arxiv.org/pdf/2010.08138.pdf
https://arxiv.org/abs/1708.03999 | https://arxiv.org/abs/1708.03999
https://arxiv.org/pdf/2201.10055.pdf | https://arxiv.org/pdf/2201.10055.pdf
https://aclanthology.org/2022.acl-long.386/ | https://aclanthology.org/2022.acl-long.386/
https://arxiv.org/pdf/2104.09667.pdf | https://arxiv.org/pdf/2104.09667.pdf
https://www.google.com/search?q=model+extraction+attacks&newwindow=1&sxsrf=APq-WBu4aWF2mc_RjTaQeUsTFpfXgX59aQ%3A1646786838384&ei=FvknYtWDF6CP9PwPw_qXwAw&ved=0ahUKEwjV5paE57f2AhWgB50JHUP9BcgQ4dUDCA8&uact=5&oq=model+extraction+attacks&gs_lcp=Cgdnd3Mtd2l6EAMyBQgAEIAEMgUIABCABDIFCAAQgAQyBggAEBYQHjoHCAAQRxCwA0oECEEYAEoECEYYAFDmAljZGmCcG2gCcAF4AIAB7AKIAbQTkgEHMC4zLjcuMZgBAKABAcgBBcABAQ&sclient=gws-wiz | model extraction attacks - Google Search
https://github.com/Lorraine333/smoothed_box_embedding | Lorraine333/smoothed_box_embedding: smoothed box embedding code
https://github.com/yasumasaonoe/Box4Types/blob/main/box4et/README.md | Box4Types/README.md at main · yasumasaonoe/Box4Types

https://www.bilibili.com/video/BV1h54y1u74A/?spm_id_from=333.1007.tianma.1-3-3.click&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | 这部名作不仅骗过全部观众！还让侦探沦为犯人的玩物？！其讲述何为真正的骗局！_哔哩哔哩_bilibili
https://www.bilibili.com/video/BV1JM4y1Z7Bt/?spm_id_from=333.1007.tianma.2-2-5.click&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | 我恨《混沌武士》，它剥夺了我忍耐平庸动画的能力【银屏系】丨机核_哔哩哔哩_bilibili

https://book.douban.com/subject/3121820/ | 殉教カテリナ車輪 (豆瓣)
https://book.douban.com/subject/5986239/ | 砂漠の薔薇 (豆瓣)
https://book.douban.com/subject/3077658/comments/ | ミステリ・オペラ―宿命城殺人事件 短评
https://lockedroom.net/blog/?p=1632 | 山田正紀『ミステリ・オペラ―宿命城殺人事件』(2001) | Fang's Mystery Blog

https://arxiv.org/pdf/2212.10560.pdf | Self-Instruct Aligning Language Model with Self Generated Instructions - Arxiv-2212.10560
https://twitter.com/SeonghyeonYe/status/1580170684466360323 | (1) Seonghyeon Ye on Twitter: "*Flipping* the instruction and label space makes stronger zero-shot LM! 🙃 0-shot 🌟 FLIPPED (3B, 11B) 🌟 outperforms 0-shot T0 (11B), 3-shot GPT3 (175B), 0-shot PaLM (540B) on BIG-bench. 💪 [1/9] w/@Doe_Young_Kim @jang_yoel Joongbo Shin @seo_minjoon 📜: https://t.co/IYF1FLtape https://t.co/LyOmxQlg6l" / Twitter
https://github.com/bigscience-workshop/promptsource | bigscience-workshop/promptsource: Toolkit for creating, sharing and using natural language prompts.
https://www.zhihu.com/search?q=beit&type=content | (42 封私信 / 80 条消息) beit - 搜索结果 - 知乎
https://arxiv.org/pdf/2302.12173.pdf | More than you've asked for A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models - Arxiv-2302.12173
https://github.com/greshake/llm-security | greshake/llm-security: New ways of breaking app-integrated LLMs
https://people.csail.mit.edu/cloudygoose/ | Tianxing He
https://twitter.com/TianxingH | (1) Tianxing He (@TianxingH) / Twitter
https://tsvetshop.github.io/people.html | : People
https://homes.cs.washington.edu/~yuliats/ | Yulia Tsvetkov
https://www.google.com/search?q=goosehe+mit&oq=goosehe+mit&aqs=chrome..69i57j0i546l5.2850j0j1&sourceid=chrome&ie=UTF-8 | goosehe mit - Google Search
https://arxiv.org/pdf/2212.10020.pdf | On the Blind Spots of Model-Based Evaluation Metrics for Text Generation - Arxiv-2212.10020
https://question406.github.io/ | Jiabao Ji (计家宝)
https://code-terminator.github.io/ | Shiyu Chang | UC Santa Barbara
https://www.google.com/search?q=graham+neubig&oq=graham+neubig&aqs=chrome.0.0i355i512j46i512j0i512l7j0i390.3552j0j1&sourceid=chrome&ie=UTF-8 | graham neubig - Google Search
https://scholar.google.com/citations?hl=en&user=r21asW4AAAAJ&view_op=list_works&sortby=pubdate | ‪Shiyu Chang‬ - ‪Google Scholar‬
https://code-terminator.github.io/index.html#students | Shiyu Chang | UC Santa Barbara
https://people.eecs.berkeley.edu/~klein/ | Dan Klein's Home Page
http://sameersingh.org/group.html | Sameer Singh: Group
https://www.cs.washington.edu/people/faculty/lsz/ | Luke Zettlemoyer | Paul G. Allen School of Computer Science & Engineering
https://arxiv.org/pdf/2110.04366.pdf | Towards a Unified View of Parameter-Efficient Transfer Learning - Arxiv-2110.04366
https://scholar.google.com/citations?hl=en&user=UjpbO6IAAAAJ&view_op=list_works&sortby=pubdate | ‪Luke Zettlemoyer‬ - ‪Google Scholar‬
https://scholar.google.com/citations?user=euc0GX4AAAAJ&hl=en | ‪Karthik Narasimhan‬ - ‪Google Scholar‬
http://rush-nlp.com/ | Main
https://yoonholee.com/ | Yoonho Lee
https://scholar.google.com/citations?hl=en&user=vfPE6hgAAAAJ&view_op=list_works&sortby=pubdate | ‪Chelsea Finn‬ - ‪Google Scholar‬
https://irislab.stanford.edu//people.html | People
https://scholar.google.com/citations?hl=en&user=euc0GX4AAAAJ&view_op=list_works&sortby=pubdate | ‪Karthik Narasimhan‬ - ‪Google Scholar‬
https://scholar.google.com/scholar?cites=17392009296900150762&as_sdt=40000005&sciodt=0,22&hl=en | Google Scholar
https://www.cs.utexas.edu/~eunsol/html_pages/group.html | Eunsol Choi
https://scholar.google.com/citations?hl=en&user=kV9XRxYAAAAJ&view_op=list_works&sortby=pubdate | ‪Samuel R. Bowman‬ - ‪Google Scholar‬

https://www.cs.princeton.edu/~karthikn/ | Karthik Narasimhan
https://arxiv.org/abs/2202.09318 | DataMUX Data Multiplexing for Neural Networks - Arxiv-2202.09318
https://people.csail.mit.edu/fisch/ | Adam Fisch
https://people.csail.mit.edu/fisch/assets/pdf/research.pdf | research.pdf
https://people.csail.mit.edu/tommi/ | Tommi Jaakkola
https://ysymyth.github.io/ | About – Shunyu Yao – 姚顺雨
https://scholar.google.com/citations?user=qJBXk9cAAAAJ | ‪Shunyu Yao‬ - ‪Google Scholar‬
https://twitter.com/ShunyuYao12 | https://twitter.com/ShunyuYao12
https://scholar.google.com/citations?hl=en&user=-hGZC54AAAAJ&view_op=list_works&sortby=pubdate | ‪Sameer Singh‬ - ‪Google Scholar‬
https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=euc0GX4AAAAJ&sortby=pubdate | ‪Karthik Narasimhan‬ - ‪Google Scholar‬
https://mila.quebec/en/person/hugo-larochelle/ | Hugo Larochelle - Mila
https://scholar.google.ca/citations?hl=en&user=U89FHq4AAAAJ&view_op=list_works&sortby=pubdate | ‪Hugo Larochelle‬ - ‪Google Scholar‬
https://scholar.google.com/citations?user=gnox0EsAAAAJ&hl=en | ‪Runzhe Yang‬ - ‪Google Scholar‬
https://twitter.com/karthik_r_n | https://twitter.com/karthik_r_n
https://scholar.google.com/citations?user=LYRkQhMAAAAJ&hl=en | ‪Adam Fisch‬ - ‪Google Scholar‬
https://www.google.com/search?q=Jeremy%20Wohlwend%20%20 | Jeremy Wohlwend - Google Search
https://twitter.com/adamjfisch?lang=en | https://twitter.com/adamjfisch?lang=en
https://robinjia.github.io/ | Robin Jia
https://www.google.com/search?q=usc+faculty&oq=usc+faul&aqs=chrome.1.69i57j0i10i512l9.3149j0j1&sourceid=chrome&ie=UTF-8 | usc faculty - Google Search

https://yoavartzi.com/ | Yoav Artzi
https://scholar.google.com/citations?user=XuQW7ogAAAAJ&hl=en | ‪Yoav Artzi‬ - ‪Google Scholar‬
https://www.google.com/search?q=UW+cs+faulty&oq=UW&aqs=chrome.0.69i59j69i57j35i39j69i59j46i131i199i433i465i512j0i131i433i512j69i60l2.2167j0j1&sourceid=chrome&ie=UTF-8 | UW cs faulty - Google Search
https://scholar.google.com/citations?hl=en&user=UjpbO6IAAAAJ&view_op=list_works&sortby=pubdate | ‪Luke Zettlemoyer‬ - ‪Google Scholar‬
https://nasmith.github.io/ | Noah A. Smith
https://noahs-ark.github.io/people/#phd | Researchers in Noah’s ARK - Noah’s ARK
https://noahs-ark.github.io/people/#postdoc | Researchers in Noah’s ARK - Noah’s ARK
https://maartensap.com/ | Maarten Sap - Home
https://scholar.google.com/citations?hl=en&user=gFN4QUYAAAAJ&view_op=list_works&sortby=pubdate | ‪Maarten Sap‬ - ‪Google Scholar‬
https://swabhs.com/ | Swabha Swayamdipta
https://jflanigan.github.io/index.html | Jeffrey Flanigan
https://people.ischool.berkeley.edu/~dbamman/ | David Bamman
http://brenocon.com/ | Brendan T. O'Connor - UMass Amherst, Computer Science
https://people.cs.georgetown.edu/nschneid/ | Nathan Schneider
https://noahs-ark.github.io/people/#masters | Researchers in Noah’s ARK - Noah’s ARK
https://noahs-ark.github.io/people/#other | Researchers in Noah’s ARK - Noah’s ARK
https://personal.ntu.edu.sg/wangwy/ | Wenya WANG (NTU)
https://homes.cs.washington.edu/~hannaneh/ | Hannaneh Hajishirzi - University of Washington
https://noahs-ark.github.io/ | About Noah’s ARK - Noah’s ARK
https://ofir.io/ | Ofir Press
https://homes.cs.washington.edu/~rahuln/ | https://homes.cs.washington.edu/~rahuln/
https://alisawuffles.github.io/ | Alisa Liu
https://www.amazon.science/search | Search - Amazon Science
https://homes.cs.washington.edu/~jkasai/ | Jungo
https://yushi-hu.github.io/ | Yushi Hu
https://twitter.com/yanaiela | https://twitter.com/yanaiela
https://blog.allenai.org/from-interviewee-to-interviewer-68c36593b305 | From Interviewee To Interviewer - A New Type of Odyssey | by Yanai Elazar | AI2 Blog
https://ranjaykrishna.com/index.html | Ranjay Krishna - Home
https://sarahwie.github.io/ | Sarah Wiegreffe – Personal website (in the works)
https://sneha-rk.github.io/ | Sneha Kudugunta's Website
https://dwadden.github.io/ | David Wadden
https://homes.cs.washington.edu/~mrsalehi/ | Reza Salehi
https://bhargaviparanjape.github.io/ | Bhargavi Paranjape – PhD student at University of Washington
https://liujch1998.github.io/ | Jiacheng (Gary) Liu
https://homes.cs.washington.edu/~yizhongw/ | Yizhong Wang - University of Washington
http://ellenmellon.github.io/ | About Me | Ellen (Zeqiu) Wu
https://danielkhashabi.com/index.html#themes | Daniel Khashabi
https://www.clsp.jhu.edu/faculty/ | Faculty - Center for Language and Speech Processing
https://www.cis.upenn.edu/~ccb/ | Chris Callison-Burch

http://www.ikonstas.net/ | Yannis Konstas - Research
http://www.janmbuys.com/ | Jan Buys
https://yonatanbisk.com/ | Yonatan Bisk
https://rudinger.github.io/ | Rachel Rudinger
https://jmhessel.com/ | Jack Hessel's Homepage
https://www.cs.ubc.ca/~vshwartz/ | Vered Shwartz - Department of Computer Science - UBC
https://swabhs.com/ | Swabha Swayamdipta
https://danielkhashabi.com/ | Daniel Khashabi
https://yj-yu.github.io/home/ | Youngjae Yu. Personal Homepage
https://maartensap.com/ | Maarten Sap - Home
https://atcbosselut.github.io/ | Antoine Bosselut
https://hrashkin.github.io/ | https://hrashkin.github.io
https://people.cs.umass.edu/~xiangl/ | Xiang Lorraine Li
https://scholar.google.com/citations?hl=en&user=SRgRwSoAAAAJ | ‪Xiang Lorraine Li‬ - ‪Google Scholar‬
http://prithvirajva.com/ | Prithviraj (Raj) Ammanabrolu
http://people.csail.mit.edu/tommi/ | Tommi Jaakkola
http://people.csail.mit.edu/tommi/people.html | Tommi Jaakkola
https://dmelis.github.io/ | David Alvarez-Melis | Home
https://scholar.google.com/citations?user=XsxZrYYAAAAJ&hl=en | ‪David Alvarez-Melis‬ - ‪Google Scholar‬
https://thashim.github.io/ | Tatsunori Hashimoto | Home

https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/ | CSC2541 Winter 2022
https://arxiv.org/pdf/2205.05638.pdf | Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning - Arxiv-2205.05638
https://arxiv.org/pdf/2208.11857.pdf | Shortcut Learning of Large Language Models in Natural Language Understanding A Survey - Arxiv-2208.11857
https://piazza.com/mit/spring2023/67830/resources | 6.7830 | Class Profile | Piazza
https://piazza.com/class_profile/get_resource/ldjefz5a4yu69n/leogz9qsbffkp | leogz9qsbffkp
https://github.com/dangkhoasdc/awesome-ai-residency | dangkhoasdc/awesome-ai-residency: List of AI Residency Programs
http://www.cs.toronto.edu/~rgrosse/teaching.html | www.cs.toronto.edu/~rgrosse/teaching.html
https://subercui.github.io/csc2515/Lectures.html | CSC2515 Winter 2021- University of Toronto Computer Science
https://duvenaud.github.io/sta414/ | STA414
https://jsc370.github.io/ | JSC370 Data Science
https://probmlcourse.github.io/csc412/ | CSC412 Winter 2020: Probabilsitic Machine Learning
https://duvenaud.github.io/learn-discrete/ | Learning Discrete Latent Structure
http://www.cs.toronto.edu/~duvenaud/courses/csc2541/index.html | index
https://duvenaud.github.io/learning-to-search/ | Learning to Search
https://www.cs.toronto.edu/~cmaddis/courses/sta4273_w21/ | Minimizing Expectations
https://aclanthology.org/2022.acl-long.429.pdf | Can Explanations Be Useful for Calibrating Black Box Models? - ACL-ACL-2022_2022.acl-long.429
https://www.mitgenaisummit.com/s-projects-side-by-side | Agenda & Speakers | MIT GenAI Summit
https://web.mit.edu/webcast/mitgenaisummit/s23/ | LIVE WEBCAST | MIT Generative AI Summit Live Webcast

https://scholar.google.com/citations?user=QMkbFp8AAAAJ&hl=en | ‪Shibani Santurkar‬ - ‪Google Scholar‬
https://arxiv.org/pdf/1811.02553.pdf | A Closer Look at Deep Policy Gradients - Arxiv-1811.02553
https://people.eecs.berkeley.edu/~klein/ | Dan Klein's Home Page
https://jsteinhardt.stat.berkeley.edu/ | Jacob Steinhardt
https://scholar.google.com/citations?hl=en&user=LKv32bgAAAAJ&view_op=list_works&sortby=pubdate | ‪Jacob Steinhardt‬ - ‪Google Scholar‬
https://thashim.github.io/ | Tatsunori Hashimoto | Home
https://mlfoundations.org/#opportunities | Harvard ML Foundations
https://carat.fas.harvard.edu/applicant/apply?type=A&publicSite=N | CARAT
http://web.mit.edu/jda/www/teaching/6.884/ | Neuro-symbolic Models for NLP (6.884)
http://web.mit.edu/jda/www/teaching/6.864/ | web.mit.edu/jda/www/teaching/6.864/
https://web.stanford.edu/~jurafsky/ | Dan Jurafsky - Home Page
https://homes.cs.washington.edu/~yuliats/ | Yulia Tsvetkov
https://people.csail.mit.edu/cloudygoose/papers/defense.pdf | defense.pdf
http://tensorlab.cms.caltech.edu/users/anima/ | Anima AI + Science Lab
https://www.google.com/search?q=anima+anandkumar&oq=Anima+Anandkumar&aqs=chrome.0.0i131i355i433i512j46i131i433i512j0i67j0i512l4j69i60.239j0j1&sourceid=chrome&ie=UTF-8 | anima anandkumar - Google Search
http://tensorlab.cms.caltech.edu/users/anima/group.html | Anima AI + Science Lab
http://starai.cs.ucla.edu/members/ | UCLA StarAI Lab - Members
https://www.google.com/search?q=Guy%20Van%20den%20Broeck | Guy Van den Broeck - Google Search
https://www.google.com/search?q=%0A%0AFollow%0ALianhui%20Qin | Follow Lianhui Qin - Google Search
https://web.cs.ucla.edu/~weiwang/ | Wei Wang's Home Page
https://khhuang.me/ | Kuan-Hao Huang

https://dill-lab.github.io/ | DILL Lab 🌿
https://swabhs.com/ | Swabha Swayamdipta
https://robinjia.github.io/ | Robin Jia
https://nlp.usc.edu/ | USC NLP
https://sghosh73.github.io/ | Sayan Ghosh
https://scholar.google.co.uk/citations?hl=en&user=IBlMTLwAAAAJ&view_op=list_works&sortby=pubdate | ‪Dani Yogatama‬ - ‪Google Scholar‬
https://watermark.silverchair.com/tacl_a_00476.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAArowggK2BgkqhkiG9w0BBwagggKnMIICowIBADCCApwGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMrMWUP0kg7j0vgkcSAgEQgIICbY1GYlKAzckFFbfgbgEWAG9XIzMahpcJkW648VAKfnqsP91mu3AlFOpcXZmDMifWoBCMjk4afPcXlnAaU86M8zXxBh0vvVk28eQb2BLn1Yrg1PkGlyUg_Y3fIw8EYGurfvXTxTih41u4fIZhyzu63TCLvWAp-K1iLaUtT20VTwJmf57ovW2rCD1hM-ZgA75QXdYalpXdp6tLtQZOeSOpZgxeER5wIzX11w4ITjSF-TxFFQvc9V-0zaEyNf0Mwbx1Zmp9GWbsr6rqZkABgTuxhLNRkO8vFFuQDdnB5_6am77SbQpZ7wKa33WDrQTHCDAD54sWMDkfou45EBxeZNiaAqT2JTn1-xraI2vtw9OR-i2EV_Bpj3HEnu1XpdbzKuuCjs5j6sslrp8Q1_3dxiUA3B_elaKk9IId8RMrfyWKUrs40UaTllYecy-NK2d2TxMwP0Vbgkfs2Lxjk-d3NGWisl6AYVrIrtSc9WxLR_HUtGvj5xdNnDTjXtCmjMVgmj-wsKQwnRdZrw8jfhAEdMhiMIkSJLIZCHk7gZPRnQcex8IenBGuQiYEHhQC9irriZYTa96SFWiIhFInUGdteuv3KK9Iq_VuKixDNNB15aT7j5BsgVhPO0LaV0oORVb3iSlXV9d7XJ0pzq88gUUkOpiaCtqENTEeyrSo8d-w7grSWTg-lB5Z7FpB7Pbfxz4uvaRWblMOyiTvIijtqOSZHU7XV8Dk-qOagb8255ss6DmKacIJKIhiWQ2sdiQwN3OgUwsZIe5xMxtsTulSOsO2DR4ofU4RVxt3zuJGMFuKjbR0hxGjUIJobS27PS-f3TFu1A | tacl_a_00476.pdf
https://arxiv.org/pdf/2203.01311.pdf | HighMMT Towards Modality and Task Generalization for High-Modality Representation Learning - Arxiv-2203.01311
https://www.google.com/search?q=Joshua%20Robinson | Joshua Robinson - Google Search
https://arxiv.org/pdf/2212.10378.pdf | Careful Data Curation Stabilizes In-context Learning - Arxiv-2212.10378
https://arxiv.org/pdf/2211.15718.pdf | CoNAL Anticipating Outliers with Large Language Models - Arxiv-2211.15718
https://www.1point3acres.com/bbs/thread-969179-1-1.html | 1st rej stanford|一亩三分地录取汇报：研究生版
https://posts.careerengine.us/p/60fb91c8bde42948f9b51064 | 申请北美博士，暑研竟然这么加分？
https://lijinzhang.com/post/2022-03-03-tutorial-phd-app/ | 北美博士申请攻略 - Lijin Zhang
https://github.com/dangkhoasdc/awesome-ai-residency | dangkhoasdc/awesome-ai-residency: List of AI Residency Programs
https://dyogatama.github.io/ | Dani Yogatama
https://leuchine.github.io/ | Qi Liu's Homepage

https://arxiv.org/pdf/2204.07705.pdf | Super-NaturalInstructions Generalization via Declarative Instructions on 1600+ NLP Tasks - Arxiv-2204.07705
https://instructions.apps.allenai.org/ | Learning From Instructions
https://arxiv.org/pdf/2205.03401.pdf | The Unreliability of Explanations in Few-Shot In-Context Learning - Arxiv-2205.03401
https://arxiv.org/abs/2010.05607 | The elephant in the interpretability room Why use attention as explanation when we have saliency methods? - Arxiv-2010.05607
https://arxiv.org/pdf/2204.02329.pdf | Can language models learn from explanations in context? - Arxiv-2204.02329

https://www.reddit.com/r/MachineLearning/comments/1080c3d/p_evaluating_several_topic_modeling/ | (2) [P] Evaluating several topic modeling implementations. What's the current best practice? BERTopic? OpenAI Ada-002? : MachineLearning
https://github.com/MilaNLProc/contextualized-topic-models#id9 | MilaNLProc/contextualized-topic-models: A python package to run contextualized topic modeling. CTMs combine contextualized embeddings (e.g., BERT) with topic models to get coherent topics. Published at EACL and ACL 2021.
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247529719&idx=1&sn=e282c8dc4a235992241813f9b88e7e7c&exportkey=n_ChQIAhIQzlxFJyQcqV3DdMuHZ6eL8BKWAgIE97dBBAEAAAAAADsTAQcK5esAAAAOpnltbLcz9gKNyK89dVj0Gw%2FaIHS%2ByF8kjmlLyB%2FHHjuvW%2BIDTyXDFNiPKDxIn%2BiStHRu0yl1dtD61NZfa4fhWHte%2FdvfP9vBYPYKuCc3b%2B%2BA0UEvXu24%2B0T%2BMz3DltccB%2F7cFwzli%2BM4xP9Oj4ExyczYCLXd4PFwSeoT87TxoL6FBmF%2BR2U9OKfHmFX%2B%2BIbJY3fBUsoUQSoAKsg3X2oUDSQVxRF1akGRx%2FG1pA1%2BfYdm6T7ST5fQ8GwzHQ%2BXd4EtTrkQI5hVK3Ah6j%2FJPCYUUaUr1ZA7luYsXuWU4NBipVj6BRHL2%2BlqAGnGns49njsmZvI1KN1jKpRdQaTUef16&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q0xYaDUhGjWF2iTn%2BxYLPlk&wx_header=0 | BERTopic：NLP主题模型的未来！
https://maartengr.github.io/BERTopic/api/bertopic.html | BERTopic - BERTopic
https://twitter.com/nazneenrajani/status/1618737741453479939 | (1) Nazneen Rajani on Twitter: "You can create your own chatbot by fine-tuning pre-trained causal LLM to follow instructions 🤖 Here is a list of datasets on @huggingface hub that you can use for Instruction fine-tuning (IFT) 🧵 /0 https://t.co/uumHYdbQ1S" / Twitter
https://github.com/allenai/natural-instructions | allenai/natural-instructions: Expanding natural instructions
https://github.com/allenai/natural-instructions/pull/557 | print contributed authors by danyaljj · Pull Request #557 · allenai/natural-instructions
https://github.com/yizhongw/Tk-Instruct | yizhongw/Tk-Instruct: Tk-Instruct is a Transformer model that is tuned to solve many NLP tasks by following instructions.
https://instructions.apps.allenai.org/ | Learning From Instructions
https://www.paperdigest.org/2023/01/recent-papers-on-chatgpt/ | Paper Digest: Recent Papers on ChatGPT – Paper Digest
https://shamulent.github.io/CS_Stat184_Fall22.html | CS/Stat 184 Intro to RL
https://arxiv.org/pdf/2109.07830.pdf | Reframing Instructional Prompts to GPTk's Language - Arxiv-2109.07830
https://arxiv.org/pdf/2205.00049.pdf | Prompt Consistency for Zero-Shot Task Generalization - Arxiv-2205.00049

https://arxiv.org/pdf/2204.05239.pdf | Exploring the Universal Vulnerability of Prompt-based Learning Paradigm - Arxiv-2204.05239
https://arxiv.org/pdf/2211.01910.pdf | Large Language Models Are Human-Level Prompt Engineers - Arxiv-2211.01910
https://arxiv.org/pdf/2302.07842.pdf | Augmented Language Models a Survey - Arxiv-2302.07842
https://scholar.google.com/scholar?q=+Memorization+without+overftiting:+Analyzing+the+training+dynamics+of+large+language+models&hl=en&as_sdt=0,22 | Memorization without overftiting: Analyzing the... - Google Scholar
https://arxiv.org/pdf/2205.10770.pdf | Memorization Without Overfitting Analyzing the Training Dynamics of Large Language Models - Arxiv-2205.10770
https://arxiv.org/pdf/2212.10403.pdf | Towards Reasoning in Large Language Models A Survey - Arxiv-2212.10403

https://arxiv.org/pdf/2210.09338.pdf | Deep Bidirectional Language-Knowledge Graph Pretraining - Arxiv-2210.09338
https://arxiv.org/pdf/2202.05262.pdf | Locating and Editing Factual Associations in GPT - Arxiv-2202.05262
file:///home/chris/Downloads/20215-Article%20Text-24228-1-2-20220628.pdf | An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA
https://arxiv.org/pdf/2101.00376.pdf | RiddleSense Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge - Arxiv-2101.00376
https://proceedings.neurips.cc/paper/2021/file/f45a1078feb35de77d26b3f7a52ef502-Paper.pdf | Gradient-based Editing of Memory Examples for Online Task-free Continual Learning - NeurIPS-2021_f45a1078
https://arxiv.org/pdf/2106.11533.pdf | Do Language Models Perform Generalizable Commonsense Inference? - Arxiv-2106.11533
https://arxiv.org/pdf/2205.12598.pdf | RobustLR Evaluating Robustness to Logical Perturbation in Deductive Reasoning - Arxiv-2205.12598
https://arxiv.org/pdf/2205.12542.pdf | ER-TEST Evaluating Explanation Regularization Methods for NLP Models - Arxiv-2205.12542
https://arxiv.org/pdf/2108.01721.pdf | Improving Counterfactual Generation for Fair Hate Speech Detection - Arxiv-2108.01721
https://github.com/yao8839836/kg-bert | yao8839836/kg-bert: KG-BERT: BERT for Knowledge Graph Completion
https://arxiv.org/pdf/2103.05327.pdf | BERTese Learning to Speak to BERT - Arxiv-2103.05327
https://proceedings.neurips.cc/paper/2021/file/5c04925674920eb58467fb52ce4ef728-Paper.pdf | True Few-Shot Learning with Language Models - NeurIPS-2021_5c049256
https://arxiv.org/pdf/2104.08315.pdf | Surface Form Competition Why the Highest Probability Answer Isn't Always Right - Arxiv-2104.08315
https://arxiv.org/abs/2205.05055 | Data Distributional Properties Drive Emergent In-Context Learning in Transformers - Arxiv-2205.05055
https://arxiv.org/pdf/2210.11560.pdf | Finding Dataset Shortcuts with Grammar Induction - ACL-EMNLP-2022_2022.emnlp-main.293
https://arxiv.org/abs/2202.06539 | Deduplicating Training Data Mitigates Privacy Risks in Language Models - Arxiv-2202.06539
https://arxiv.org/pdf/2106.13353.pdf | Cutting Down on Prompts and Parameters Simple Few-Shot Learning with Language Models - Arxiv-2106.13353
https://arxiv.org/pdf/2301.03044.pdf | https://arxiv.org/pdf/2301.03044.pdf
https://www.surgehq.ai/blog/introduction-to-reinforcement-learning-with-human-feedback-rlhf-series-part-1 | Introduction to Reinforcement Learning with Human Feedback
https://huggingface.co/blog/rlhf | Illustrating Reinforcement Learning from Human Feedback (RLHF)
https://arxiv.org/abs/2204.05186 | Correcting Robot Plans with Natural Language Feedback - Arxiv-2204.05186

https://www.proquest.com/openview/17b48d375b45931f6739a01f9086d6b0/1?pq-origsite=gscholar&cbl=18750&diss=y | Finding and Fixing Undesirable Behaviors in Pretrained Language Models - ProQuest
https://openreview.net/forum?id=r6wu2WDhib9 | Learning from Natural Language Feedback | OpenReview
https://openreview.net/forum?id=89qDzjrWHLs | Can Large Language Models Truly Follow your Instructions? | OpenReview
https://arxiv.org/pdf/2202.03286.pdf | Red Teaming Language Models with Language Models - Arxiv-2202.03286
https://arxiv.org/pdf/2209.01975.pdf | Selective Annotation Makes Language Models Better Few-Shot Learners - Arxiv-2209.01975
https://arxiv.org/pdf/2205.00049.pdf | Prompt Consistency for Zero-Shot Task Generalization - Arxiv-2205.00049
https://arxiv.org/pdf/2212.08410.pdf | Teaching Small Language Models to Reason - Arxiv-2212.08410
https://arxiv.org/pdf/2209.07686.pdf | Text and Patterns For Effective Chain of Thought, It Takes Two to Tango - Arxiv-2209.07686
https://arxiv.org/pdf/2203.09161.pdf | How Many Data Samples is an Additional Instruction Worth? - Arxiv-2203.09161
https://aclanthology.org/2022.emnlp-main.410.pdf | Fine-tuned Language Models are Continual Learners - ACL-EMNLP-2022_2022.emnlp-main.410
https://arxiv.org/pdf/2212.03827.pdf | Discovering Latent Knowledge in Language Models Without Supervision - Arxiv-2212.03827
https://proceedings.mlr.press/v203/jang23a/jang23a.pdf | jang23a.pdf
https://arxiv.org/pdf/2206.13757.pdf | Flexible text generation for counterfactual fairness probing - Arxiv-2206.13757
https://arxiv.org/pdf/2210.11416.pdf | Scaling Instruction-Finetuned Language Models - Arxiv-2210.11416
https://twitter.com/EthanJPerez/status/1604886125482344449 | (1) Ethan Perez on Twitter: "Worrying behavior 2: LMs/RLHF models are people-pleasers, learning to repeat back dialog users’ views as their own (“sycophancy”). Sycophancy creates echo-chambers. Below, the same RLHF model gives opposite answers to a political question, in line with the user’s view: https://t.co/BT6AzMUMic" / Twitter
https://arxiv.org/pdf/2109.01652.pdf | Finetuned Language Models Are Zero-Shot Learners - Arxiv-2109.01652

https://arxiv.org/abs/2109.09193 | Towards Zero-Label Language Learning - Arxiv-2109.09193
https://arxiv.org/pdf/2109.09193.pdf | Towards Zero-Label Language Learning - Arxiv-2109.09193
https://arxiv.org/pdf/1911.03118.pdf | Not Enough Data? Deep Learning to the Rescue! - Arxiv-1911.03118
https://arxiv.org/pdf/2202.04538.pdf | Generating Training Data with Language Models Towards Zero-Shot Language Understanding - Arxiv-2202.04538
https://arxiv.org/pdf/2202.07922.pdf | ZeroGen Efficient Zero-shot Learning via Dataset Generation - Arxiv-2202.07922
https://arxiv.org/pdf/2211.03044.pdf | Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning - Arxiv-2211.03044
https://arxiv.org/pdf/2108.13487.pdf | Want To Reduce Labeling Cost? GPT-3 Can Help - Arxiv-2108.13487
https://arxiv.org/pdf/1811.00741.pdf | Stronger Data Poisoning Attacks Break Data Sanitization Defenses - Arxiv-1811.00741

https://arxiv.org/pdf/1806.00692.pdf | Stress Test Evaluation for Natural Language Inference - Arxiv-1806.00692

https://arxiv.org/pdf/2203.10133.pdf | Probing Factually Grounded Content Transfer with Factual Ablation - Arxiv-2203.10133
https://arxiv.org/pdf/2107.01294.pdf | Is GPT-3 Text Indistinguishable from Human Text? Scarecrow A Framework for Scrutinizing Machine Text - Arxiv-2107.01294

https://arxiv.org/abs/2010.02399 | Guiding Attention for Self-Supervised Learning with Transformers - Arxiv-2010.02399
https://arxiv.org/abs/2105.11115 | Self-Attention Networks Can Process Bounded Hierarchical Languages - Arxiv-2105.11115
https://arxiv.org/abs/2110.14782 | When is BERT Multilingual? Isolating Crucial Ingredients for Cross-lingual Transfer - Arxiv-2110.14782
https://arxiv.org/pdf/2301.11309.pdf | SemSup-XC Semantic Supervision for Zero and Few-shot Extreme Classification - Arxiv-2301.11309

https://docs.google.com/presentation/d/1bZFTW7c-mH5yQOUYmI4vzymJB6Pf7w1Bki-QyviaL7g/edit#slide=id.g11acd991225_0_330 | Adversarial Data Collection - Google Slides
https://arxiv.org/pdf/2211.09788.pdf | DiffusionDet Diffusion Model for Object Detection - Arxiv-2211.09788
https://scholar.google.com/citations?user=gGB0L4kAAAAJ | ‪Jonas Pfeiffer‬ - ‪Google Scholar‬
https://cs-sop.org/ | CS PhD Statements of Purpose
https://www.google.com/search?q=%E5%BF%A7%E9%83%81%E7%9A%84%E7%83%AD%E5%B8%A6&oq=%E5%BF%A7%E9%83%81%E7%9A%84%E7%83%AD%E5%B8%A6&aqs=chrome.0.0i355i512j46i512j0i512l8.2213j0j1&sourceid=chrome&ie=UTF-8 | 忧郁的热带 - Google Search
https://www.google.com/search?q=%E6%9C%AC%E5%A4%9A%E7%B9%81%E9%82%A6&oq=%E6%9C%AC%E5%A4%9Afan&aqs=chrome.1.69i57j0i512j0i4i15i30.4289j0j1&sourceid=chrome&ie=UTF-8 | 本多繁邦 - Google Search
https://www.google.com/search?q=%E6%97%B6%E9%92%9F%E4%B8%8D%E4%BC%9A%E6%92%92%E8%B0%8E | 时钟不会撒谎 - Google Search
https://zhuanlan.zhihu.com/p/391365563 | 读书笔记69《2019中国悬疑小说精选》 - 知乎
https://hitomi.la/search.html?artist%3Autu%20language%3Achinese | Search | Hitomi.la
https://hitomi.la/manga/%EF%BC%BB%E3%81%99%E3%81%8E%E3%81%A2--%E3%82%B9%E3%82%B1%E3%83%99%E3%83%89%E3%83%AC%E3%83%84%E3%82%B7%E3%83%B3%E3%82%B0-%E4%B8%AD%E6%96%87-2354959.html#1 | ［すぎぢ-]スケベドレツシング by sugi g | Hitomi.la
https://hitomi.la/search.html?artist%3Ajamming%20language%3Achinese | Search | Hitomi.la
https://hitomi.la/doujinshi/visiting-%E4%B8%AD%E6%96%87-2456947.html#1 | VISITING by laliberte | Hitomi.la
https://hitomi.la/doujinshi/hamegaki-x-yaritsuma-%E4%B8%AD%E6%96%87-2400588.html#1 | Hamegaki x Yaritsuma by jamming | Hitomi.la

https://arxiv.org/abs/2205.11482 | Tracing Knowledge in Language Models Back to the Training Data - Arxiv-2205.11482
https://www.neuralnet.science/reading-group/ | MIT Reading Group (Fall 2022): The Science of Deep Learning
https://arxiv.org/abs/2106.00737 | Implicit Representations of Meaning in Neural Language Models - Arxiv-2106.00737
https://arxiv.org/abs/2006.14032 | Compositional Explanations of Neurons - Arxiv-2006.14032
https://arxiv.org/pdf/2212.09257.pdf | PromptBoosting Black-Box Text Classification with Ten Forward Passes - Arxiv-2212.09257
https://twitter.com/jacobandreas/status/1600118539263741952 | (1) Jacob Andreas on Twitter: "Speculative (!!!) paper arguing that big LMs can model agency &amp; communicative intent: https://t.co/WYaedqx9TT (somehow in EMNLP findings). Briefly: 1. LMs do not in general have beliefs or goals. An LM trained on the Internet models a distribution over next tokens *marginalized* https://t.co/tZ8eFUhWOq" / Twitter
https://arxiv.org/abs/2211.15661 | What learning algorithm is in-context learning? Investigations with linear models | Abstract

https://arxiv.org/pdf/2210.11610.pdf | Large Language Models Can Self-Improve - Arxiv-2210.11610
https://book.douban.com/subject/36178388/ | 魔蟲人間 2 (豆瓣)
https://book.douban.com/subject/36081279/ | 杀人推理竞赛 (豆瓣)
https://book.douban.com/subject/35802295/ | 天堂之音，魔鬼之名 (豆瓣)
https://book.douban.com/subject/5375591/ | 放課後探偵団 (豆瓣)
https://book.douban.com/subject/35977847/ | 哲学家的密室 上 (豆瓣)
https://lockedroom.net/blog/?p=2061 | 霞流一 综评 | Fang's Mystery Blog

https://docs.google.com/document/d/1sPSV8VY4AUYXiUZJS3uvVesquYA7V5aslN01ynXcQ_Y/edit | Compute - SC - Google 文档
https://cs.stanford.edu/sc/job-submissions | Job Submissions | Stanford Computer Science
https://cs.stanford.edu/sc/managing-jobs | Managing Jobs | Stanford Computer Science
https://cs.stanford.edu/sc/cluster-storage | Cluster Storage | Stanford Computer Science
https://cs.stanford.edu/sc/useful-cli-tools | Useful CLI tools | Stanford Computer Science
https://baike.baidu.com/item/%E7%A5%9E%E9%9E%AD/66652 | 神鞭（冯骥才著中篇小说）_百度百科

https://piazza.com/class_profile/get_resource/ldjefz5a4yu69n/le4m7z1yijx13b | broderick_lecture3_spring2023
https://arxiv.org/abs/2012.07421 | WILDS A Benchmark of in-the-Wild Distribution Shifts - Arxiv-2012.07421
https://arxiv.org/pdf/2301.11305v1.pdf | DetectGPT Zero-Shot Machine-Generated Text Detection using Probability Curvature - Arxiv-2301.11305
https://detectgpt.ericmitchell.ai/ | DetectGPT: a GPT-2 Detector
https://www.google.com/search?q=conservative+prediction+via+transductive+confidence+minimzaiton&newwindow=1&sxsrf=AJOqlzVFX2zULqzvNNP0trqaifS73NK2Ig%3A1677269686848&ei=thr5Y8meM62q5NoPrMeC6Ak&ved=0ahUKEwjJ0YG__K79AhUtFVkFHayjAJ0Q4dUDCBA&uact=5&oq=conservative+prediction+via+transductive+confidence+minimzaiton&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzoKCAAQRxDWBBCwAzoFCCEQoAE6BQghEKsCOgcIIRCgARAKSgQIQRgAUKwJWNYnYKEpaAFwAXgBgAGGAYgB4BCSAQQxOC41mAEAoAEByAEIwAEB&sclient=gws-wiz-serp | conservative prediction via transductive confidence minimzaiton - Google Search
https://www.google.com/search?q=diversity+and+disambiguate&oq=diversity+and+disambiguate&aqs=chrome..69i57j33i160l2.5407j0j1&sourceid=chrome&ie=UTF-8 | diversity and disambiguate - Google Search
https://arxiv.org/pdf/2202.03418.pdf | Diversify and Disambiguate Learning From Underspecified Data - Arxiv-2202.03418
https://github.com/yoonholee/DivDis | yoonholee/DivDis
https://sites.google.com/view/diversify-and-disambiguate | DivDis

https://arxiv.org/pdf/2302.07459.pdf | The Capacity for Moral Self-Correction in Large Language Models - Arxiv-2302.07459
https://arxiv.org/pdf/2212.09251.pdf | Discovering Language Model Behaviors with Model-Written Evaluations - Arxiv-2212.09251
https://www.google.com/search?q=BBQ%3A+A+Hand-Built+Bias+Benchmark+for+Question+Answering+scholar&newwindow=1&sxsrf=AJOqlzViaecZeW2lvDWx76ScxQyH21RSUg%3A1677251227933&ei=m9L4Y8DGOMSl5NoPgPejkAE&ved=0ahUKEwiAgI_dt679AhXEElkFHYD7CBIQ4dUDCBA&uact=5&oq=BBQ%3A+A+Hand-Built+Bias+Benchmark+for+Question+Answering+scholar&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzoECCMQJzoGCAAQFhAeOgUIABCGAzoFCCEQoAE6BQghEKsCSgQIQRgBUIQBWJEIYMwIaAFwAHgAgAGHAYgBvAaSAQMxLjaYAQCgAQHAAQE&sclient=gws-wiz-serp | BBQ: A Hand-Built Bias Benchmark for Question Answering scholar - Google Search
https://github.com/nyu-mll/BBQ | nyu-mll/BBQ: Repository for the Bias Benchmark for QA dataset.
https://arxiv.org/pdf/2110.08193.pdf | BBQ A Hand-Built Bias Benchmark for Question Answering - Arxiv-2110.08193


https://canvas.mit.edu/courses/20206 | 6.S986 Special Subject in EECS
https://docs.google.com/spreadsheets/d/1dTI2gddd2m5N3YEticolGjK9g29dH7fUkjrIv_cMN0o/edit#gid=1270115572 | LLM Papers - Google 表格
https://arxiv.org/pdf/2212.07677.pdf | Transformers learn in-context by gradient descent - Arxiv-2212.07677
https://arxiv.org/abs/2211.15661 | What learning algorithm is in-context learning? Investigations with linear models | Abstract
https://openreview.net/forum?id=0g0X4H8yN4I | ​​What learning algorithm is in-context learning? Investigations with linear models - OR-ICLR-2023_0g0X4H8yN4I
https://openreview.net/pdf?id=0g0X4H8yN4I | ​​What learning algorithm is in-context learning? Investigations with linear models - OR-ICLR-2023_0g0X4H8yN4I
https://twitter.com/ethayarajh/status/1628442002454085632?s=46&t=py8ptijkIjzYInrpRa-RPQ | Kawin Ethayarajh on Twitter: "📢 Models like #ChatGPT are trained on tons of human feedback. But collecting this costs $$$! That's why we're releasing the Stanford Human Preferences Dataset (🚢SHP), a collection of 385K *naturally occurring* *collective* human preferences over text. https://t.co/cRY1F8TjFz" / Twitter
https://arxiv.org/pdf/2012.15723.pdf | Making Pre-trained Language Models Better Few-shot Learners - Arxiv-2012.15723
https://arxiv.org/abs/2212.02475 | Meta-Learning Fast Weight Language Models - Arxiv-2212.02475
https://arxiv.org/abs/2110.15943 | MetaICL Learning to Learn In Context - Arxiv-2110.15943
https://arxiv.org/pdf/2212.05129.pdf | Measuring Data - Arxiv-2212.05129
https://arxiv.org/pdf/1704.01444.pdf | Learning to Generate Reviews and Discovering Sentiment - Arxiv-1704.01444

https://book.douban.com/subject/1861809/ | 東京異聞 (豆瓣)
https://book.douban.com/subject/34882130/ | 絃之聖域 (豆瓣)

https://arxiv.org/pdf/2109.09193.pdf | Towards Zero-Label Language Learning - Arxiv-2109.09193
https://arxiv.org/abs/2108.13487 | Want To Reduce Labeling Cost? GPT-3 Can Help - Arxiv-2108.13487
https://arxiv.org/pdf/2205.12640.pdf | Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating Spurious Correlations in Entity Typing - Arxiv-2205.12640
https://arxiv.org/pdf/1610.05820.pdf | Membership Inference Attacks against Machine Learning Models - Arxiv-1610.05820
https://openaccess.thecvf.com/content/CVPR2021/papers/Rezaei_On_the_Difficulty_of_Membership_Inference_Attacks_CVPR_2021_paper.pdf | On the Difficulty of Membership Inference Attacks - CVPR-2021_17609415
https://arxiv.org/pdf/2203.03929.pdf | Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks - Arxiv-2203.03929

https://mp.weixin.qq.com/s?__biz=MzIzNzU4OTAxMQ==&mid=2247485997&idx=1&sn=004a561bfc87aa868abe3bd58dcdfe33&exportkey=n_ChQIAhIQ8ZyzeDsxsY1%2BQKi%2BlJpQzxKWAgIE97dBBAEAAAAAADlJIIA6PrYAAAAOpnltbLcz9gKNyK89dVj09uu03swE51UzoM%2FpDA%2Fa0RMk1Fu%2ByVGb1tX3k8zvtH2CeZa3F3GT1PmybC4CVP6ZzMvltyuxPr%2Fs%2B%2BV4l3VwldcEJLmNeCOlQYph3Quc669owPVMHfAWQI2LkVMWf8M5x6ijHhKMSNAher1YysXd9ybAafX8%2FGuJXuJ2uk2PvQz9JCidEv0PTRzv0R1HoQNJ%2BUUZmQDLVtxVo3iTQrarUox5fMNAr1lLxkWXLMrH0sGTdjPkeRXzjNYiaMh42jxSLWzuetIJ3YWBThwy2%2FAPSQRseu3GIGQuS5fjHHQCzTcGeffgyy24HuU4WE03zyOV&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q2YiWRGp6wOl8yvbOvb7F7m&wx_header=0 | ECCV2022 | RU&谷歌提出用CLIP进行zero-shot目标检测！​
https://github.com/microsoft/unilm/tree/master/beit2 | unilm/beit2 at master · microsoft/unilm
https://github.com/IDEA-Research/DINO | IDEA-Research/DINO: [ICLR 2023] Official implementation of the paper "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection"
https://github.com/IDEA-Research/detrex | IDEA-Research/detrex: detrex is a research platform for Transformer-based Instance Recognition algorithms including DETR (ECCV 2020), Deformable-DETR (ICLR 2021), Conditional-DETR (ICCV 2021), DAB-DETR (ICLR 2022), DN-DETR (CVPR 2022), DINO (ICLR 2023), H-DETR (arXiv 2022), MaskDINO (arXiv 2022), etc.
https://github.com/microsoft/GLIP | microsoft/GLIP: Grounded Language-Image Pre-training
https://gligen.github.io/ | GLIGEN:Open-Set Grounded Text-to-Image Generation.
https://arxiv.org/abs/2005.00545 | Low-Dimensional Hyperbolic Knowledge Graph Embeddings - Arxiv-2005.00545
https://homepages.inf.ed.ac.uk/rsarkar/papers/HyperbolicDelaunayFull.pdf | HyperbolicDelaunayFull.pdf
https://github.com/shizhediao/ChatGPTPapers | shizhediao/ChatGPTPapers: Must-read papers, related blogs and API tools on the pre-training and tuning methods for ChatGPT.
https://cohere.for.ai/?utm_term=&utm_campaign=na_textclassification_performancemax&utm_source=google&utm_medium=paidsearch&hsa_acc=4946693046&hsa_cam=17376869464&hsa_grp=&hsa_ad=&hsa_src=x&hsa_tgt=&hsa_kw=&hsa_mt=&hsa_net=adwords&hsa_ver=3&gclid=Cj0KCQiAi8KfBhCuARIsADp-A55qccz4zsaRu7oscuoOfaEjImnaVsYlySKTaKjQopnl1cCSI5ipS28aAo7KEALw_wcB | Home | Cohere For AI
https://arxiv.org/pdf/2302.09419.pdf | A Comprehensive Survey on Pretrained Foundation Models A History from BERT to ChatGPT - Arxiv-2302.09419
https://arxiv.org/abs/1706.02216 | Inductive Representation Learning on Large Graphs - Arxiv-1706.02216
https://github.com/NoviScl/GPT3-Reliability | NoviScl/GPT3-Reliability

https://huggingface.co/docs/timm/main/en/training_script | Scripts
https://github.com/microsoft/FocalNet | microsoft/FocalNet: [NeurIPS 2022] Official code for "Focal Modulation Networks"
https://github.com/SwinTransformer/Feature-Distillation | SwinTransformer/Feature-Distillation
https://detrex.readthedocs.io/en/latest/tutorials/Installation.html | Installation — detrex documentation

https://captum.ai/api/search.html?q=SHAP | Captum · Model Interpretability for PyTorch
https://www.bilibili.com/video/BV1FD4y1A7Ye/?spm_id_from=333.1007.tianma.1-1-1.click&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | 少女被困密室，生死取决于你，1998年世嘉土星经典AVG游戏_单机游戏热门视频
https://arxiv.org/abs/2106.09685 | LoRA Low-Rank Adaptation of Large Language Models - Arxiv-2106.09685
https://arxiv.org/pdf/2104.07540.pdf | Generating Datasets with Pretrained Language Models - Arxiv-2104.07540
https://arxiv.org/pdf/2110.05448.pdf | Unsupervised Neural Machine Translation with Generative Language Models Only - Arxiv-2110.05448

https://zhuanlan.zhihu.com/p/91383421 | EMNLP 最佳论文解读：来自信息瓶颈的新语言学理论 - 知乎
https://zhuanlan.zhihu.com/p/340329943 | RealFormer：Real 简单，Real 有效 - 知乎
https://zhuanlan.zhihu.com/p/348402227 | GPT 的野望 - 知乎
https://zhuanlan.zhihu.com/p/86900556 | BERT 瘦身之路：Distillation，Quantization，Pruning - 知乎
https://zhuanlan.zhihu.com/p/75893972 | SpanBert：对 Bert 预训练的一次深度探索 - 知乎
https://www.zhihu.com/people/zhang-jun-lin-76/posts | (41 封私信 / 80 条消息) 张俊林 - 知乎

https://arxiv.org/pdf/2208.12242.pdf | DreamBooth Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation - Arxiv-2208.12242
https://posts.careerengine.us/author/5f4c50ffd1f9823b82ea2eb8/posts?from=authorDetailSidePanel | 雨石记资讯

https://www-cs-faculty.stanford.edu/people/jure/pubs/interpretable-kdd16.pdf | interpretable-kdd16.pdf
https://arxiv.org/pdf/1602.04938.pdf | 'Why Should I Trust You?' Explaining the Predictions of Any Classifier - ACM-2016_10114529396722939778
https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf | A Unified Approach to Interpreting Model Predictions - Arxiv-1705.07874
https://arxiv.org/pdf/2302.03494.pdf | A Categorical Archive of ChatGPT Failures - Arxiv-2302.03494

https://lilianweng.github.io/posts/2018-10-13-flow-models/ | Flow-based Deep Generative Models | Lil'Log
https://lilianweng.github.io/posts/2017-09-28-information-bottleneck/ | Anatomize Deep Learning with Information Theory | Lil'Log

https://book.douban.com/subject/35272690/ | 海葵 (豆瓣)
https://book.douban.com/subject/30405794/ | 尸语女法医 (豆瓣)
https://book.douban.com/subject/36127454/ | 雪祭 (豆瓣)
https://book.douban.com/subject/35812032/ | 食血草 (豆瓣)
https://book.douban.com/subject/33418859/ | 本所七怪谈 (豆瓣)
https://book.douban.com/subject/35755556/ | 恶意的兔子 (豆瓣)
https://www.google.com/search?q=%E5%9B%9B%E5%8F%A0%E5%8D%8A%E6%97%B6%E5%85%89%E6%9C%BA%E5%B8%83%E9%B2%81%E6%96%AF | 四叠半时光机布鲁斯 - Google Search
https://www.99csw.com/book/8812/index.htm | 搪瓷灵魂的比重_佐藤友哉_在线阅读_九九藏书网
https://www.douban.com/search?q=%E6%B8%90%E5%8F%98%E6%B6%88%E5%A4%B1 | 搜索: 渐变消失
https://book.douban.com/subject/34784730/ | 银河铁道之夜 (豆瓣)
https://www.douban.com/search?q=%E4%B8%80%E6%A1%A9%E4%BA%8B%E5%85%88%E5%BC%A0%E6%89%AC%E7%9A%84%E8%B0%8B%E6%9D%80%E6%A1%88 | 搜索: 一桩事先张扬的谋杀案
https://lilianweng.github.io/page/2/ | Lil'Log
https://lilianweng.github.io/posts/2022-04-15-data-gen/ | Learning with not Enough Data Part 3: Data Generation | Lil'Log
https://lilianweng.github.io/posts/2022-02-20-active-learning/ | Learning with not Enough Data Part 2: Active Learning | Lil'Log
https://lilianweng.github.io/posts/2021-12-05-semi-supervised/ | Learning with not Enough Data Part 1: Semi-Supervised Learning | Lil'Log
https://lilianweng.github.io/posts/2017-06-21-overview/ | An Overview of Deep Learning for Curious People | Lil'Log
https://lilianweng.github.io/posts/2017-08-01-interpretation/ | How to Explain the Prediction of a Machine Learning Model? | Lil'Log
https://lilianweng.github.io/posts/2017-08-20-gan/ | From GAN to WGAN | Lil'Log
https://www.douban.com/search?q=%E9%9B%B6%E7%9A%84%E8%9C%9C%E6%9C%88 | 搜索: 零的蜜月
https://book.douban.com/subject/30170658/ | 献给谋杀的供物 (豆瓣)

https://book.douban.com/subject/30488074/ | 思想的黄昏 (豆瓣)
https://book.douban.com/subject/35900190/ | 在绝望之巅 (豆瓣)
https://book.douban.com/subject/25774978/ | 眼泪与圣徒 (豆瓣)
https://movie.douban.com/subject/24708811/ | 8号房间 (豆瓣)
https://movie.douban.com/subject/3531406/ | 再生门 (豆瓣)
https://www.wikiwand.com/en/Concentration_inequality | Concentration inequality - Wikiwand

https://arxiv.org/pdf/2108.04106.pdf | Noisy Channel Language Model Prompting for Few-Shot Text Classification - Arxiv-2108.04106
https://github.com/shmsw25/Channel-LM-Prompting | shmsw25/Channel-LM-Prompting: An original implementation of "Noisy Channel Language Model Prompting for Few-Shot Text Classification"
https://aclanthology.org/2022.acl-long.365.pdf | 2022.acl-long.365.pdf
https://github.com/Timothyxxx/Chain-of-ThoughtsPapers | Timothyxxx/Chain-of-ThoughtsPapers: A trend starts from "Chain of Thought Prompting Elicits Reasoning in Large Language Models".

https://openreview.net/forum?id=vsShetzoRG9 | InsNet: An Efficient, Flexible, and Performant Insertion-based Text Generation Model | OpenReview
https://openreview.net/pdf?id=vsShetzoRG9 | InsNet An Efficient, Flexible, and Performant Insertion-based Text Generation Model - OR-NeurIPS-2022_vsShetzoRG9
https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation | SolidGoldMagikarp (plus, prompt generation) - LessWrong
https://www.wikiwand.com/en/Conditional_expectation | Conditional expectation - Wikiwand
https://www.wikiwand.com/en/Borel-Kolmogorov_paradox | Borel–Kolmogorov paradox - Wikiwand

https://arxiv.org/pdf/2210.10749.pdf | Transformers Learn Shortcuts to Automata - Arxiv-2210.10749
https://arxiv.org/pdf/2301.13196.pdf | Looped Transformers as Programmable Computers | PDF
https://github.com/facebookresearch/CutLER | facebookresearch/CutLER: Code release for "Cut and Learn for Unsupervised Object Detection and Instance Segmentation"
http://people.eecs.berkeley.edu/~xdwang/projects/CutLER/ | Cut and Learn for Unsupervised Object Detection and Instance Segmentation

https://twitter.com/lateinteraction/status/1617953413576425472 | Omar Khattab on Twitter: "Introducing Demonstrate–Search–Predict (𝗗𝗦𝗣), a framework for composing search and LMs w/ up to 120% gains over GPT-3.5. No more prompt engineering.❌ Describe a high-level strategy as imperative code and let 𝗗𝗦𝗣 deal with prompts and queries.🧵 https://t.co/2265Ii0vCS https://t.co/WAaHGH7fCQ" / Twitter
https://github.com/hwchase17/langchain | hwchase17/langchain: ⚡ Building applications with LLMs through composability ⚡
https://proceedings.mlr.press/v162/lang22a.html | Co-training Improves Prompt-based Learning for Large Language Models - Arxiv-2202.00828
https://arxiv.org/pdf/2212.14052.pdf | Hungry Hungry Hippos Towards Language Modeling with State Space Models - Arxiv-2212.14052
https://slideslive.com/38967412/sequencetosequence-learning-with-latent-neural-grammars?ref=recommended | Yoon Kim · Sequence-to-Sequence Learning with Latent Neural Grammars · SlidesLive
https://arxiv.org/pdf/1904.09545.pdf | Good-Enough Compositional Data Augmentation - Arxiv-1904.09545
https://slideslive.com/38984059/cotraining-improves-promptbased-learning-for-large-language-models?ref=speaker-24304 | Hunter Lang, Monica Agrawal, Yoon Kim, David Sontag · Co-Training Improves Prompt-Based Learning for Large Language Models · SlidesLive
https://arxiv.org/pdf/1904.05521v1.pdf | UniVSE Robust Visual Semantic Embeddings via Structured Semantic Representations - Arxiv-1904.05521
https://www.google.com/search?q=Unified+Visual-Semantic+Embeddings%3A+Bridging+Vision+and+Language+with+Structured+Meaning+Representations&sourceid=chrome&ie=UTF-8 | Unified Visual-Semantic Embeddings: Bridging Vision and Language with Structured Meaning Representations - Google Search
https://jiayuanm.com/#topic-concept-learning | Jiayuan Mao

https://arxiv.org/pdf/2105.08127.pdf | Finding an Unsupervised Image Segmenter in Each of Your Deep Generative Models - Arxiv-2105.08127
https://www.dropbox.com/s/keiaw6tib1afj38/apm120-notes.pdf?dl=0 | apm120-notes.pdf
https://github.com/stanfordnlp/dsp | stanfordnlp/dsp: The Demonstrate-Search-Predict Framework: Composing retrieval and language models for knowledge-intensive NLP
https://github.com/stanfordnlp/dsp/blob/main/intro.ipynb | dsp/intro.ipynb at main · stanfordnlp/dsp
https://colab.research.google.com/github/stanfordnlp/dsp/blob/main/intro.ipynb#scrollTo=LJVaof2m3dng | intro.ipynb - Colaboratory

https://arxiv.org/pdf/2301.08721v1.pdf | Batch Prompting Efficient Inference with Large Language Model APIs - Arxiv-2301.08721
https://twitter.com/cwolferesearch/status/1612886048949915650 | Cameron R. Wolfe on Twitter: "Recently, I’ve read and overviewed publications for nearly 20 different large language models (LLMs) from GPT to ChatGPT. Here’s what I learned… 🧵 [1/10]" / Twitter
https://twitter.com/ChrisXU35407830/likes | Tweets liked by Chris XU (@ChrisXU35407830) / Twitter
https://twitter.com/sharifshameem/status/1618369196387340294 | Sharif Shameem on Twitter: "unpopular opinion: open source LLMs are *really good*. they suck at benchmarks like HELM compared to closed models, but fine-tuned accuracy is amazing, inference is dirt cheap, and both flan-t5-xxl + gpt-neox-20b fit on a single a100. now imagine 1-click RLAIF fine-tuning…🤔" / Twitter
https://qntm.org/mmacevedo | Lena @ Things Of Interest
https://twitter.com/rajammanabrolu/status/1616590789953589248 | Prithviraj (Raj) Ammanabrolu on Twitter: "Now accepted to #ICLR2023! Look forward to our talk on open source, efficient natural language RLHF algorithms at Kigali, Rwanda!!!" / Twitter
https://twitter.com/dair_ai/status/1612153093101174784 | DAIR.AI on Twitter: "Top ML Papers of the Week (Jan 1-8): - Muse (new text-to-image generation/editing model) - Rethinking with retrieval - Pruning LLMs in one-shot - ConvNeXt V2 - LLMs for corporate lobbying-related activities - StitchNet - VALL-E … 1 of 11 https://t.co/XAAQrFN6ma" / Twitter
https://twitter.com/srchvrs/status/1612288813170135047 | Leo Boytsov on Twitter: "🧵Attention the IR community! The era of cheap UNSUPERVISED domain adaptation has begun! Let me introduce the InPars-Light training recipe enabling a small MiniLM model (with 30M parameters) to consistently outperform BM25 on all datasets used in the original InPars study." / Twitter
https://twitter.com/cwolferesearch/status/1612099963013529609 | Cameron R. Wolfe on Twitter: "OPT-IML (just released by @MetaAI) is a version of OPT-175B that's fine-tuned on a benchmark (OPT-IML Bench) of ~2000 instruction-based tasks. Prior work indicated that instruction-tuning is useful for LLMs, but we learn more details about this with OPT-IML... 🧵[1/8]" / Twitter
https://github.com/facebookresearch/metaseq/tree/main/projects/OPT-IML | metaseq/projects/OPT-IML at main · facebookresearch/metaseq
https://twitter.com/weijiavxu/status/1616504852447694861 | Weijia Xu on Twitter: "[1/6] What is an NMT model "thinking" when it hallucinates? Are there any internal symptoms that may flag a hallucination? Our new paper "Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection" is out! https://t.co/Wjhfx2XHUP https://t.co/tmhHNlYQbm" / Twitter
https://twitter.com/EzraJNewman/status/1618628985096933377 | Ezra Newman on Twitter: "Almost all human reasoning is next token prediction. This is not a bit. I think this might be true." / Twitter
https://twitter.com/ebugliarello/status/1547741535382294531 | Emanuele Bugliarello | ebugliarello@sigmoid.social on Twitter: "Tired of tokenizers/subwords? Check out PIXEL, a new language model that processes written text as images📸 “Language Modelling with Pixels” 📄 https://t.co/pmp7Yvhx9W 🧑‍💻https://t.co/RbMemZOpub 🤖https://t.co/J80eju62eB by @rust_phillip @jonasflotz me @esalesk @mdlhx @delliott https://t.co/6nzqlpoPMU" / Twitter
https://twitter.com/realDanFu/status/1617605971395891201 | Dan Fu on Twitter: "Attention is all you need... but how much of it do you need? Announcing H3 - a new generative language models that outperforms GPT-Neo-2.7B with only *2* attention layers! Accepted as a *spotlight* at #ICLR2023! 📣 w/ @tri_dao 📜 https://t.co/vKdOTCH8Lk 1/n" / Twitter
https://twitter.com/suchenzang/status/1617093563061522432 | Susan Zhang on Twitter: "Piling on to the pile-on (sorry - it's always easy to criticize 😛), here's a rant about benchmarks for LLMs that are used to back claims of "stronger" or "better" models. Let's start with a tour through GPT-3's Appendix G... 1/8" / Twitter
https://twitter.com/omarsar0/status/1620090029451403264 | elvis on Twitter: "LLMs still struggle with complex reasoning. Chain-of-thought prompting has shown potential on tasks such as arithmetic and commonsense reasoning. ThoughtSource provides datasets and tools for CoT reasoning in LLMs. https://t.co/OtfynefjLS https://t.co/e2ZilCSaSv" / Twitter
https://twitter.com/peterjliu/status/1620157580114030592 | Peter J. Liu on Twitter: "As generative language models hit production, there’s increased risk from bad outputs. It’s useful to know when to *not* show the outputs to the user, or defer to better, larger models (at the cost of compute). A 🧵on an ICLR 2023 paper from Google. (1/n)" / Twitter
https://twitter.com/mathemagic1an/status/1620111511321710592 | Jay Hack on Twitter: "What if you could fit an *entire codebase* in an LLM? 🤔 "Efficiently Scaling Transformer Inference" (11/2022) https://t.co/OsNPRwkfC8 Jeff Dean + co break out all the hacks to scale PALM-540B's context length to 43,000 tokens! Here's how 👇 https://t.co/FkfSqi0R1Q" / Twitter
https://twitter.com/cwolferesearch/status/1613643034717028352 | Cameron R. Wolfe on Twitter: "Large Language Models (LLMs) have the potential to be incredibly useful, but they also make a lot of mistakes (e.g., by generating false or biased information). To eliminate this behavior, recent generations of LLMs utilize a two-part refinement process… 🧵 [1/10]" / Twitter
https://github.com/stanford-futuredata/ColBERT | stanford-futuredata/ColBERT: ColBERT: state-of-the-art neural search (SIGIR'20, TACL'21, NeurIPS'21, NAACL'22, CIKM'22)
https://twitter.com/yoavgo/status/1617970474088288256 | (((ل()(ل() 'yoav))))👾 on Twitter: ""science and engineering, 2023” https://t.co/OFen6PNjso" / Twitter
https://github.com/Xpitfire/symbolicai | Xpitfire/symbolicai: Compositional Differentiable Programming Library
https://arxiv.org/pdf/2301.04272.pdf | Data Distillation A Survey - Arxiv-2301.04272

https://shamulent.github.io/Lectures/Lecture6_annotated.pdf | Lecture6_annotated.pdf
https://www.bilibili.com/video/BV1zb4y1Q7Js/?from=search&seid=14499920823264205383&spm_id_from=333.337.0.0&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | 【完结】极乐迪斯科-最终剪辑版 全流程 无解说 【全集】_哔哩哔哩_bilibili
https://shamulent.github.io/CS_Stat184_Fall22.html | CS/Stat 184 Intro to RL

https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Unified_Visual-Semantic_Embeddings_Bridging_Vision_and_Language_With_Structured_Meaning_CVPR_2019_paper.pdf | Unified Visual-Semantic Embeddings Bridging Vision and Language With Structured Meaning Representations - CVPR-2019_65564619
https://scholar.google.com/citations?user=-xaOIZIAAAAJ&hl=en | ‪Jiayuan Mao‬ - ‪Google Scholar‬
https://jiayuanm.com/ | Jiayuan Mao
https://arxiv.org/pdf/1906.02890.pdf | Visually Grounded Neural Syntax Acquisition - Arxiv-1906.02890
https://people.csail.mit.edu/tommi/ | Tommi Jaakkola
https://arxiv.org/pdf/1606.02447.pdf | Learning Language Games through Interaction - Arxiv-1606.02447
https://arxiv.org/pdf/1904.05521v1.pdf | UniVSE Robust Visual Semantic Embeddings via Structured Semantic Representations - Arxiv-1904.05521

https://www.youtube.com/watch?v=5Zk8eHxmql8&t=2379s | (631) Jiayuan Mao - Neuro-Symbolic Frameworks for Visual Concept Learning and Language Acquisition - YouTube
https://pdsketch.csail.mit.edu/data/papers/2022NeurIPS-PDSketch.pdf | 2022NeurIPS-PDSketch.pdf
https://arxiv.org/pdf/2203.16639.pdf | FALCON Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations - Arxiv-2203.16639
https://arxiv.org/pdf/1906.02890.pdf | Visually Grounded Neural Syntax Acquisition - Arxiv-1906.02890
https://arxiv.org/pdf/2202.08806.pdf | Grammar-Based Grounded Lexicon Learning - Arxiv-2202.08806

https://rltheorybook.github.io/rltheorybook_AJKS.pdf | rltheorybook_AJKS.pdf
http://incompleteideas.net/book/bookdraft2017nov5.pdf | bookdraft2017nov5.pdf
https://www.di.ens.fr/~fbach/ | Francis Bach - INRIA - ENS - PSL
https://mjt.cs.illinois.edu/dlt/#rademacher-complexity | Deep learning theory lecture notes
https://transformer-circuits.pub/ | Transformer Circuits Thread
http://www.cs.toronto.edu/~avner/teaching/S6-2414/ | Metrc Embeddings (CSC 2414H), Spring 2006: Home Page
https://zhuanlan.zhihu.com/p/36699314 | Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision - 知乎
https://zhuanlan.zhihu.com/p/22513016 | 读《Neural Turing Machines》 - 知乎
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=n_ts4eYAAAAJ&sortby=pubdate&citation_for_view=n_ts4eYAAAAJ:TQgYirikUcIC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=n_ts4eYAAAAJ&sortby=pubdate&citation_for_view=n_ts4eYAAAAJ:R3hNpaxXUhUC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=n_ts4eYAAAAJ&sortby=pubdate&citation_for_view=n_ts4eYAAAAJ:HDshCWvjkbEC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=n_ts4eYAAAAJ&sortby=pubdate&citation_for_view=n_ts4eYAAAAJ:mB3voiENLucC | View article
https://arxiv.org/abs/2203.01146 | Controlling the Focus of Pretrained Language Generation Models - Arxiv-2203.01146
https://proceedings.neurips.cc/paper/2021/file/dd17e652cd2a08fdb8bf7f68e2ad3814-Paper.pdf | Sequence-to-Sequence Learning with Latent Neural Grammars - NeurIPS-2021_dd17e652
https://neurips2022-enlsp.github.io/papers/paper_27.pdf | paper_27.pdf
https://arxiv.org/pdf/2212.09140.pdf | Unsupervised Discontinuous Constituency Parsing with Mildly Context-Sensitive Grammars - Arxiv-2212.09140

https://openreview.net/forum?id=rg-zrfteOZc | Reasoning with Transformer-based Models Deep Learning, but Shallow Reasoning - OR-AKBC-2021_Ozp1WrgtF5_
https://worksheets.codalab.org/worksheets/0x2b314dc3536b482dbba02783a24719fd/ | CodaLab Worksheets
https://arxiv.org/pdf/2105.08127.pdf | Finding an Unsupervised Image Segmenter in Each of Your Deep Generative Models - Arxiv-2105.08127
https://openreview.net/pdf?id=nUmCcZ5RKF | IS SYNTHETIC DATA FROM GENERATIVE MODELS READY FOR IMAGE RECOGNITION? - OR-ICLR-2023_nUmCcZ5RKF
https://openreview.net/forum?id=nUmCcZ5RKF | IS SYNTHETIC DATA FROM GENERATIVE MODELS READY FOR IMAGE RECOGNITION? - OR-ICLR-2023_nUmCcZ5RKF
https://arxiv.org/pdf/2104.14294.pdf | Emerging Properties in Self-Supervised Vision Transformers - Arxiv-2104.14294
https://www.google.com/search?q=%E9%87%91%E6%9E%9D | 金枝 - Google Search

https://www.danfu.org/ | Dan Fu - Stanford University
https://cs.stanford.edu/people/chrismre/ | Homepage of Christopher Re (Chris Re)
https://arxiv.org/pdf/2212.14052
https://arxiv.org/abs/2301.07014 | Dataset Distillation A Comprehensive Review - Arxiv-2301.07014
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LIjnUGgAAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=LIjnUGgAAAAJ:z_wVstp3MssC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LIjnUGgAAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=LIjnUGgAAAAJ:Fu2w8maKXqMC | View article
https://arxiv.org/pdf/2012.07463.pdf | https://arxiv.org/pdf/2012.07463.pdf
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LIjnUGgAAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=LIjnUGgAAAAJ:ZuybSZzF8UAC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LIjnUGgAAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=LIjnUGgAAAAJ:hkOj_22Ku90C | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LIjnUGgAAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=LIjnUGgAAAAJ:LjlpjdlvIbIC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LIjnUGgAAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=LIjnUGgAAAAJ:J-pR_7NvFogC | View article

https://arxiv.org/abs/2204.09664 | Deep Learning meets Nonparametric Regression Are Weight-Decayed DNNs Locally Adaptive? - Arxiv-2204.09664

http://people.umass.edu/klement/tlp/tlp.html | Tractatus Logico-Philosophicus | Side-by-side-by-side edition
https://gligen.github.io/ | GLIGEN:Open-Set Grounded Text-to-Image Generation.
https://huggingface.co/spaces/gligen/demo | Demo - a Hugging Face Space by gligen
https://arxiv.org/pdf/2301.07093.pdf | GLIGEN Open-Set Grounded Text-to-Image Generation - Arxiv-2301.07093
https://phillipi.github.io/6.s898/ | 6.S898 Deep Learning, Fall 2022
https://phillipi.github.io/6.s898/materials/slides/13_rep_learning_theory.pdf | 13_rep_learning_theory
https://www.wikiwand.com/en/The_Idiot | The Idiot - Wikiwand

https://arxiv.org/pdf/2209.12711.pdf | Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts - Arxiv-2209.12711
https://github.com/facebookresearch/NPM | facebookresearch/NPM: The original implementation of Min et al. "Nonparametric Masked Language Modeling" (paper https//arxiv.org/abs/2212.01349)
https://www.wikiwand.com/en/William_James | William James - Wikiwand
https://www.reddit.com/r/cioran/comments/sx0kc1/i_want_to_get_into_cioran_i_enjoyed_kafkas/ | (3) I want to get into Cioran, I enjoyed Kafka's aphorisms the most of any philosophical texts I've read, where shoudl I start? : cioran
https://www.google.com/search?q=kafka+all+aphorisms&newwindow=1&sxsrf=AJOqlzVvvnrragkQYp1Zpln91qvUyUAOsw%3A1674234680412&ei=OMvKY8bYGLSg5NoP_NCQ2AU&ved=0ahUKEwjG1JWb0tb8AhU0EFkFHXwoBFsQ4dUDCBA&uact=5&oq=kafka+all+aphorisms&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIFCCEQoAE6CggAEEcQ1gQQsAM6BAgjECc6EQguEIMBEMcBELEDENEDEIAEOgUILhCABDoNCC4QsQMQxwEQ0QMQCjoECC4QQzoLCC4QsQMQxwEQ0QM6CAguEIAEELEDOhEILhCABBCxAxCDARDHARDRAzoFCAAQgAQ6CwgAELEDEIMBEJECOgUIABCRAjoNCC4QgAQQFBCHAhCxAzoLCAAQgAQQsQMQgwE6CwguEIMBELEDEIAEOggIABCABBCxAzoOCC4QsQMQgwEQxwEQrwE6CAguEIAEENQCOgsILhCABBCxAxCDAToICC4QgwEQsQM6DQgAEIAEEBQQhwIQsQM6BQguEJECOggIABCABBDLAToFCAAQhgM6BggAEBYQHkoECEEYAEoECEYYAFCuB1i4L2DKMGgEcAF4AIABoAGIAYESkgEENi4xNJgBAKABAcgBCMABAQ&sclient=gws-wiz-serp | kafka all aphorisms - Google Search
https://press.princeton.edu/books/hardcover/9780691205922/the-aphorisms-of-franz-kafka | The Aphorisms of Franz Kafka | Princeton University Press
http://zurauaphorisms.blogspot.com/ | Zurau Aphorisms
https://app.yinxiang.com/Home.action#n=7bb066d2-da7f-4294-86be-ee24e46cb7bf&s=s70&b=b5787273-91e4-4403-ad4f-eaa924cbe09e&ses=4&sh=1&sds=5& | 判决 | 印象笔记网页版
https://tieba.baidu.com/p/1246919155?pn=1 | 卡夫卡笔记及箴言补充_卡夫卡吧_百度贴吧
https://www.douban.com/group/topic/3489955/?_i=4235368KLQjbnS | 附卡夫卡日记
http://miniyuan.com/simple/?t1013.html | 卡夫卡：箴言录——对罪愆、苦难、希望和真正的道路的观察 | 诗与诗学 - 元知 - Powered by PHPWind
https://www.kafka-online.info/a-report-for-an-academy.html | A Report for An Academy by Franz Kafka

https://arxiv.org/pdf/2210.06726.pdf | Explanations from Large Language Models Make Small Reasoners Better - Arxiv-2210.06726
https://plato.stanford.edu/entries/ryle/ | Gilbert Ryle (Stanford Encyclopedia of Philosophy)
https://arxiv.org/pdf/2212.10001.pdf | Towards Understanding Chain-of-Thought Prompting An Empirical Study of What Matters - Arxiv-2212.10001
https://arxiv.org/pdf/2212.09865.pdf | Z-ICL Zero-Shot In-Context Learning with Pseudo-Demonstrations - Arxiv-2212.09865
https://arxiv.org/pdf/2210.12517.pdf | Exploring The Landscape of Distributional Robustness for Question Answering Models - Arxiv-2210.12517
https://arxiv.org/pdf/2205.12507.pdf | Re-Examining Calibration The Case of Question Answering - Arxiv-2205.12507
https://arxiv.org/abs/2110.08387 | Generated Knowledge Prompting for Commonsense Reasoning - Arxiv-2110.08387
https://arxiv.org/abs/2108.04106 | Noisy Channel Language Model Prompting for Few-Shot Text Classification - Arxiv-2108.04106
https://arxiv.org/abs/2111.02080 | An Explanation of In-context Learning as Implicit Bayesian Inference - Arxiv-2111.02080
https://arxiv.org/pdf/2210.03350.pdf | Measuring and Narrowing the Compositionality Gap in Language Models - Arxiv-2210.03350

https://arxiv.org/pdf/2301.00234.pdf | A Survey for In-context Learning - Arxiv-2301.00234

https://openaccess.thecvf.com/content/CVPR2022/papers/Wortsman_Robust_Fine-Tuning_of_Zero-Shot_Models_CVPR_2022_paper.pdf | Robust Fine-Tuning of Zero-Shot Models - CVPR-2022_23206643
https://arxiv.org/pdf/2104.07885.pdf | Probing Across Time What Does RoBERTa Know and When? - Arxiv-2104.07885
https://arxiv.org/pdf/2110.08387.pdf | Generated Knowledge Prompting for Commonsense Reasoning - Arxiv-2110.08387
https://arxiv.org/pdf/2211.09260.pdf | Task-aware Retrieval with Instructions - Arxiv-2211.09260
https://aclanthology.org/2022.findings-acl.50.pdf | 2022.findings-acl.50.pdf
https://arxiv.org/pdf/2002.06305.pdf | Fine-Tuning Pretrained Language Models Weight Initializations, Data Orders, and Early Stopping - Arxiv-2002.06305
https://danielkhashabi.com/files/2022_super_natural_instructions/nvidia-superni-talk.pdf | nvidia-superni-talk
https://aclanthology.org/2021.naacl-main.422.pdf | Probing Contextual Language Models for Common Ground with Visual Representations - ACL-NAACL-2021_2021.naacl-main.422
https://arxiv.org/pdf/2011.07127.pdf | IIRC A Dataset of Incomplete Information Reading Comprehension Questions - Arxiv-2011.07127
https://arxiv.org/pdf/2212.09865.pdf | Z-ICL Zero-Shot In-Context Learning with Pseudo-Demonstrations - Arxiv-2212.09865

https://arxiv.org/pdf/2205.05638.pdf | Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning - Arxiv-2205.05638
https://github.com/Shivanshu-Gupta | Shivanshu-Gupta (Shivanshu Gupta)
https://proceedings.neurips.cc/paper/2021/file/009c434cab57de48a31f6b669e7ba266-Paper.pdf | Counterfactual Explanations Can Be Manipulated - NeurIPS-2021_009c434c
https://arxiv.org/pdf/2109.05052.pdf | Entity-Based Knowledge Conflicts in Question Answering - Arxiv-2109.05052
https://arxiv.org/pdf/2202.07206.pdf | Impact of Pretraining Term Frequencies on Few-Shot Reasoning - Arxiv-2202.07206
https://arxiv.org/abs/2207.00747 | Rationale-Augmented Ensembles in Language Models - Arxiv-2207.00747
https://movie.douban.com/subject/35876302/?source=2022_annual_movie | 晒后假日 (豆瓣)
https://movie.douban.com/subject/35769174/?source=2022_annual_movie | 万湖会议 (豆瓣)
https://movie.douban.com/subject/35354759/?source=2022_annual_movie | 巴黎夜旅人 (豆瓣)
https://movie.douban.com/subject/3042261/?source=2022_annual_movie | 西线无战事 (豆瓣)
https://movie.douban.com/subject/35242938/?source=2022_annual_movie | 瀑布 (豆瓣)
https://movie.douban.com/subject/35371261/ | 铃芽之旅 (豆瓣)
https://movie.douban.com/subject/2237378/ | 在世界尽头相遇 (豆瓣)
https://movie.douban.com/subject/1299661/ | 苏州河 (豆瓣)
https://movie.douban.com/subject/1301912/ | 秋天的童话 (豆瓣)
https://movie.douban.com/subject/1305164/ | 甜蜜蜜 (豆瓣)
https://movie.douban.com/subject/25814705/ | 小森林 夏秋篇 (豆瓣)
https://movie.douban.com/subject/1299054/ | 香草的天空 (豆瓣)
https://movie.douban.com/subject/1789283/ | 时空线索 (豆瓣)
https://movie.douban.com/subject/33400376/?from=subject-page | 平原上的夏洛克 (豆瓣)
https://movie.douban.com/subject/26576995/ | 同步 (豆瓣)
https://movie.douban.com/subject/26269726/ | 无尽 (豆瓣)
https://movie.douban.com/subject/1295815/ | 那年夏天，宁静的海 (豆瓣)
https://movie.douban.com/subject/35652715/?source=2021_annual_movie | 杰伊·比姆 (豆瓣)
https://movie.douban.com/subject/34805873/?source=2021_annual_movie | 孤味 (豆瓣)
https://movie.douban.com/subject/34850598/?source=2021_annual_movie | 无声 (豆瓣)
https://movie.douban.com/subject/1292275/ | 罗拉快跑 (豆瓣)
https://movie.douban.com/subject/26790580/ | 菲利普·迪克的电子梦 (豆瓣)
https://movie.douban.com/subject/26353671/ | 明日战记 (豆瓣)
https://movie.douban.com/subject/34963486/ | 电影之神 (豆瓣)
https://movie.douban.com/subject/1793909/ | 预见未来 (豆瓣)
https://book.douban.com/subject/34934119/ | 权力的眼睛 (豆瓣)
https://movie.douban.com/subject/1307200/ | 机遇之歌 (豆瓣)
https://movie.douban.com/subject/35715638/ | 时间会议 (豆瓣)
https://movie.douban.com/subject/30387441/ | 天方异谈 (豆瓣)
https://book.douban.com/subject/11599245/ | 量子江湖·燕子坞（上） (豆瓣)
https://movie.douban.com/subject/1308993/ | 灵幻夹克 (豆瓣)
https://movie.douban.com/subject/3813779/ | 剑雨 (豆瓣)
https://book.douban.com/subject/24935042/ | 时间之墟 (豆瓣)
https://book.douban.com/subject/35218578/ | 七国银河 (豆瓣)

https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247541344&idx=2&sn=8a3e3f02b82fe89cce8c940a5acf4dba&exportkey=n_ChQIAhIQTqvEYqSlT9A7LDjZytRiQxKWAgIE97dBBAEAAAAAAG1hJPwqrnkAAAAOpnltbLcz9gKNyK89dVj0fnW0%2Fwakqf0gKRNuG1hNvg4yIuFdmuiI95dNPOZ%2FJ9joCOvy5WAzG4mngwFrl8S%2BxKNixlVq95SB2aAibtsEYKtqbt11XM0QAXnIXUvna9gCtHVAFgGWkZ7Jpsd7LcJiGvui1Frxh%2FG0B%2Bg8I6Qx3gkkI2e27ixWJnw2RwCNvVh1I5TMl47ZKl1tePVDrzAltTzaiHc7eA7zmZWJSEdrwV%2Fcm2reIYqBjaYTs2kzTLRYm7guMwKIEJeBaITjkhNAeHCejVb5U4ivFLFvmsIPEaKnEEOz4St5ZB0AgdaheybZXgBYIVU1Hl0TDS1lmSVT&acctmode=0&pass_ticket=3HIlKyjzTAFL%2FxFljj2tEtyLfkTobYOBU23FblbSLUp8Vu1SdLzUzUslMYnl2lP3NKfoplIeqIBS54Iibm%2FChw%3D%3D&wx_header=0 | 一句话修图时代来了！谷歌新模型碾压DALLE·2和Imagen
https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652281804&idx=5&sn=d8314b372447232920af9406f22fec6d&exportkey=n_ChQIAhIQTSZ7LNbEkDg7CPjDj89fWhKWAgIE97dBBAEAAAAAAKHkFBKiBT8AAAAOpnltbLcz9gKNyK89dVj0rwXOVow%2BTokElchimHSFig7afNUP0rOBOhWYkBEYbxRh%2BhQozb2jxSZOycl30vyr4ILjxP22vdwg1Pq7VQuHELPj1Xvh5Rn7KbfEJa%2F%2FzdD2cNA4NRpCPQDeCPhTqtX%2B9xeVgoumuyc9L80OOqua2Bl2zkpqC06GSw4%2BL1%2BiiOD%2BwglaPvvVdTslsJUKiSGejFVOhxt%2FDMzEK8dcXBJjc333yzZqKsAsGsfVBLpsErBgGaBA13RgSfmqRl8QHgwO0GrypbUYW8YjIQDkniDVrt0Ci24qW8rNpkWA2%2BsgTTaEUcXUZhsfLbzZTYqrghoX&acctmode=0&pass_ticket=3HIlKyjzTAFL%2FxFljj2tEtyLfkTobYOBU23FblbSLUoOBbNUDxmZkfPNQYuQsky1hOlaOIRRZKNtxEF1pnL3Hw%3D%3D&wx_header=0 | 不要think step by step！谷歌最新自然语言推理算法LAMBADA：「反向链推理」才是答案
https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650864860&idx=5&sn=0a35281f4335fd751099cca8548a8048&chksm=84e538a2b392b1b48a60ac2fb6c5023da30112c51c7c34848ebae905672afe259a3061721bc5&mpshare=1&scene=1&srcid=0116TfX12tQz0Ta30fXd5FYL&sharer_sharetime=1673842963015&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQ%2FJBMsjFi4m9MEi4uMB191xKWAgIE97dBBAEAAAAAAMq1JxAf7fYAAAAOpnltbLcz9gKNyK89dVj0POOsT8RU%2BUXt8FnqyeMPDoH4JTwn45mFbNNsxZP61jM8t1VebMCfu8psKk6QhsO%2BvhOBsRJG5uRXE8GXUj9HEIJOiemuXRt8HzubcmvyH12hXAYouTpNGomoTZP1ASdrznt9Ve194nMCdPE%2FxI4YFmfhO2WPFyqrlfxnlD7mWflZwgG%2BCgRSjso3fL4DNZ3YdSY5Kbzou7y4imDzNS3j3ryx4Smj4BXcZ7vTxd9j%2BijUwS%2B4DJVpTm9Dldv4M7H5quHFpHtud9L1T2gZug07dBH33Fv9yW3PLMuSZgZnLJeGi3q5a%2FtCI9pmPVz%2BdFk2&acctmode=0&pass_ticket=3HIlKyjzTAFL%2FxFljj2tEtyLfkTobYOBU23FblbSLUoESymFRz29uU1%2FRX7h6w2gnrjMHpQi8dKB1psxTVfoFA%3D%3D&wx_header=0#rd | 7 Papers & Radios | 无需注意力的预训练；被GPT带飞的In-Context Learning
https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652270885&idx=4&sn=0232d96036808b701d6c28c49ee50d8d&exportkey=n_ChQIAhIQEuq9NZ7HGXSEHZW4Ool20RKWAgIE97dBBAEAAAAAACu%2FCkw9QjAAAAAOpnltbLcz9gKNyK89dVj0uPYtdDJEuklrmT3ERHHWEPmtgMRjdlDgROzIuuSXCFcQCsPQK13bDmvu27PjE7byY7FkQq%2F6zWNJjXPbPBBcVpW0jxc14jVrfMDZ3XQBEOPlfkb9AH5%2FrtG1ZGJLgbiyByATNTAuPsriqflHDKdoEuj%2FfJdKpbi6tBWFJL0NU9JBATfl3DQAl5JnJxgVGX4llotlDtqDSfSV45fR5snf7u0TUDbJxyjIa2obk79iA%2BQvxECAKXxuDS2y%2By4j%2Fxjk4ERDQAFVYoJ9mqWcJo9GZrHzXjupgKSTbl8Ofj50NENCbXKXbRRTJdlnow62M4ap&acctmode=0&pass_ticket=3HIlKyjzTAFL%2FxFljj2tEtyLfkTobYOBU23FblbSLUovf3ue%2BRPJG7MUeTbcw7WZzRDq3lq4p6HKylx4LkngeQ%3D%3D&wx_header=0 | Meta发布首个「非参数化」掩码语言模型NPM：吊打500倍参数量的GPT-3
https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247492344&idx=1&sn=aeebe09529a9bd531249d6cdcd5a771f&exportkey=n_ChQIAhIQvU2bxWX5fMwQeblGWDIKoBKWAgIE97dBBAEAAAAAAAsCBOdq8GcAAAAOpnltbLcz9gKNyK89dVj0i6%2FseyLQ4pEXKuqV3uTfzjDy4Ne4380HnoSJ7LZDdMTtSbglamTzzNZPO9Yz%2BzKshTV0j0spcA4To4bqoCzFbuRs5oZnXz8%2FuzVbNuyK%2B5pkFmsZdIjeZNnaAwDVSgmR4uJs6coS2LR4PBjZWx9Afsb6Vv63hFRU5Hcd%2BLkM4Nnp7OEx%2F0o0ok%2FlfIU9opIL2jXUYuRx%2FFATissF1rb%2BUaaFq2KpI%2FJ4KkPrFBiaDC4xromXAGFmJhc5KZW2xotknSxiQagkGNt41ilB%2BE6Kuqhjc5usDxwfjDDToUBDhSnX8q9rs1HorWjtrcYQoVW6&acctmode=0&pass_ticket=3HIlKyjzTAFL%2FxFljj2tEtyLfkTobYOBU23FblbSLUpSDf1wOvjeezXbG9ET1fGmpu4cuIfoXjSTPxsQpWO56g%3D%3D&wx_header=0 | In-Context Learning玩法大全
https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650864545&idx=2&sn=cba397f55eb950da0fd794ae67c5c050&chksm=84e53fdfb392b6c9e2a7e360ac6d61c174691bdfdf5641f25d6caeb6adca80c5e99b50651b67&mpshare=1&scene=1&srcid=011696retelBJro4j4DxJL0z&sharer_sharetime=1673842926153&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQyKpMkoydhR1O%2BR%2F5UIzinxKWAgIE97dBBAEAAAAAAKwuJ5WrLbsAAAAOpnltbLcz9gKNyK89dVj0tmoDHRRqo5kCj1Qv6bIEUtpG4xo7EVvv3iX%2Fb%2F8ozFmXeTvuvF9AaSh9kW6WsLa%2FWB%2B76MBcF60gaqPc4l0N3blv5CteML7PYB3LTqPUyuDkoY%2Fvefn87kOdnxmJ0LiWlaaiInoQ02OR4Nexh5xgpBwqkk7iQ4zBSz97fqoxYOKDWc6G8Wz0oouXZsBZtmjIAQQAJGP1GVpBVQMG17nU4cBfribHme7ypaBgS62buxbF0aBdf7rl%2BWvaCgmOcbJ1GOLTfAoqR6jCXE3wSNqND2ULHi70SLiZfmqkBiTNld8UpSlna9WMJtSwAGeaCUr%2B&acctmode=0&pass_ticket=3HIlKyjzTAFL%2FxFljj2tEtyLfkTobYOBU23FblbSLUrq2L6n%2FO86L57931qjUdyQl6P4jx9HpLS6%2FHWr%2BWDmUQ%3D%3D&wx_header=0#rd | 被GPT带飞的In-Context Learning为什么起作用？模型在秘密执行梯度下降
https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650864503&idx=1&sn=ce2ee0ef67b5dbb3a5d317876fe6c4ee&chksm=84e53f09b392b61fd147a882c62182fc763c5a3b277088034bee6f7fe7f688f326cee51d05d5&mpshare=1&scene=1&srcid=0116IbftODk0hCiCpDtSZbCW&sharer_sharetime=1673842915752&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQNxXIXhvBYUSYqYAIcEMMGRKWAgIE97dBBAEAAAAAAHfeIrYeodAAAAAOpnltbLcz9gKNyK89dVj0FRXdhxZDWMBJ7xf76hktqTo5Mpc2kfAJ%2B4BXC3zFWKTYtba9X8ERBv4D89j7ztJFvs14NSAntweIt3IDMeXb00l%2BqXFVh6tCIGXFpuPKTL2hFB29bPs4HY8peLBZMxf%2BkZ7Jl%2BsUgcRJbwRTA7W7FhNPdEvwoQXXxzuZ4yF21bhyBjGCl9hkA96JzgMIO5zPyDC8b0LlxLePuuVeRQ%2FODNnJ2z%2FuN%2FM3lRjGsUWFSAQaOp1Gd31DH7Mn%2BEB8JBpZb%2BwQmQ7jSQeVYTw6R2YAjgTDtCNhikrHGW0cy%2Fe7DovtC391XqM9Prj46rfuYkSA&acctmode=0&pass_ticket=3HIlKyjzTAFL%2FxFljj2tEtyLfkTobYOBU23FblbSLUryiXQKeukyC1UTESVF%2FQzmPV58UkgRAhOEPkiHbp%2ByKg%3D%3D&wx_header=0#rd | 2022出圈的ML研究：爆火的Stable Diffusion、通才智能体Gato，LeCun转推
https://docs.google.com/document/u/0/ | Google 文档
https://arxiv.org/pdf/1909.01492.pdf | Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation - Arxiv-1909.01492

https://probml.github.io/pml-book/book2.html | https://probml.github.io/pml-book/book2.html
https://twitter.com/astonzhangAZ/status/1611400421255557122 | https://twitter.com/astonzhangAZ/status/1611400421255557122
https://twitter.com/cwolferesearch/status/1612099963013529609 | (2) Cameron R. Wolfe on Twitter: "OPT-IML (just released by @MetaAI) is a version of OPT-175B that's fine-tuned on a benchmark (OPT-IML Bench) of ~2000 instruction-based tasks. Prior work indicated that instruction-tuning is useful for LLMs, but we learn more details about this with OPT-IML... 🧵[1/8]" / Twitter
https://plato.stanford.edu/entries/wittgenstein/ | Ludwig Wittgenstein (Stanford Encyclopedia of Philosophy)
https://arxiv.org/pdf/2003.05997.pdf | Efficient Content-Based Sparse Attention with Routing Transformers - Arxiv-2003.05997
https://arxiv.org/pdf/2108.12284.pdf | The Devil is in the Detail Simple Tricks Improve Systematic Generalization of Transformers - ACL-EMNLP-2021_2021.emnlp-main.49
https://arxiv.org/abs/2212.13894 | LAMBADA Backward Chaining for Automated Reasoning in Natural Language - Arxiv-2212.13894
https://arxiv.org/pdf/2101.00010.pdf | UnNatural Language Inference - Arxiv-2101.00010
https://arxiv.org/pdf/2205.11482.pdf | Tracing Knowledge in Language Models Back to the Training Data - Arxiv-2205.11482
https://arxiv.org/pdf/2301.02828.pdf | Why do Nearest Neighbor Language Models Work? - Arxiv-2301.02828
https://openreview.net/pdf?id=-h6WAS6eE4 | pdf

https://book.douban.com/subject/35778212/ | ルームメイトと謎解きを (豆瓣)
https://book.douban.com/subject/36180916/ | The Red Death Murders (豆瓣)
https://book.douban.com/subject/35380220/ | アンデッドガール・マーダーファルス 3 (豆瓣)
https://book.douban.com/subject/35223121/ | 傍聴者 (豆瓣)
https://book.douban.com/subject/35581093/ | 指切りパズル (豆瓣)
https://book.douban.com/subject/35806504/ | 俺ではない炎上 (豆瓣)
https://book.douban.com/subject/35152470/ | ジョン・ディクスン・カーの最終定理 (豆瓣)
https://book.douban.com/subject/35982719/ | 灰かぶりの夕海 (豆瓣)
https://book.douban.com/subject/35216978/ | 揺籠のアディポクル (豆瓣)
https://book.douban.com/subject/5309519/ | Z Is for Zombie (豆瓣)
https://book.douban.com/subject/36159115/ | 密室狂乱時代の殺人 (豆瓣)
https://www.douban.com/search?q=11%E6%96%87%E5%AD%97%E3%81%AE%E6%AA%BB | 搜索: 11文字の檻
https://www.douban.com/search?q=%E3%82%B5%E3%83%BC%E3%82%AB%E3%82%B9%E3%81%8B%E3%82%89%E6%9D%A5%E3%81%9F%E5%9F%B7%E9%81%94%E5%90%8F | 搜索: サーカスから来た執達吏
https://www.douban.com/search?q=Butterfly%20World%20%E6%9C%80%E5%BE%8C%E3%81%AE%E5%85%AD%E6%97%A5 | 搜索: Butterfly World 最後の六日
https://www.douban.com/search?q=The%20Mask%20of%20the%20Vampire | 搜索: The Mask of the Vampire
https://www.douban.com/search?q=%E6%8E%A8%E7%90%86%E5%A4%A7%E6%88%A6 | 搜索: 推理大戦

https://movie.douban.com/subject/3073124/ | 月球 (豆瓣)
https://movie.douban.com/subject/1361276/ | 八部半 (豆瓣)
https://movie.douban.com/subject/26799731/ | 请以你的名字呼唤我 (豆瓣)
https://movie.douban.com/subject/2222996/ | 步履不停 (豆瓣)
https://ai.papers.bar/paper/8b992d3e4b6c11ed92e7bfd3ad86b7ef | Diffusion models as plug-and-play priors
https://arxiv.org/abs/2202.02435 | On Neural Differential Equations - Arxiv-2202.02435
https://arxiv.org/abs/2301.01821 | Parameter-Efficient Fine-Tuning Design Spaces - Arxiv-2301.01821
https://lilianweng.github.io/posts/2023-01-10-inference-optimization/ | Large Transformer Model Inference Optimization | Lil'Log
https://arxiv.org/abs/2212.04488 | Multi-Concept Customization of Text-to-Image Diffusion - Arxiv-2212.04488
https://twitter.com/giffmana/status/1608568387583737856 | https://twitter.com/giffmana/status/1608568387583737856
https://twitter.com/Hidenori8Tanaka/status/1613561620999077889 | https://twitter.com/Hidenori8Tanaka/status/1613561620999077889
https://twitter.com/hardmaru/status/1611237067589095425 | https://twitter.com/hardmaru/status/1611237067589095425
https://wandb.ai/carperai/summarize_RLHF/reports/Implementing-RLHF-Learning-to-Summarize-with-trlX--VmlldzozMzAwODM2 | Implementing RLHF: Learning to Summarize with trlX – Weights & Biases
https://arxiv.org/abs/2212.05015 | Robustness Implies Privacy in Statistical Estimation - Arxiv-2212.05015
https://arxiv.org/abs/2301.02241 | CiT Curation in Training for Effective Vision-Language Data - Arxiv-2301.02241
https://arxiv.org/abs/2212.14024 | Demonstrate-Search-Predict Composing retrieval and language models for knowledge-intensive NLP - Arxiv-2212.14024
https://arxiv.org/abs/2301.01947 | StitchNet Composing Neural Networks from Pre-Trained Fragments - Arxiv-2301.01947
https://arxiv.org/abs/2210.09276 | Imagic Text-Based Real Image Editing with Diffusion Models - Arxiv-2210.09276
https://www.youtube.com/watch?v=1aXOXHA7Jcw | (600) Greg Yang | Large N Limits: Random Matrices & Neural Networks | The Cartesian Cafe w/ Timothy Nguyen - YouTube
https://aclanthology.org/P18-1008/ | The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation - ACL Anthology
https://twitter.com/omarsar0/status/1607080018546417665?s=20&t=CdsM6CNvFJRvaABUZ5Bj8w | (3) elvis on Twitter: "2022: A Year in Review (ML Papers Edition) In this thread, let's take a look at some of the top trending ML papers of 2022 ↓ https://t.co/E4VIuF23HX" / Twitter
https://huggingface.co/blog/clipseg-zero-shot | Zero-shot image segmentation with CLIPSeg

https://www.99csw.com/book/10538/index.htm | 推理要在本格前_谷崎润一郎 芥川龙之介 梦野久作_在线阅读_九九藏书网
https://plato.stanford.edu/entries/montague-semantics/ | Montague Semantics (Stanford Encyclopedia of Philosophy)
https://arxiv.org/pdf/1812.06834.pdf | A Tutorial on Deep Latent Variable Models of Natural Language - Arxiv-1812.06834

https://arxiv.org/pdf/2102.07350.pdf | Prompt Programming for Large Language Models Beyond the Few-Shot Paradigm - ACM-2021_10114534117633451760
https://openreview.net/pdf?id=yf1icZHC-l9 | pdf
https://github.com/Timothyxxx/Chain-of-ThoughtsPapers | Timothyxxx/Chain-of-ThoughtsPapers: A trend starts from "Chain of Thought Prompting Elicits Reasoning in Large Language Models".
https://arxiv.org/pdf/2206.02336.pdf | On the Advance of Making Language Models Better Reasoners - Arxiv-2206.02336
https://github.com/WENGSYX/Self-Verification | WENGSYX/Self-Verification
https://arxiv.org/pdf/2212.09561.pdf | Large Language Models are reasoners with Self-Verification - Arxiv-2212.09561
https://github.com/amazon-science/auto-cot | amazon-science/auto-cot: Official implementation for "Automatic Chain of Thought Prompting in Large Language Models" (stay tuned & more will be updated)
https://arxiv.org/pdf/2212.10403.pdf | Towards Reasoning in Large Language Models A Survey - Arxiv-2212.10403
https://arxiv.org/abs/2205.12393 | Fine-tuned Language Models are Continual Learners - Arxiv-2205.12393
https://arxiv.org/pdf/2212.10923.pdf | Language Models as Inductive Reasoners - Arxiv-2212.10923
https://github.com/jeffhj/LM-reasoning | jeffhj/LM-reasoning: This repository contains a collection of papers and resources on Reasoning in Large Language Models.
https://arxiv.org/pdf/2210.11610.pdf | Large Language Models Can Self-Improve - Arxiv-2210.11610
https://openreview.net/pdf?id=5NTt8GFjUHkr | pdf
https://arxiv.org/pdf/2211.10435.pdf | PAL Program-aided Language Models - Arxiv-2211.10435
https://arxiv.org/pdf/2210.07128.pdf | Language Models of Code are Few-Shot Commonsense Learners - Arxiv-2210.07128
https://arxiv.org/pdf/2210.03493.pdf | Automatic Chain of Thought Prompting in Large Language Models - Arxiv-2210.03493
https://arxiv.org/pdf/2208.10760.pdf | How good are deep models in understanding the generated images? - Arxiv-2208.10760
https://arxiv.org/pdf/2203.14465.pdf | STaR Bootstrapping Reasoning With Reasoning - Arxiv-2203.14465
https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html | In-context Learning and Induction Heads
https://arxiv.org/abs/2010.03648 | A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks - Arxiv-2010.03648
https://arxiv.org/pdf/2111.02080.pdf | An Explanation of In-context Learning as Implicit Bayesian Inference - Arxiv-2111.02080
https://proceedings.neurips.cc/paper/2021/hash/86b3e165b8154656a71ffe8a327ded7d-Abstract.html | Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning - NeurIPS-2021_86b3e165
https://arxiv.org/pdf/2205.05055.pdf | Data Distributional Properties Drive Emergent In-Context Learning in Transformers - Arxiv-2205.05055
https://www.google.com/search?q=predictability+and+surprise+in+large+generative+models&oq=Predictability+and+Surprise+in+Large+Generative+Models&aqs=chrome.0.35i39j0i390l3j69i61j69i60.2084j0j1&sourceid=chrome&ie=UTF-8 | predictability and surprise in large generative models - Google Search
https://arxiv.org/pdf/2104.08691.pdf | The Power of Scale for Parameter-Efficient Prompt Tuning - Arxiv-2104.08691
https://arxiv.org/pdf/1906.02361.pdf | Explain Yourself! Leveraging Language Models for Commonsense Reasoning - Arxiv-1906.02361
https://github.com/HazyResearch/ama_prompting | HazyResearch/ama_prompting: Ask Me Anything language model prompting
https://reasonwithpal.com/ | PAL: Program-aided Language Models
https://github.com/reasoning-machines/pal/blob/main/pal/prompt/math_prompts.py | pal/math_prompts.py at main · reasoning-machines/pal

https://www.douban.com/note/833554818/?_i=3221336KLQjbnS | 【自翻】【武田绫乃】漠然与五体（重发试试）
https://www.google.com/search?q=%E5%B0%8F%E5%8C%85%E5%AD%90%E5%92%8CJumbo | 小包子和Jumbo - Google Search
https://www.google.com/search?q=%E9%80%81%E8%91%AC%E5%88%97%E8%BD%A6 | 送葬列车 - Google Search
https://www.google.com/search?q=%E6%B2%89%E7%9D%A1%E7%9A%84%E5%90%8D%E4%BE%A6%E6%8E%A2%E4%B8%8E%E9%9B%B7%E7%94%B5%E5%AF%86%E5%AE%A4 | 沉睡的名侦探与雷电密室 - Google Search
https://www.google.com/search?q=%E6%96%A9%E9%A6%96T%E5%AD%97%E4%B9%8B%E8%B0%9C | 斩首T字之谜 - Google Search
https://www.google.com/search?q=%E5%B0%8F%E6%9E%97%E6%B3%B0%E4%B8%89%E3%80%8Ajunk%E3%80%8B | 小林泰三《junk》 - Google Search
https://www.google.com/search?q=%E4%BA%91%E4%B8%AD%E4%BA%BA | 云中人 - Google Search
https://www.google.com/search?q=%E5%86%BB%E7%BB%93%E7%9A%84%E4%BF%84%E7%BD%97%E6%96%AF+%E6%A2%93%E5%B4%8E%E4%BC%98&newwindow=1&sxsrf=ALiCzsaW-G7Z4bmx_DHvIg80i_2Du1S0ew%3A1673220134952&ei=JlC7Y_DbOb6u5NoP2fS0wAo&ved=0ahUKEwjwhpnejrn8AhU-F1kFHVk6DagQ4dUDCBA&uact=5&oq=%E5%86%BB%E7%BB%93%E7%9A%84%E4%BF%84%E7%BD%97%E6%96%AF+%E6%A2%93%E5%B4%8E%E4%BC%98&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzoICAAQogQQsAM6BQgAEKIESgQIQRgBSgQIRhgAUF9YswJgnQdoAXAAeACAAWmIAbkBkgEDMS4xmAEAoAEBoAECyAEEwAEB&sclient=gws-wiz-serp | 冻结的俄罗斯 梓崎优 - Google Search
https://www.google.com/search?q=%E8%BF%BD%E9%9A%8F%E5%A5%B9%E7%9A%84%E6%97%85%E7%A8%8B | 追随她的旅程 - Google Search
https://www.google.com/search?q=%E5%B0%91%E5%B9%B4%E5%B7%B4%E6%AF%94%E4%BC%A6 | 少年巴比伦 - Google Search
https://www.google.com/search?q=%E9%A3%9F%E4%B9%8B%E4%BF%A1%E6%9D%A1 | 食之信条 - Google Search
https://www.google.com/search?q=%E6%AD%A4%E3%81%AE%E4%B8%96%E3%81%AE%E6%9E%9C%E3%81%A6%E3%81%AE%E6%AE%BA%E4%BA%BA | 此の世の果ての殺人 - Google Search

https://piazza.com/class/l75f9r92zbz4ha/post/349 | 6.7810 (142 unread)
https://math.stackexchange.com/questions/4611091/conditional-independence-property?noredirect=1#comment9718294_4611091 | probability - Conditional Independence property - Mathematics Stack Exchange
https://arxiv.org/pdf/2301.00774.pdf | Massive Language Models Can Be Accurately Pruned in One-Shot - Arxiv-2301.00774
https://arxiv.org/pdf/2105.03075.pdf | A Survey of Data Augmentation Approaches for NLP - Arxiv-2105.03075
https://arxiv.org/pdf/2108.08485.pdf | Language Model Augmented Relevance Score - Arxiv-2108.08485
https://github.com/apoorvumang/kgt5 | apoorvumang/kgt5: Sequence-to-Sequence Knowledge Graph Completion and Question Answering (KGT5)
https://arxiv.org/pdf/2210.14975.pdf | MABEL Attenuating Gender Bias using Textual Entailment Data - Arxiv-2210.14975
https://arxiv.org/pdf/2204.00408.pdf | Structured Pruning Learns Compact and Accurate Models - Arxiv-2204.00408
https://arxiv.org/pdf/2010.04762.pdf | Counterfactually-Augmented SNLI Training Data Does Not Yield Better Generalization Than Unaugmented Data - ACL-EMNLP| insights-2020_2020.insights-1.13
https://arxiv.org/abs/2210.01848 | Explaining Patterns in Data with Language Models via Interpretable Autoprompting - Arxiv-2210.01848
https://www.google.com/search?q=Robustness+of+Demonstration-based+Learning+Under+Limited+Data+Scenario&oq=Robustness+of+Demonstration-based+Learning+Under+Limited+Data+Scenario&aqs=chrome.0.69i59j69i61l2.188j0j1&sourceid=chrome&ie=UTF-8 | Robustness of Demonstration-based Learning Under Limited Data Scenario - Google Search
https://book.douban.com/subject/30448939/ | 夜光与独行者 (豆瓣)
https://ja.annas-archive.org/search?q=%E5%A4%9C%E5%85%89%E4%B8%8E%E7%8B%AC%E8%A1%8C%E8%80%85 | 夜光与独行者 - Search - アンナのアーカイブ
https://www.douban.com/group/topic/48243678/?_i=3216156KLQjbnS | [原创翻译]《死刑犯之谜 》法月纶太郎（完结）
https://arxiv.org/abs/2211.15029 | DiffusionBERT Improving Generative Masked Language Models with Diffusion Models - Arxiv-2211.15029
https://github.com/krasserm/bayesian-machine-learning | krasserm/bayesian-machine-learning: Notebooks about Bayesian methods for machine learning
https://arxiv.org/abs/2112.14757 | A Simple Baseline for Open-Vocabulary Semantic Segmentation with Pre-trained Vision-language Model - Arxiv-2112.14757
https://arxiv.org/abs/2107.06278#:~:text=Per%2DPixel%20Classification%20is%20Not%20All%20You%20Need%20for%20Semantic%20Segmentation,-Bowen%20Cheng%2C%20Alexander&text=Modern%20approaches%20typically%20formulate%20semantic,with%20an%20alternative%20mask%20classification. | Per-Pixel Classification is Not All You Need for Semantic Segmentation - Arxiv-2107.06278
https://arxiv.org/abs/1707.09835 | Meta-SGD Learning to Learn Quickly for Few-Shot Learning - Arxiv-1707.09835
https://arxiv.org/abs/1907.10529 | SpanBERT Improving Pre-training by Representing and Predicting Spans - Arxiv-1907.10529

https://arxiv.org/abs/2301.00303 | Rethinking with Retrieval Faithful Large Language Model Inference - Arxiv-2301.00303
https://arxiv.org/abs/2208.01066 | What Can Transformers Learn In-Context? A Case Study of Simple Function Classes - Arxiv-2208.01066
https://arxiv.org/abs/1905.02175 | Adversarial Examples Are Not Bugs, They Are Features - Arxiv-1905.02175
https://arxiv.org/abs/2112.01008 | Editing a classifier by rewriting its prediction rules - Arxiv-2112.01008
https://aclanthology.org/2022.acl-long.269/ | Is Attention Explanation? An Introduction to the Debate - ACL Anthology
https://arxiv.org/abs/2301.01379 | A Succinct Summary of Reinforcement Learning - Arxiv-2301.01379
http://proceedings.mlr.press/v139/liu21f/liu21f.pdf | Just Train Twice Improving Group Robustness without Training Group Information - PMLR-2021-liu21f
http://proceedings.mlr.press/v139/miller21b/miller21b.pdf | Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization
https://arxiv.org/pdf/1911.08731.pdf | Distributionally Robust Neural Networks for Group Shifts On the Importance of Regularization for Worst-Case Generalization - Arxiv-1911.08731
https://www.google.com/search?q=scalable+differential+privacy+with+certified+robustness+in+adversarial+learning&oq=scalable+differen&aqs=chrome.0.35i39j69i57j0i512j0i15i22i30j0i22i30l2j69i61l2.4136j0j1&sourceid=chrome&ie=UTF-8 | scalable differential privacy with certified robustness in adversarial learning - Google Search
https://www.google.com/search?q=a+unified+approach+to+interpreting+model+predictions&oq=a+unified+approach&aqs=chrome.0.0i20i263i512j69i57j0i512l8.4177j0j1&sourceid=chrome&ie=UTF-8 | a unified approach to interpreting model predictions - Google Search
https://arxiv.org/abs/1705.07874 | A Unified Approach to Interpreting Model Predictions - Arxiv-1705.07874
https://arxiv.org/abs/1711.11279 | Interpretability Beyond Feature Attribution Quantitative Testing with Concept Activation Vectors (TCAV) - Arxiv-1711.11279
https://www.google.com/search?q=snorkel+paper&oq=snorkel+paper&aqs=chrome..69i57j0i15i22i30j0i22i30j0i390l5.2204j0j1&sourceid=chrome&ie=UTF-8 | snorkel paper - Google Search
https://www.google.com/search?q=deep+learning+with+label+differential+privacy&oq=deep+learning+with+label+d&aqs=chrome.0.0i512j69i57j0i22i30j69i61.2665j0j1&sourceid=chrome&ie=UTF-8 | deep learning with label differential privacy - Google Search
https://www.google.com/search?q=commonsense+knowledge+mining+from+pretrained+models&oq=commonsense+knowledge+mining&aqs=chrome.0.0i512j69i57.4632j0j1&sourceid=chrome&ie=UTF-8 | commonsense knowledge mining from pretrained models - Google Search

https://www.bilibili.com/video/BV1Ke4y1j737/?spm_id_from=333.1007.tianma.5-4-20.click&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | 【完结】雨血 | 冷门武侠神作 | 古龙味道 | 声声流程解说【系列三部曲】_单机游戏热门视频
https://arxiv.org/abs/2208.01618 | An Image is Worth One Word Personalizing Text-to-Image Generation using Textual Inversion - Arxiv-2208.01618
https://distill.pub/2021/understanding-gnns/ | Understanding Convolutions on Graphs
https://distill.pub/2021/gnn-intro/ | A Gentle Introduction to Graph Neural Networks
https://arxiv.org/pdf/2204.07697.pdf | Theory of Graph Neural Networks Representation and Learning - Arxiv-2204.07697
https://arxiv.org/abs/1710.10571v5 | Certifying Some Distributional Robustness with Principled Adversarial Training - Arxiv-1710.10571
https://arxiv.org/pdf/1811.08489.pdf | Balanced Datasets Are Not Enough Estimating and Mitigating Gender Bias in Deep Image Representations - Arxiv-1811.08489
https://huggingface.co/course/chapter7/6?fw=pt | Training a causal language model from scratch - Hugging Face Course
https://arxiv.org/pdf/1806.08010.pdf | Fairness Without Demographics in Repeated Loss Minimization - PMLR-2018-hashimoto18a
https://arxiv.org/pdf/1801.09344.pdf | Certified Defenses against Adversarial Examples - Arxiv-1801.09344
http://proceedings.mlr.press/v80/wong18a/wong18a.pdf | Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope - PMLR-2018-wong18a
https://github.com/stefanoteso/awesome-explanatory-supervision | stefanoteso/awesome-explanatory-supervision: List of relevant resources for machine learning from explanatory supervision
https://arxiv.org/pdf/2103.10773.pdf | UniMoCo Unsupervised, Semi-Supervised and Full-Supervised Visual Representation Learning - Arxiv-2103.10773
https://arxiv.org/pdf/2012.00893.pdf | Evaluating Explanations How much do explanations from the teacher aid students? - Arxiv-2012.00893
https://pgmpy.org/examples/Monty%20Hall%20Problem.html | 2. Monty Hall Problem — pgmpy 0.1.19 documentation
https://arxiv.org/pdf/2009.06732.pdf | Efficient Transformers A Survey - Arxiv-2009.06732
https://arxiv.org/abs/2109.14119 | Stochastic Training is Not Necessary for Generalization - Arxiv-2109.14119
https://github.com/JonasGeiping/breaching | JonasGeiping/breaching: Breaching privacy in federated learning scenarios for vision and text
https://github.com/tuero/perturbations-differential-pytorch | tuero/perturbations-differential-pytorch: Differentiable Optimizers with Perturbations in Pytorch
https://github.com/google-research/google-research/tree/master/perturbations | google-research/perturbations at master · google-research/google-research
http://cs.emory.edu/site/aims/pub/wang21naacl.pdf | wang21naacl.pdf
https://www.google.com/search?q=Learning%20with%20Differentiable%20Perturbed%20Optimizers | Learning with Differentiable Perturbed Optimizers - Google Search
https://www.zhihu.com/question/66794537 | 卡尔维诺的《分成两半的子爵》好在哪里？ - 知乎
https://arxiv.org/abs/2102.06701 | Explaining Neural Scaling Laws - Arxiv-2102.06701
https://arxiv.org/abs/2102.04074 | Learning Curve Theory - Arxiv-2102.04074

https://arxiv.org/abs/2204.05832 | What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization? - Arxiv-2204.05832
https://arxiv.org/pdf/2212.09282.pdf | APOLLO A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning - Arxiv-2212.09282
https://twitter.com/G_S_Bhogal/status/1608135379055480833 | (2) Gurwinder on Twitter: "In 2022 I learned hundreds of useful concepts that improved my understanding of the world. Here are the 10 best:" / Twitter
https://twitter.com/DrJimFan/status/1607746957753057280 | (2) Jim Fan on Twitter: "The AI explosion is warping our sense of time. Can you believe Stable Diffusion is only 4 months old, and ChatGPT &lt;4 weeks old 🤯? If you blink, you miss a whole new industry. Here are my TOP 10 AI spotlights, from a breathtaking 2022 in rewind ⏮: a long thread 🧵 https://t.co/5k8Q6VQ0tD" / Twitter
https://twitter.com/larsiusprime/status/1605765732968677378 | (2) Lars "Land is a Big Deal" Doucet on Twitter: "@ojoshe @Noahpinion @emollick @delong Per your example about a child growing up; I would slightly caveat that human brains have been genetically "pretrained" with built in grain hardware capabilities primed for language acquisition, ie pretrained on the conversations of one's countless ancestors" / Twitter
https://arxiv.org/abs/2203.03466v2 | Tensor Programs V Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer - Arxiv-2203.03466

https://arxiv.org/abs/2212.03714 | Reconstructing Training Data from Model Gradient, Provably - Arxiv-2212.03714
https://arxiv.org/abs/2201.12675 | Decepticons Corrupted Transformers Breach Privacy in Federated Learning for Language Models - Arxiv-2201.12675
https://yaofu.notion.site/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f | A Closer Look at Large Language Models Emergent Abilities
https://arxiv.org/abs/2212.14024v1 | Demonstrate-Search-Predict Composing retrieval and language models for knowledge-intensive NLP - Arxiv-2212.14024
https://arxiv.org/abs/2205.05055 | Data Distributional Properties Drive Emergent In-Context Learning in Transformers - Arxiv-2205.05055
https://arxiv.org/abs/2207.10551 | Scaling Laws vs Model Architectures How does Inductive Bias Influence Scaling? - Arxiv-2207.10551
https://aps.arxiv.org/abs/2212.12131 | Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times? - Arxiv-2212.12131
http://www.cs.tau.ac.il/~oriram/projections.pdf | projections.pdf
https://arxiv.org/pdf/2212.10947.pdf | Parallel Context Windows Improve In-Context Learning of Large Language Models - Arxiv-2212.10947
https://arxiv.org/abs/2211.09260 | Task-aware Retrieval with Instructions - Arxiv-2211.09260
https://instructor-embedding.github.io/ | Instructor Text Embedding
https://arxiv.org/abs/2212.10560 | Self-Instruct Aligning Language Model with Self Generated Instructions - Arxiv-2212.10560
https://arxiv.org/abs/2212.09803 | Training Trajectories of Language Models Across Scales - Arxiv-2212.09803
https://arxiv.org/pdf/2212.09736.pdf | Don't Generate, Discriminate A Proposal for Grounding Language Models to Real-World Environments - Arxiv-2212.09736
https://arxiv.org/abs/2212.10466 | Controllable Text Generation with Language Constraints - Arxiv-2212.10466
https://arxiv.org/abs/2212.09849 | Dataless Knowledge Fusion by Merging Weights of Language Models - Arxiv-2212.09849
https://arxiv.org/pdf/2212.02027.pdf | Retrieval as Attention End-to-end Learning of Retrieval and Reading within a Single Transformer - Arxiv-2212.02027
https://www.google.com/search?q=Robustness+of+Demonstration-based+Learning+Under+Limited+Data+Scenario&oq=Robustness+of+Demonstration-based+Learning+Under+Limited+Data+Scenario&aqs=chrome..69i57j69i61l2.265j0j1&sourceid=chrome&ie=UTF-8 | Robustness of Demonstration-based Learning Under Limited Data Scenario - Google Search

https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf | Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss - NeurIPS-2019_621461af
https://openreview.net/pdf?id=AmUhwTOHgm | pdf
https://openreview.net/forum?id=fe2S7736sNS | $k$NN Prompting: Learning Beyond the Context with Nearest Neighbor Inference | OpenReview
https://proceedings.mlr.press/v162/rame22a/rame22a.pdf | Fishr Invariant Gradient Variances for Out-of-Distribution Generalization - PMLR-2022-rame22a
https://arxiv.org/pdf/2204.02937.pdf | Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations - Arxiv-2204.02937
https://svivek.com/research/publications/karidi2021putting.pdf | Putting Words in BERT's Mouth: Navigating Contextualized Vector Spaces with Pseudowords
https://arxiv.org/abs/2210.11560 | Finding Dataset Shortcuts with Grammar Induction - Arxiv-2210.11560
https://arxiv.org/abs/2210.14011 | Are All Spurious Features in Natural Language Alike? An Analysis through a Causal Lens - Arxiv-2210.14011
https://arxiv.org/pdf/2205.11432.pdf | Logical Reasoning with Span-Level Predictions for Interpretable and Robust NLI Models - Arxiv-2205.11432
https://arxiv.org/pdf/2210.17541.pdf | Zero-Shot Text Classification with Self-Training - Arxiv-2210.17541
https://www.eurecom.fr/en/publication/7095 | You are my type! Type embeddings for pre-trained language models | EURECOM
https://arxiv.org/pdf/2209.00840.pdf | FOLIO Natural Language Reasoning with First-Order Logic - Arxiv-2209.00840
https://arxiv.org/abs/2210.16637 | Beyond Prompting Making Pre-trained Language Models Better Zero-shot Learners by Clustering Representations - Arxiv-2210.16637
https://arxiv.org/abs/2205.12548 | RLPrompt Optimizing Discrete Text Prompts with Reinforcement Learning - Arxiv-2205.12548
https://arxiv.org/abs/2211.07830 | Prompting Language Models for Linguistic Structure - Arxiv-2211.07830
https://arxiv.org/pdf/2210.03575.pdf | Are Representations Built from the Ground Up? An Empirical Examination of Local Composition in Language Models - Arxiv-2210.03575
http://proceedings.mlr.press/v119/han20c/han20c.pdf | SIGUA Forgetting May Make Learning with Noisy Labels More Robust - PMLR-2020-han20c
https://arxiv.org/pdf/2208.05592.pdf | Patching open-vocabulary models by interpolating weights - Arxiv-2208.05592
https://www.google.com/search?q=empowering+language+modle&oq=empowering+language+modle&aqs=chrome..69i57j69i59j69i60l6.3325j0j1&sourceid=chrome&ie=UTF-8 | empowering language modle - Google Search
https://arxiv.org/abs/2211.08380 | Empowering Language Models with Knowledge Graph Reasoning for Question Answering - Arxiv-2211.08380
https://www.google.com/search?q=robust+curriculum+learning&oq=robust+curriculum+learning&aqs=chrome..69i57j0i390l4.3696j1j1&sourceid=chrome&ie=UTF-8 | robust curriculum learning - Google Search
https://openreview.net/forum?id=lmTWnm3coJJ | Robust Curriculum Learning: from clean label detection to noisy label self-correction | OpenReview
https://arxiv.org/abs/2210.10362 | CPL Counterfactual Prompt Learning for Vision and Language Models - Arxiv-2210.10362
https://arxiv.org/abs/2211.05110 | Large Language Models with Controllable Working Memory - Arxiv-2211.05110
https://arxiv.org/abs/2106.09129 | A Winning Hand Compressing Deep Networks Can Improve Out-Of-Distribution Robustness - Arxiv-2106.09129
https://arxiv.org/abs/2104.06644 | Masked Language Modeling and the Distributional Hypothesis Order Word Matters Pre-training for Little - Arxiv-2104.06644
https://arxiv.org/abs/2202.07206 | Impact of Pretraining Term Frequencies on Few-Shot Reasoning - Arxiv-2202.07206
https://arxiv.org/pdf/2110.11328.pdf | A Fine-Grained Analysis on Distribution Shift - Arxiv-2110.11328
https://arxiv.org/abs/2202.10054 | Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution - Arxiv-2202.10054
https://arxiv.org/abs/2002.08307 | Compressing BERT Studying the Effects of Weight Pruning on Transfer Learning - Arxiv-2002.08307
https://arxiv.org/abs/2204.01691 | Do As I Can, Not As I Say Grounding Language in Robotic Affordances - Arxiv-2204.01691
https://arxiv.org/abs/2204.04163 | Contextual Representation Learning beyond Masked Language Modeling - Arxiv-2204.04163
https://akariasai.github.io/files/evidentiality_arxiv_2021.pdf | evidentiality_arxiv_2021.pdf
https://arxiv.org/abs/2204.00511 | Learning Disentangled Representations of Negation and Uncertainty - Arxiv-2204.00511
https://arxiv.org/abs/2109.05602 | Good-Enough Example Extrapolation | Abstract
https://aclanthology.org/2021.acl-long.422/ | Counterfactual Inference for Text Classification Debiasing - ACL-ACL| IJCNLP-2021_2021.acl-long.422
https://openreview.net/forum?id=StHCELh9PVE | Analyzing Commonsense Emergence in Few-shot Knowledge Models | OpenReview
https://aclanthology.org/2021.emnlp-main.7/ | Raise a Child in Large Language Model Towards Effective and Generalizable Fine-tuning - ACL-EMNLP-2021_2021.emnlp-main.749
https://aclanthology.org/2021.emnlp-main.113.pdf | Shortcutted Commonsense: Data Spuriousness in Deep Learning of Commonsense Reasoning
https://arxiv.org/abs/2108.10604 | Prompt-Learning for Fine-Grained Entity Typing - Arxiv-2108.10604
https://www.google.com/search?q=%22Spurious+Correlations+in+Reference-Free+Evaluation+of+Text+Generation&oq=%22Spurious+Correlations+in+Reference-Free+Evaluation+of+Text+Generation&aqs=chrome..69i57j0i512.293j0j1&sourceid=chrome&ie=UTF-8 | "Spurious Correlations in Reference-Free Evaluation of Text Generation - Google Search
https://www.google.com/search?q=generating+data+to+mitigate+spurious+correlations+in+natural+language+inference+datasets&oq=Generating+Data+to+Mitigate+Spurious+Correlations+in+Natural+Language+Inference+Datasets&aqs=chrome.0.0i512.277j0j1&sourceid=chrome&ie=UTF-8 | generating data to mitigate spurious correlations in natural language inference datasets - Google Search
https://arxiv.org/pdf/2110.07736.pdf | Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models - Arxiv-2110.07736

https://arxiv.org/abs/2103.08933 | Reweighting Augmented Samples by Minimizing the Maximal Expected Loss - Arxiv-2103.08933
https://arxiv.org/abs/1908.06112 | Symmetric Cross Entropy for Robust Learning with Noisy Labels - Arxiv-1908.06112
https://arxiv.org/abs/2110.08387 | Generated Knowledge Prompting for Commonsense Reasoning - Arxiv-2110.08387
https://arxiv.org/abs/2205.12604 | Leveraging QA Datasets to Improve Generative Data Augmentation - Arxiv-2205.12604
https://arxiv.org/pdf/2205.12404.pdf | FLUTE Figurative Language Understanding through Textual Explanations - Arxiv-2205.12404
https://www.google.com/search?q=scalable+penalized+regression+for+noise+detection+in+learning+with+noisy+labels&oq=Scalable+Penalized+Regression+for+Noise+Detection+in+Learning+with+Noisy+Labels&aqs=chrome.0.0i512.289j0j1&sourceid=chrome&ie=UTF-8 | scalable penalized regression for noise detection in learning with noisy labels - Google Search
https://arxiv.org/abs/2203.07788 | Scalable Penalized Regression for Noise Detection in Learning with Noisy Labels - Arxiv-2203.07788
https://arxiv.org/abs/2210.15859 | You can't pick your neighbors, or can you? When and how to rely on retrieval in the $k$NN-LM - Arxiv-2210.15859
https://www.google.com/search?q=Uncertainty+Calibration+for+Ensemble-Based+Debiasing+Method&oq=Uncertainty+Calibration+for+Ensemble-Based+Debiasing+Method&aqs=chrome..69i57.262j0j1&sourceid=chrome&ie=UTF-8 | Uncertainty Calibration for Ensemble-Based Debiasing Method - Google Search
https://www.google.com/search?q=Can+Prompt+Probe+Pretrained+Language+Models%3F+Understanding+the+Invisible+Risks+from+a+Causal+View&oq=Can+Prompt+Probe+Pretrained+Language+Models%3F+Understanding+the+Invisible+Risks+from+a+Causal+View&aqs=chrome..69i57.346j0j1&sourceid=chrome&ie=UTF-8 | Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View - Google Search
https://www.google.com/search?q=owards+Interpreting+and+Mitigating+Shortcut+Learning+Behavior+of+NLU+Models&oq=owards+Interpreting+and+Mitigating+Shortcut+Learning+Behavior+of+NLU+Models&aqs=chrome..69i57j0i22i30l2.352j0j1&sourceid=chrome&ie=UTF-8 | owards Interpreting and Mitigating Shortcut Learning Behavior of NLU Models - Google Search
https://arxiv.org/abs/2103.06922 | Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU Models - Arxiv-2103.06922
https://www.google.com/search?q=PASTA%3A+Parameter-efficient+Tuning+with+Special+Token+Adaptation&oq=PASTA%3A+Parameter-efficient+Tuning+with+Special+Token+Adaptation&aqs=chrome..69i57j69i58.260j0j1&sourceid=chrome&ie=UTF-8 | PASTA: Parameter-efficient Tuning with Special Token Adaptation - Google Search
https://www.google.com/search?q=Can+NLI+Models+Verify+QA+Systems%E2%80%99+Predictions%3F&oq=Can+NLI+Models+Verify+QA+Systems%E2%80%99+Predictions%3F&aqs=chrome..69i57j0i22i30i625.266j0j1&sourceid=chrome&ie=UTF-8 | Can NLI Models Verify QA Systems’ Predictions? - Google Search
https://arxiv.org/abs/2110.07904 | SPoT Better Frozen Model Adaptation through Soft Prompt Transfer - Arxiv-2110.07904
https://www.google.com/search?q=few+shot+learning+with+noisy+labels&oq=few+shot+learning+with+noisy+labels&aqs=chrome.0.0i512j0i22i30i625.3666j0j1&sourceid=chrome&ie=UTF-8 | few shot learning with noisy labels - Google Search
https://www.google.com/search?q=dversarial+Examples+for+Evaluating+Math+Word+Problem+Solvers&oq=dversarial+Examples+for+Evaluating+Math+Word+Problem+Solvers&aqs=chrome..69i57j33i299l2.390j0j1&sourceid=chrome&ie=UTF-8 | dversarial Examples for Evaluating Math Word Problem Solvers - Google Search
https://aclanthology.org/2021.findings-emnlp.230.pdf | Adversarial Examples for Evaluating Math Word Problem Solvers

https://neurips2022-enlsp.github.io/papers/paper_27.pdf | paper_27.pdf
https://arxiv.org/abs/2211.09748 | Probing for Incremental Parse States in Autoregressive Language Models - Arxiv-2211.09748
https://arxiv.org/abs/2211.07906 | Hierarchical Phrase-based Sequence-to-Sequence Learning - Arxiv-2211.07906
https://proceedings.mlr.press/v162/lang22a/lang22a.pdf | Co-training Improves Prompt-based Learning for Large Language Models
https://proceedings.neurips.cc/paper/2021/file/dd17e652cd2a08fdb8bf7f68e2ad3814-Paper.pdf | Sequence-to-Sequence Learning with Latent Neural Grammars - NeurIPS-2021_dd17e652
https://arxiv.org/pdf/2205.01464.pdf | Inducing and Using Alignments for Transition-based AMR Parsing - Arxiv-2205.01464
https://arxiv.org/pdf/2012.07463.pdf | Parameter-Efficient Transfer Learning with Diff Pruning - Arxiv-2012.07463
https://arxiv.org/pdf/2011.09039.pdf | Sequence-Level Mixed Sample Data Augmentation - Arxiv-2011.09039
https://arxiv.org/pdf/1904.03746.pdf | Unsupervised Recurrent Neural Network Grammars - Arxiv-1904.03746
http://proceedings.mlr.press/v89/dieng19a/dieng19a.pdf | Avoiding Latent Variable Collapse with Generative Skip Models - PMLR-2019-dieng19a
https://proceedings.neurips.cc/paper/2018/file/b691334ccf10d4ab144d672f7783c8a3-Paper.pdf | Latent Alignment and Variational Attention - NeurIPS-2018_b691334c
http://proceedings.mlr.press/v80/kim18e/kim18e.pdf | Semi-Amortized Variational Autoencoders - PMLR-2018-kim18e
http://proceedings.mlr.press/v80/zhao18b/zhao18b.pdf | zhao18b.pdf
https://www.google.com/search?q=AMR+parsing&newwindow=1&sxsrf=ALiCzsbn8WyPccVNDRkgNr8PP4HYGnUsaA%3A1672644505820&ei=mYeyY9TTMb6g5NoPgL2L-Ak&ved=0ahUKEwiUhOusrqj8AhU-EFkFHYDeAp8Q4dUDCBA&uact=5&oq=AMR+parsing&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIFCAAQgAQyBQgAEIAEMgUIABCABDIFCAAQgAQyBQgAEIAEMggIABCABBDLATIGCAAQFhAeMgYIABAWEB4yCQgAEBYQHhDxBDIGCAAQFhAeOgcIIxDqAhAnOg0IABCPARDqAhC0AhgBOg0ILhCPARDqAhC0AhgBOgQIIxAnOgQIABBDOgcILhDUAhBDOgUIABCRAjoQCC4QsQMQgwEQxwEQ0QMQQzoLCAAQgAQQsQMQgwE6EQguEIAEELEDEIMBEMcBENEDOgoILhDHARDRAxBDOgsILhCABBCxAxCDAToLCC4QgAQQxwEQrwE6CwguEIAEELEDENQCOg4ILhDHARCxAxDRAxCABDoOCC4QgAQQxwEQ0QMQ1AI6CAgAEIAEELEDOg4ILhCABBCxAxDHARDRAzoNCC4QgAQQxwEQ0QMQCjoFCAAQhgNKBAhBGAFKBAhGGAFQ4xpYjydgiyhoAnAAeACAAfMBiAGHC5IBBTQuNy4xmAEAoAEBsAEUwAEB2gEGCAEQARgK&sclient=gws-wiz-serp | AMR parsing - Google Search
https://openreview.net/pdf?id=NiEtU7blzN | pdf
https://openreview.net/forum?id=NiEtU7blzN | Large Language Models Can Self-improve | OpenReview
https://arxiv.org/pdf/2009.07118.pdf | It's Not Just Size That Matters Small Language Models Are Also Few-Shot Learners - Arxiv-2009.07118
https://arxiv.org/abs/2211.01910 | Large Language Models Are Human-Level Prompt Engineers - Arxiv-2211.01910
https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650860970&idx=1&sn=2a5548eaf927cdd10fe14c3b0e3a4420&chksm=84e529d4b392a0c2cc418e4f2a1702716bd9dd876d4adeb2cdee9e48c31940b1ff4bcfd278c5&mpshare=1&scene=1&srcid=1116KsrixVC7xbDzLWOzGAoc&sharer_sharetime=1668532797367&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQWfEPwrafmpqFT5ZY2b%2FZXBKWAgIE97dBBAEAAAAAALUuBZ7j1psAAAAOpnltbLcz9gKNyK89dVj0h4rTIVRFwA7QH7JNzsEPoLbqfbZdKQACuYCIdMM0ANMI9xhaQzCplVSeSfUCgWEqTAsG5s2SB8okU6pgBbvTR%2BI1F%2Fy26sp1aqwAahFsxQQLRvHWk24Sx8%2B%2F3J156Sz11V3yzXDFdyVn%2FOMTiqQfxgpRxY%2F7jM%2FzsH%2FVt9QsKNi%2BtlqvRQF6PGZPLJc1GoLz0EsgCgWJYDQspp20KtN8UrnMd9gZz5mN37PJ6WYLzwtbkVERyeoa5knwCzlC1CPBMfzws9Q%2BWAesd6Ee47ZGxaz8bFCGCtR2cZ7HLNV48qSYyD%2Fc2CVU6fZR8j99VSZn&acctmode=0&pass_ticket=zUvJ6Bt8HsTNAPBA6U4qajGZ5sn0Pt6WEGVtKUG1A6PRrzVvi9Ssje8SgNLIF2OW&wx_header=0#rd | AI自动生成prompt媲美人类，网友：工程师刚被聘用，又要淘汰了
https://github.com/thunlp/PromptPapers | thunlp/PromptPapers: Must-read papers on prompt-based tuning for pre-trained language models.
https://arxiv.org/pdf/2103.10385.pdf | GPT Understands, Too - Arxiv-2103.10385
https://github.com/THUDM/P-tuning | THUDM/P-tuning: A novel method to tune language models. Codes and datasets for paper ``GPT understands, too''.
https://zhuanlan.zhihu.com/p/366771566 | Prompt-based Language Models：模版增强语言模型小结 - 知乎
https://zhuanlan.zhihu.com/p/488279606 | NLP Prompt系列——Prompt Engineering方法详细梳理 - 知乎
https://www.zhihu.com/question/439114659/answer/2409287359 | NLP 里 prompt engineering （设计一个问题的背景提示）有多重要? - 知乎
https://www.zhihu.com/question/508658141 | NLP 中 prompt learning 有哪些可能的天生缺陷？目前有什么样的方法来解决这样的缺陷？ - 知乎

https://arxiv.org/pdf/1909.13375.pdf | A Simple and Effective Model for Answering Multi-span Questions - Arxiv-1909.13375
https://arxiv.org/pdf/2104.07478.pdf | Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations - Arxiv-2104.07478
https://www.zhihu.com/question/68730628 | Batch normalization和Instance normalization的对比？ - 知乎
https://arxiv.org/pdf/2102.06062.pdf | Deep Learning with Label Differential Privacy - Arxiv-2102.06062
http://proceedings.mlr.press/v119/phan20a/phan20a.pdf | Scalable Differential Privacy with Certified Robustness in Adversarial Learning - PMLR-2020-phan20a
https://proceedings.mlr.press/v119/phan20a.html | Scalable Differential Privacy with Certified Robustness in Adversarial Learning - PMLR-2020-phan20a
https://arxiv.org/pdf/1902.02918.pdf) | Certified Adversarial Robustness via Randomized Smoothing - PMLR-2019-cohen19c
https://arxiv.org/abs/2003.03284 | TaskNorm Rethinking Batch Normalization for Meta-Learning - Arxiv-2003.03284
https://stats.stackexchange.com/questions/377287/need-help-understanding-the-benefit-of-score-function-estimator | reinforcement learning - need help understanding the benefit of score function estimator - Cross Validated

https://github.com/Yutong-Zhou-cv/Awesome-Text-to-Image | Yutong-Zhou-cv/Awesome-Text-to-Image: (ෆ`꒳´ෆ) A Survey on Text-to-Image Generation/Synthesis.
https://hackmd.io/@prajwalsingh/imagesynthesis# | Survey Text Based Image Synthesis - HackMD
https://www.zhihu.com/people/FesianXu | 徐土豆 - 知乎
https://mercurixito.github.io/ | Hako
https://arxiv.org/pdf/2102.12092.pdf | Zero-Shot Text-to-Image Generation | PDF
https://github.com/sheqi/GAN_Review | sheqi/GAN_Review: A Survey and Taxonomy of the Recent GANs Development，computer vision & time series
https://github.com/CompVis/taming-transformers | CompVis/taming-transformers: Taming Transformers for High-Resolution Image Synthesis
https://www.zhihu.com/search?q=SeqGAN&type=content | SeqGAN - 搜索结果 - 知乎
https://arxiv.org/pdf/1806.02817.pdf | Probabilistic Model-Agnostic Meta-Learning | PDF

https://arxiv.org/pdf/1611.01734.pdf | Deep Biaffine Attention for Neural Dependency Parsing - Arxiv-1611.01734
https://github.com/ShannonAI/mrc-for-dependency-parsing | ShannonAI/mrc-for-dependency-parsing
https://arxiv.org/pdf/2105.07654.pdf | Dependency Parsing as MRC-based Span-Span Prediction - Arxiv-2105.07654

https://francisbach.com/the-gumbel-trick/ | The Gumbel trick – Machine Learning Research Blog
https://bair.berkeley.edu/blog/2022/08/29/reverse-engineering/ | Reverse engineering the NTK: towards first-principles architecture design – The Berkeley Artificial Intelligence Research Blog
https://iclr-blog-track.github.io/2022/03/25/unnormalized-resnets/ | Normalization is dead, long live normalization! · The ICLR Blog Track
https://iclr-blog-track.github.io/blog/ | The ICLR Blog Track ·
https://movie.douban.com/subject/30171424/?source=2021_annual_movie | 拆弹专家2 (豆瓣)
https://www.douban.com/note/839749776/?_i=2431422KLQjbnS,2433879KLQjbnS | 《过去之书》、《未来之书》简评
https://proceedings.neurips.cc/paper/2020/file/322f62469c5e3c7dc3e58f5a4d1ea399-Paper.pdf | Bayesian Deep Learning and a Probabilistic Perspective of Generalization - NeurIPS-2020_322f6246
https://people.csail.mit.edu/ronitt/COURSE/F22/ | 6.5240 Sublinear Time Algorithms, Fall 2022
https://arxiv.org/pdf/2211.11719.pdf | First Steps Toward Understanding the Extrapolation of Nonlinear Models to Unseen Domains - Arxiv-2211.11719

https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247538592&idx=2&sn=3af58b8a69d3ac334a514f62ffbc4cc0&chksm=970f7f76a078f660f1a168d5302084fd5bd47a934f5e568b1746cc954bbeb027bf238009d767&mpshare=1&scene=1&srcid=1218pGLvdDyQbNtXim7P7tPh&sharer_sharetime=1671346345841&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQIv5oRiVsxKsBe%2FM%2F8a5pehKWAgIE97dBBAEAAAAAAP2tKTnkwfIAAAAOpnltbLcz9gKNyK89dVj0LFpb1pRiE0GDoI7IWxLeIF%2BB6LaijW4sBACAihH1pRA9AsdJ5j9aLP5vA0mQH1MsBRKceN7fH1TDNPXClyCYMG6tqlBOhAOD0q0waWf%2BwAjnuYUH8oYJDN9HRQikKDxBsPV4zoaPWqpMuPmEiixQwl6r2AButDrX047mH%2BcalhUH6pZHTA0gvwJ8Is1D75Q4e6u62r%2B6dSRMBWxEkqsS5OUNvpW%2BOgRw0bE%2BraxKhU2giNbudcL%2BXWcCmpRw8LXZxotEQfSYQoPux2UibPxX%2BZZeJE%2Fi7Mkdmmt%2BxJJkbtm4hxhD3TYRLFhA9I1oMH0l&acctmode=0&pass_ticket=B%2Fr2wa0S%2FM5lL%2FB24fD%2FIrSWC3GeT01IvggTYRYcwup5y6H%2Fp1m1UNUUaUC%2Fd0sL6rzQIBDg3kKPGV9TdODIAg%3D%3D&wx_header=0#rd | 大幅超越DALL·E 2和Imagen，斯坦福发布RA-CM3模型，融合检索与生成
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247537380&idx=2&sn=e39083ddd3ef4b7af88386761ef32380&chksm=970f7a32a078f324e79e3595feb25bd874c51add9bee3a84000b667e2a9475e116251869b6bf&mpshare=1&scene=1&srcid=1218axF8smChbSnHPgN06AXT&sharer_sharetime=1671346261151&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQ21kFn5QO3zq%2FIfeuaDoO3xKWAgIE97dBBAEAAAAAAGyxMN%2BZoP4AAAAOpnltbLcz9gKNyK89dVj0Yd06ao31f3X4bGtEc%2BzEB6ns80LartdjTdxQF5wTwDy20hJi4KWcjPBw6cHQ3GVlJ%2BflLdF%2FWryCUvJclD9BLFOH8j3jvCMJt4Kf7btqdIzhGPEog1GRQVBKE5043Qm8bZUjVyl2k20RBCmz4s1WGUsr5b0w6Ek5YMwAS%2F6%2B89EAjVTcH4qfEoUFCo128D%2FnvWxXAeGpwvVsbDduEBfp%2Ft6AEwNXIRIWQdwhFxt2SDW3if7kRRAVthtE4tU543ae0g1YJN532lS7uxi%2FuJnLNmwNI49kEVKiMKed5HKYbi%2BNlPCH5q2iNTkGSb8%2FLJTr&acctmode=0&pass_ticket=B%2Fr2wa0S%2FM5lL%2FB24fD%2FIrSWC3GeT01IvggTYRYcwupWc8AJ%2FN4acx237B3KDw7x1nmkCqb%2BsP5jDrU5l%2FHsoA%3D%3D&wx_header=0#rd | MIT发现语言模型内的事实知识可被修改？？
https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652259237&idx=1&sn=cb5cfcd242f6bd0db2e16039fb52643b&exportkey=n_ChQIAhIQgf%2BbYLwl6oMGw%2FoFoGNfLBKWAgIE97dBBAEAAAAAAO39JSP6k4EAAAAOpnltbLcz9gKNyK89dVj0ej6oJ%2BQLKMAE5bTxnV%2FSkAB6mtO1FPO3HNXyLGqrNfItyOWqw3limou79xA1BA7dJoNgYm0ptLc%2FsvnUWmDmyjU7d1gAerLAqzb0RjbdTMtTvWLiJ3XgbFLMk%2FI71eLEtWmudLslhaFJacH1FxwMqxUHzWfs3srXe9YrCLtyVDc3e3tjrkGUfKuLIDm9t9pxP3poXNuQhY1QIfqCYwMi%2F4YRiR5xUN1hKey42aHVjhJrl5hsVA6naAvRhNVwwwnXc%2F9SJpo%2BJkDN4rKnSH%2BwJIMtUtXqQHiP6IGDyaJkRtWL3ayuMKjf6INhokEMOebZ&acctmode=0&pass_ticket=B%2Fr2wa0S%2FM5lL%2FB24fD%2FIrSWC3GeT01IvggTYRYcwuqZeCg78PNJVjfkT8mPfGMrvEDF%2FnRd53u4VktMnA499A%3D%3D&wx_header=0 | AIGC大一统模型来了！CV界泰斗黄煦涛创立团队提出「全能Diffusion」
https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247565529&idx=2&sn=2ebbcdee73318a925664a5db7ae9513e&chksm=f9a0b256ced73b4067f294041819e8b49f76d00ced442e9e8cb6479cb78aa53484492667308c&mpshare=1&scene=1&srcid=1218Xs2cfHnFPJf0nmsbiv4L&sharer_sharetime=1671346237329&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQWyKo5EJ9rIcEL0g2bh3bBhKWAgIE97dBBAEAAAAAAP%2FkKsKhABwAAAAOpnltbLcz9gKNyK89dVj0hLfpghxDTMMjptFK2oOMuYW6eLieNlQmpNPfvzEvRszlEWrrB%2BCykSWHVCKRCSsCXUlZZYShKvwrAw7UJeKyHiEz0V%2BNZcL6wL3vQZJ8HfSCepUxILyvEJdx9g6Klzezu%2BL8L8jEvrsnMHnoEz%2FQcTq2TMCXdFIczVpYeO1qAb6ieLhj9YIOGXkqmg0zckEbqLlK5uwqILgqe%2F1%2BVY1b9VMg%2BwvQq2Y8n8WA043qDzY3B%2FUFjTGh7ONJAGjbnFSypWE5OKy%2BaM9SSQRp1igGdv33I6hpiD2SPT%2BfyYGK44oamVyG7AoGxV8y0kww%2BlWL&acctmode=0&pass_ticket=B%2Fr2wa0S%2FM5lL%2FB24fD%2FIrSWC3GeT01IvggTYRYcwurD%2BsY4u%2Bo39nB6zH3MmZWD6TS0GPZhM5cL2jeJ87IpEA%3D%3D&wx_header=0#rd | NeurIPS 2022高分论文！DeRy：让知识迁移像拼积木一样简单高效！
https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247655026&idx=1&sn=6ab838f15974ba349caea39bf03450d0&chksm=e8de2900dfa9a016e01e465a4e02e685aaa7807993660defcd0874d3e4962abacb7676b30fe9&mpshare=1&scene=1&srcid=1218p4T6wCLNCEHIpjdjDJrl&sharer_sharetime=1671346229351&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQoxurM3%2Fu729dD0sjYOtIZhKWAgIE97dBBAEAAAAAAGSyACyoEg0AAAAOpnltbLcz9gKNyK89dVj0%2BGSargFudjg%2Bu2lywggNHvbiRFArlho12H1E5Il%2BlDyiuP61IvERvCoyvDhQxP5OjJiIcTWldmNArX32qmLfsTbhs4DpXpq%2Bo959H1mzfz1v3L2lyxR%2B2Oy8eWARxICWwcEld5JubufkmT6ri%2FfseRhRkzEVlKi9uU%2FpZ9EWMdaQczrdlChedIR2PIAwguk1MFIs3LAYMQ%2FhoP6qmfpOyikTOcW8s%2Fhy0Ck0bc4EbOhZKvhh507U%2BLjzrSCllZEEZOJf3E1ZsBxnnNCKUGrhykXg9uEPQ7FDx4j2g%2BpH%2Faan9d9%2F%2B35dovu%2FGwgX5DOr&acctmode=0&pass_ticket=B%2Fr2wa0S%2FM5lL%2FB24fD%2FIrSWC3GeT01IvggTYRYcwurTeFLLgFUN%2FX6lpucFhYNM3qSP4D2rlw4ndVtnva2kJA%3D%3D&wx_header=0#rd | 何恺明团队12页论文新作剑指AIGC！“新CLIP”只需一个trick，训练速度快3.7倍！性能不降反升
https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247492157&idx=1&sn=e429832288f50701056dfbaddc84cb52&chksm=9bba6b59accde24f2333e4c5004d45803a5aca256c13ee9f36d40c9b2ea592b91522ca142bbd&mpshare=1&scene=1&srcid=1218DyBY4tX32tEMyCu5J2q2&sharer_sharetime=1671346219166&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQnoWWuaFpvsZSZXPZG7L3rBKWAgIE97dBBAEAAAAAAG95MhSFeYkAAAAOpnltbLcz9gKNyK89dVj0JaRzsFvQODKXQx1RT0ZnNnU6ZRDfUClVynHx3qx0a28%2B7VnxRFmDRZ%2FJPJAe83Hla0gcNb2acQBdqp4f439O4h5d5tyP4jet2KzZPeWWAXFvXUa%2B10g5suDMyZQGLNoBonMuYkayLr9n1VuEw0q36H6r9Evww2tZu1idCBv%2FVbYnUqsVVcDjO3ITMvjrog1NC%2BJ3xOAmXELWCuc9rzbQg2nBX1HgUmFvXRlirKy3gq2KEw9NseH0aoqGzVIOje6ovlWUpa6iRHtfpTlMU1ol7wZ%2F4FK0kusrrM2OPMmhrCM%2Bt7eCol2E2gISKsKUMBd3&acctmode=0&pass_ticket=B%2Fr2wa0S%2FM5lL%2FB24fD%2FIrSWC3GeT01IvggTYRYcwuoqrEEB65byq958NpPvNdBJa12vc2jRboP3Uoa8Dgn0NQ%3D%3D&wx_header=0#rd | 读了14篇论文，终于会拿捏Diffusion了
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247539037&idx=1&sn=e7214950088bb67210cfc356ad770c17&chksm=970f718ba078f89db9a19155b1a112cb6e425e7191a3a4999aa560b0f47f1a59c18da964f751&mpshare=1&scene=1&srcid=1218rcOan93DpU8Vvc3Ojum3&sharer_sharetime=1671346137769&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQs9SxE%2BQdpPGOSMrFFd0xPxKWAgIE97dBBAEAAAAAAEgyLmzBLUcAAAAOpnltbLcz9gKNyK89dVj0M2CtOBjpjUPagWPQECC1RBblws6Ten6W0n99v2zMc3lt%2BK8l2mG%2F45WTJnz37S%2B5nv%2F26WzMHhjWXVxy3LlnN4y4WK7jnFkn5GVo3UneBfDcXO3Xxj9BFBN4Ja4xo7Q0CMHvV5gntvjHHTXVSqXL49TLcM8P6c7T94iZ2zgdl2cK6kxHhMe1QycXh3PzFIR%2BFWMat3iBtZnsWPx1D7DeLdKQNtnzwM49tSwtv0%2BDQMsLQMICgxpYndeur6KKTUM3QJttD4BiLs0mAzCFbdYVUILLslPNN%2F%2BvstkpLi3PA32%2B1GZQalbKdUxEq6ZN8Mjt&acctmode=0&pass_ticket=B%2Fr2wa0S%2FM5lL%2FB24fD%2FIrSWC3GeT01IvggTYRYcwurIfNm7WOoqiOpddrAS3DLlwj1c8hYtXV9J7sTP2lwaxg%3D%3D&wx_header=0#rd | EMNLP 2022 最佳论文揭晓！这脑洞绝了….
https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650864226&idx=3&sn=44a9b991a0eba9b16ca8fc575d9e3685&chksm=84e53e1cb392b70a7382611dee0cec478c1180f573013aadf2c086f35bdda6ea4c728b9edead&mpshare=1&scene=1&srcid=1224kJ13eUE0cNf2GNH9AT5u&sharer_sharetime=1671836770873&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQ0HpEpWPPojp71BqhZscKARKUAgIE97dBBAEAAAAAAGAFKyyeEGQAAAAOpnltbLcz9gKNyK89dVj0NXuugfCzE2O8UZH3z06gk9Ug2obhOOxzF%2FNn6NA%2BHa%2B%2BGxhG5Zye4PG8yss5WXq99b8ElQB8eEfxDojjXexz33q8iQA7yeYAEntpQy3xT9iwfIrtq4Lux1%2BH61Xw06xitYu%2Filu4Geww0h604oyF0cfxz1kIQcC4X40ajXQ28lmQn5%2BGxqo7cOQWzhFVhEyUUpHF1h9NopX5PYwHtj%2Fm8Z7bvfUfzOnLNpu61m5SBm3qmtvENC8jmvwDr3o1QzmNY0%2FJOi7snzG0ie40p5dMEIzpgg01%2BfvthmCafw5k6mFWXH9fExhrokXmsgQqSA%3D%3D&acctmode=0&pass_ticket=RuGGTB4SplvCwKsI2KQuIGeMic0YEMrpZC0xdlI2bqbLFRwR3pxiQhTHo50quxu1e65Mg%2Ffoz1ivddwfWjHoWA%3D%3D&wx_header=0#rd | 统治扩散模型的U-Net要被取代了，谢赛宁等引入Transformer提出DiT
https://dreamfusion3d.github.io/ | DreamFusion: Text-to-3D using 2D Diffusion
https://download.arxiv.org/pdf/2211.08332v2 | Versatile Diffusion Text, Images and Variations All in One Diffusion Model - Arxiv-2211.08332
https://github.com/ashawkey/stable-dreamfusion | ashawkey/stable-dreamfusion: A pytorch implementation of text-to-3D dreamfusion, powered by stable diffusion.
https://arxiv.org/pdf/2212.00794.pdf | Scaling Language-Image Pre-training via Masking - Arxiv-2212.00794
https://book.douban.com/subject/20563216/ | 喂食者协会 (豆瓣)
https://book.douban.com/subject/34980177/ | 阿帕忒遊戲 (豆瓣)
https://book.douban.com/subject/35245405/ | 不知山上 (豆瓣)
https://book.douban.com/subject/34809604/ | 強弱 (豆瓣)

https://scholar.google.com/scholar?start=10&q=gpt-3+world+knowledge&hl=en&as_sdt=0,22&as_vis=1 | gpt-3 world knowledge - Google Scholar
https://arxiv.org/pdf/2206.01718.pdf | A-OKVQA A Benchmark for Visual Question Answering using World Knowledge - Arxiv-2206.01718
https://arxiv.org/abs/2104.13478 | Geometric Deep Learning Grids, Groups, Graphs, Geodesics, and Gauges - Arxiv-2104.13478
https://towardsdatascience.com/geometric-foundations-of-deep-learning-94cdd45b451d | Geometric foundations of Deep Learning | by Michael Bronstein | Towards Data Science
https://www.di.ens.fr/~fbach/learning_theory_class/lecture9.pdf | lecture9.dvi
https://arxiv.org/pdf/1906.11300.pdf | Benign Overfitting in Linear Regression - Arxiv-1906.11300

https://www.youtube.com/watch?v=gmoX2SSk9q4 | (516) OPTML++ 10/12/2022 Tengyu Ma - YouTube

https://book.douban.com/subject/36104107/ | 长安的荔枝 (豆瓣)

https://ai.stanford.edu/blog/understanding-incontext/ | How does in-context learning work? A framework for understanding the differences from traditional supervised learning | SAIL Blog
https://www.google.com/search?q=garg+et+al+2022+in+contex+learning&newwindow=1&sxsrf=ALiCzsZhehlGH0y-T-FZtEB9KIiu6jH14g%3A1670951396834&ei=5LGYY7i5Moue5NoPyK-KiAo&ved=0ahUKEwi4u8eDi_f7AhULD1kFHciXAqEQ4dUDCBA&uact=5&oq=garg+et+al+2022+in+contex+learning&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzoKCAAQRxDWBBCwAzoFCCEQqwI6BQghEKABOgUIIRCSAzoHCCEQoAEQCkoECEEYAEoECEYYAFDWA1jQK2D6LGgBcAF4AIABigGIAfcOkgEEMTMuNpgBAKABAcgBB8ABAQ&sclient=gws-wiz-serp | garg et al 2022 in contex learning - Google Search
https://arxiv.org/abs/2106.13884 | Multimodal Few-Shot Learning with Frozen Language Models - Arxiv-2106.13884
https://www.google.com/search?q=What+can+transformers+learn+in-context%3F+a+case+study+of+simple+function+classes&sourceid=chrome&ie=UTF-8 | What can transformers learn in-context? a case study of simple function classes - Google Search
https://www.google.com/search?q=text+based+real+image+editing+with+diffusion+models&oq=text+based+real+image+editing+with+d&aqs=chrome.0.0i512j69i57j0i22i30i625j0i390l4.6058j0j1&sourceid=chrome&ie=UTF-8 | text based real image editing with diffusion models - Google Search
https://arxiv.org/abs/2210.09276 | Imagic Text-Based Real Image Editing with Diffusion Models - Arxiv-2210.09276
https://arxiv.org/pdf/2112.10741.pdf | GLIDE Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models - Arxiv-2112.10741
https://arxiv.org/abs/2203.08414 | Unsupervised Semantic Segmentation by Distilling Feature Correspondences - Arxiv-2203.08414
https://arxiv.org/abs/2104.14294 | Emerging Properties in Self-Supervised Vision Transformers - Arxiv-2104.14294
https://arxiv.org/abs/2203.03605 | DINO DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection - Arxiv-2203.03605
https://arxiv.org/abs/2202.04200 | MaskGIT Masked Generative Image Transformer - Arxiv-2202.04200
https://www.google.com/search?q=NeurIPS+2022+best+papers&newwindow=1&sxsrf=ALiCzsb8ZFY7XK_IsIonniRyVV31KUdnrw%3A1670970420677&ei=NPyYY_2AKY3QkPIPwMiUqAQ&ved=0ahUKEwj96ury0ff7AhUNKEQIHUAkBUUQ4dUDCBA&uact=5&oq=NeurIPS+2022+best+papers&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIFCAAQgAQyBQgAEIYDOgoIABBHENYEELADOgUIABCiBDoICCEQwwQQoAFKBAhBGABKBAhGGABQhQVYqw1gyQ9oAnABeACAAc0BiAHSCZIBBTAuNi4xmAEAoAEByAEIwAEB&sclient=gws-wiz-serp | NeurIPS 2022 best papers - Google Search
https://blog.neurips.cc/2022/11/21/announcing-the-neurips-2022-awards/ | Announcing the NeurIPS 2022 Awards – NeurIPS Blog
https://www.2022.aclweb.org/best-paper-awards | ACL 2022 | 60th Annual Meeting of the Association for Computational Linguistics | Ireland
https://stableboost.ai/home?tab=2&createModel=1 | Stableboost
https://github.com/f/awesome-chatgpt-prompts | f/awesome-chatgpt-prompts: This repo includes ChatGPT promt curation to use ChatGPT better.
https://arxiv.org/pdf/2212.06138v1.pdf | CLIP Itself is a Strong Fine-tuner Achieving 85.7% and 88.0% Top-1 Accuracy with ViT-B and ViT-L on ImageNet - Arxiv-2212.06138
https://openreview.net/pdf?id=dF6aEW3_62O | You Can Have Better Graph Neural Networks by Not Training Weights at All Finding Untrained GNNs Tickets - OR-logconference-2022_dF6aEW3_62O
https://www.google.com/search?q=revisiting+parameter+efficient+tuning&oq=revisiting+parameter+efficient+tuning&aqs=chrome..69i57j0i546l3.5798j0j1&sourceid=chrome&ie=UTF-8 | revisiting parameter efficient tuning - Google Search
https://arxiv.org/abs/2202.07962#:~:text=Parameter%2DEfficient%20Tuning%20(PETuning),or%20even%20better%20than%20finetuning. | Revisiting Parameter-Efficient Tuning Are We Really There Yet? - Arxiv-2202.07962
https://arxiv.org/abs/2210.10693 | Robustness of Demonstration-based Learning Under Limited Data Scenario - Arxiv-2210.10693
https://arxiv.org/pdf/2209.08206.pdf | Selective Token Generation for Few-shot Natural Language Generation - Arxiv-2209.08206
https://github.com/jmschrei/torchegranate | jmschrei/torchegranate: A temporary repository hosting a pomegranate re-write using PyTorch as the backend.
https://pyro.ai/ | Pyro
https://www.google.com/search?q=atlas+neurips+map&oq=atlas+neurips+map&aqs=chrome..69i57j33i160l3j33i299l2.5276j0j1&sourceid=chrome&ie=UTF-8 | atlas neurips map - Google Search
https://atlas.nomic.ai/map/neurips | Atlas: NeurIPS 1987-2022
https://arxiv.org/abs/2211.11719 | First Steps Toward Understanding the Extrapolation of Nonlinear Models to Unseen Domains - Arxiv-2211.11719
https://www.google.com/search?q=arxiv+2207+09640&oq=arxiv+2207+09640&aqs=chrome..69i57j0i546l3j69i64.4104j0j1&sourceid=chrome&ie=UTF-8 | arxiv 2207 09640 - Google Search
https://www.google.com/search?q=text+guided+3D+diffusion+mdoel&oq=text+guided+3D+diffusion+mdoel&aqs=chrome..69i57j33i10i160.4530j0j1&sourceid=chrome&ie=UTF-8 | text guided 3D diffusion mdoel - Google Search
https://deepai.org/publication/3ddesigner-towards-photorealistic-3d-object-generation-and-editing-with-text-guided-diffusion-models | 3DDesigner: Towards Photorealistic 3D Object Generation and Editing with Text-guided Diffusion Models | DeepAI

https://huggingface.co/docs/transformers/main/en/task_summary#sequence-classification | Summary of the tasks
https://huggingface.co/docs/transformers/main/en/bertology | BERTology
https://huggingface.co/course/chapter7/6 | Training a causal language model from scratch - Hugging Face Course
https://huggingface.co/docs/transformers/tasks/language_modeling | Language modeling
https://huggingface.co/blog/big-bird | Understanding BigBird's Block Sparse Attention
https://huggingface.co/blog/pretraining-bert | Pre-Train BERT with Hugging Face Transformers and Habana Gaudi
https://www.google.com/search?q=huggingface+NSP+pretraining&newwindow=1&sxsrf=ALiCzsYiRHTC84K7ePgh8qADHAYKke1-Uw%3A1670871966029&ei=nnuXY4aiAb6h5NoP6pum6A0&ved=0ahUKEwiG6f-P4_T7AhW-EFkFHeqNCd0Q4dUDCBE&uact=5&oq=huggingface+NSP+pretraining&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIFCAAQogQyBQgAEKIEMgUIABCiBDIFCAAQogQ6CggAEEcQ1gQQsAM6BggAEBYQHjoFCCEQoAE6BQghEKsCOgcIIRCgARAKSgQIQRgASgQIRhgAUM8DWN0MYLwNaAFwAXgAgAFuiAGoCZIBAzYuNpgBAKABAcgBCMABAQ&sclient=gws-wiz-serp | huggingface NSP pretraining - Google Search
https://github.com/gyhandy/Caption_Generation_Detection/blob/main/imagenet_diffusion.yaml | Caption_Generation_Detection/imagenet_diffusion.yaml at main · gyhandy/Caption_Generation_Detection
https://spaces.ac.cn/archives/6853 | 为节约而生：从标准Attention到稀疏Attention - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/7575 | BERT-of-Theseus：基于模块替换的模型压缩方法 - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/7430 | Google新作Synthesizer：我们还不够了解自注意力 - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/7325 | 突破瓶颈，打造更强大的Transformer - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/7124 | 基于Conditional Layer Normalization的条件文本生成 - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/7921 | Performer：用随机投影将Attention的复杂度线性化 - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/7661 | 修改Transformer结构，设计一个更快更好的MLM模型 - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/8180 | Nyströmformer：基于矩阵分解的线性化Attention方案 - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/8027 | RealFormer：把残差转移到Attention矩阵上面去 - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/8823 | 从熵不变性看Attention的Scale操作 - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/8431 | 也来盘点一些最近的非Transformer工作 - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/9034 | 熵不变性Softmax的一个快速推导 - 科学空间|Scientific Spaces

https://github.com/thu-ml/tianshou/blob/master/test/discrete/test_dqn.py | tianshou/test_dqn.py at master · thu-ml/tianshou
https://tianshou.readthedocs.io/en/master/tutorials/dqn.html | Deep Q Network — Tianshou 0.4.10 documentation
https://nn.labml.ai/rl/dqn/replay_buffer.html | Prioritized Experience Replay Buffer
https://nn.labml.ai/rl/ppo/gae.html | Generalized Advantage Estimation (GAE)
https://nn.labml.ai/rl/dqn/ | Deep Q Networks (DQN)
https://nn.labml.ai/rl/dqn/model.html | Deep Q Network (DQN) Model
https://arxiv.org/pdf/1511.06581.pdf | Dueling Network Architectures for Deep Reinforcement Learning - Arxiv-1511.06581
https://nn.labml.ai/rl/index.html | Reinforcement Learning Algorithms
https://nn.labml.ai/rl/ppo/experiment.html | PPO Experiment with Atari Breakout
https://nn.labml.ai/rl/ppo/ | Proximal Policy Optimization - PPO
https://papers.labml.ai/paper/e2bbad8cc8e211eb80dc0bd1877e23b6 | Proximal Policy Optimization Algorithms
https://www.google.com/search?q=dqn+paper&oq=DQN+paper&aqs=chrome.0.0i512l2j0i22i30l5j69i65.1085j0j1&sourceid=chrome&ie=UTF-8 | dqn paper - Google Search
https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf | dqn.pdf

https://arxiv.org/abs/2102.12092 | Zero-Shot Text-to-Image Generation - Arxiv-2102.12092
https://arxiv.org/abs/2103.00020 | Learning Transferable Visual Models From Natural Language Supervision - Arxiv-2103.00020

https://arxiv.org/pdf/2202.04824.pdf | AdaPrompt Adaptive Model Training for Prompt-based NLP - Arxiv-2202.04824
https://arxiv.org/pdf/2209.08721.pdf | Joint Language Semantic and Structure Embedding for Knowledge Graph Completion - Arxiv-2209.08721

https://www.google.com/search?q=nlp+%E7%AC%AC%E5%9B%9B%E8%8C%83%E5%BC%8F+%E8%BF%9E%E7%BB%AD&newwindow=1&sxsrf=ALiCzsbzhaNNgu9RVTfeY15Nbj5Y8nwF9Q%3A1670459268193&ei=hC-RY5ijC_ah5NoPvIqUoAU&ved=0ahUKEwiY9ara4ej7AhX2EFkFHTwFBVQQ4dUDCBA&uact=5&oq=nlp+%E7%AC%AC%E5%9B%9B%E8%8C%83%E5%BC%8F+%E8%BF%9E%E7%BB%AD&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIFCCEQoAEyBQghEKABOgcIABAeELADOgQIABAeOgcIABAeEKIEOgUIABCiBEoECEEYAUoECEYYAFCTA1j8IWD5JGgCcAB4AIABogGIAcYIkgEDNy40mAEAoAEByAEBwAEB&sclient=gws-wiz-serp | nlp 第四范式 连续 - Google Search
https://jishuin.proginn.com/p/763bfbd6b477 | NLP的“第四范式”之Prompt Learning总结：44篇论文逐一梳理-技术圈
https://zhuanlan.zhihu.com/p/397004230 | CMU 刘鹏飞：NLP的第四范式 - 知乎
https://zhuanlan.zhihu.com/p/550705038 | NLP第四范式：连续性Prompt - 知乎
https://www.google.com/search?q=Few-Shot%20Text%20Generation%20with%20Natural%20Language%20Instructions | Few-Shot Text Generation with Natural Language Instructions - Google Search
https://spaces.ac.cn/archives/8295/comment-page-1 | P-tuning：自动构建模版，释放语言模型潜能 - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/8671 | 曾被嫌弃的预训练任务NSP，做出了优秀的Zero Shot效果 - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/8978 | 训练1000层的Transformer究竟有什么困难？ - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/8764 | ChildTuning：试试把Dropout加到梯度上去？ - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/7805 | 如何划分一个跟测试集更接近的验证集？ - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/7476 | 无监督分词和句法分析！原来BERT还可以这样用 - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/7764 | 必须要GPT3吗？不，BERT的MLM模型也能小样本学习 - 科学空间|Scientific Spaces
https://spaces.ac.cn/category/Big-Data/19/ | 分类 信息时代 下的文章 - 科学空间|Scientific Spaces
https://arxiv.org/abs/1907.07355 | Probing Neural Network Comprehension of Natural Language Arguments - Arxiv-1907.07355
https://github.com/chong-z/nlp-second-order-attack | chong-z/nlp-second-order-attack: [NAACL 2021] Code for "Double Perturbation: On the Robustness of Robustness and Counterfactual Bias Evaluation"
https://www.google.com/search?q=Learning+to+rank+using+gradient+descent+%E7%9F%A5%E4%B9%8E&newwindow=1&sxsrf=ALiCzsb7rXPRYJd0HBK0d_3R7rnzJz1ZCg%3A1656810633838&ei=iezAYoPYMsjPkPIPkOegyA8&oq=Learning+to+rank+using+gradient+descent+%E7%9F%A5%E4%B9%8E&gs_lcp=ChNtb2JpbGUtZ3dzLXdpei1zZXJwEAM6BwgAEEcQsAM6BwgAELADEEM6BggAEB4QFjoFCAAQhgM6BQghEKABSgQIQRgAUMMNWMMXYIcZaAFwAXgAgAGcAYgBswKSAQMwLjKYAQCgAQHIAQvAAQE&sclient=mobile-gws-wiz-serp | Learning to rank using gradient descent 知乎 - Google Search
https://github.com/kzkadc/ranknet | kzkadc/ranknet: PyTorch and Chainer implementation of RankNet
https://zhuanlan.zhihu.com/p/68682607 | 浅谈Learning to Rank中的RankNet和LambdaRank算法 - 知乎
https://zhuanlan.zhihu.com/p/26539920 | Learning to rank基本算法小结 - 知乎
https://www.google.com/search?q=PT-Ranking%3A+A+Benchmarking+Platform+for+Neural+Learning-to-Rank&sourceid=chrome-mobile&ie=UTF-8 | PT-Ranking: A Benchmarking Platform for Neural Learning-to-Rank - Google Search
https://wildltr.github.io/ptranking/ | PT-Ranking
https://wildltr.github.io/ptranking/ltr_diversification/Direct_Metric_Optimization/ | Direct Metric Optimization - PT-Ranking
https://github.com/wildltr/ptranking | wildltr/ptranking: Learning to Rank in PyTorch
https://www.google.com/search?q=learning+2+rank+pytorch&oq=&aqs=chrome.1.69i176j35i39i362l14.-1j0j7&sourceid=chrome-mobile&ie=UTF-8 | learning 2 rank pytorch - Google Search
https://www.google.com/search?q=Learning+to+rank+using+gradient+descent&sourceid=chrome-mobile&ie=UTF-8 | Learning to rank using gradient descent - Google Search
https://zhuanlan.zhihu.com/p/20711017 | 《Learning to Rank using Gradient Descent》 - 知乎
https://www.google.com/search?q=learning+2+rank+%E7%9F%A5%E4%B9%8E&newwindow=1&sxsrf=APq-WBthK18uxk-5suj97oxpybhDKukdHw%3A1645752044777&ei=7C4YYu_vLq7PkPIPvqWx4A4&oq=learning+2+rank+%E7%9F%A5%E4%B9%8E&gs_lcp=ChNtb2JpbGUtZ3dzLXdpei1zZXJwEAMyBQgAEKIEOgcIABBHELADOgQIIxAnOgUIIRCgAToICCEQFhAdEB46BggAEBYQHjoFCCEQqwJKBAhBGABQ7gRYyRhg4BtoAXABeACAAbUBiAGOCZIBAzAuOZgBAKABAcgBCMABAQ&sclient=mobile-gws-wiz-serp | learning 2 rank 知乎 - Google Search
https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/examples/README.md | adversarial-robustness-toolbox/README.md at main · Trusted-AI/adversarial-robustness-toolbox
https://www.google.com/search?q=randomized+smoothing+transformer&newwindow=1&sxsrf=APq-WBu7Pw-v7fWSW4M4vrry6XcqjfJdkw%3A1643881562115&ei=WqT7YYC3Bq_DkPIP1bOz2A8&oq=randomized+smoothing+transformer&gs_lcp=ChNtb2JpbGUtZ3dzLXdpei1zZXJwEAMyBQghEKABMgUIIRCgAToHCCMQsAMQJzoCCAA6BAgjECc6BQgAEIAEOgYIABAWEB46BwghEAoQoAFKBAhBGAFQqgRYoS9gijFoAHAAeACAAasCiAG9CpIBBTAuNi4xmAEAoAEByAEBwAEB&sclient=mobile-gws-wiz-serp | randomized smoothing transformer - Google Search

https://www.bilibili.com/video/BV1W3411H79o/?spm_id_from=333.851.b_7265636f6d6d656e64.4&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | 【Q君】最好的金田一游戏！扮演犯人全程抗压！中文字幕 01

https://www.zhihu.com/search?q=Muv-Luv%20Alternative&type=content | Muv-Luv Alternative - 搜索结果 - 知乎

https://waxworksmath.com/Authors/N_Z/Sutton/RLAI_1st_Edition/WWW/chapter_2.html | Chapter 2 in Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto.
https://ai.stanford.edu/~cbfinn/ | Chelsea Finn, Stanford University
https://ai.stanford.edu/~cbfinn/_files/neurips19_memorization.pdf | neurips19_memorization
https://slideslive.com/38922670/invited-talk-the-big-problem-with-metalearning-and-how-bayesians-can-fix-it | Chelsea Finn · Invited Talk: The Big Problem with Meta-Learning and How Bayesians Can Fix It · SlidesLive
https://www.zhihu.com/people/superbrother-58/posts | superbrother - 知乎

https://book.douban.com/author/1988420/books?sortby=collect&format=pic | 涩泽龙彦 Tatsuhiko Shibusawa的作品（81）
https://www.google.com/search?q=%E7%94%9F%E6%AD%BB%E7%96%B2%E5%8A%B3&oq=%E7%94%9F%E6%AD%BB%E7%96%B2%E5%8A%B3&aqs=chrome.0.0i355i512j46i512j0i512l8.2258j0j1&sourceid=chrome&ie=UTF-8 | 生死疲劳 - Google Search
https://www.google.com/search?q=%E4%BD%A9%E9%9B%B7%E5%85%8B&oq=%E4%BD%A9%E9%9B%B7%E5%85%8B&aqs=chrome..69i57j0i15i30.14359j0j1&sourceid=chrome&ie=UTF-8 | 佩雷克 - Google Search
https://www.google.com/search?q=%E6%AD%BB%E5%9C%A8%E5%8D%97%E6%96%B9&oq=%E6%AD%BB%E5%9C%A8%E5%8D%97%E6%96%B9&aqs=chrome..69i57j0i30j0i5i30.6308j0j1&sourceid=chrome&ie=UTF-8 | 死在南方 - Google Search
https://www.douban.com/search?cat=1001&q=%E5%88%9D%E6%9C%9F%E5%A5%8E%E5%9B%A0%E8%AE%BA | 搜索: 初期奎因论
https://book.douban.com/subject/35407396/ | 埃勒里·奎因论 (豆瓣)
https://www.wikiwand.com/ja/%E9%A3%AF%E5%9F%8E%E5%8B%87%E4%B8%89 | 飯城勇三 - Wikiwand
https://www.google.com/search?q=%E3%82%A8%E3%83%A9%E3%83%AA%E3%83%BC%E3%83%BB%E3%82%AF%E3%82%A4%E3%83%BC%E3%83%B3%E8%AB%96 | エラリー・クイーン論 - Google Search
https://book.douban.com/subject/35301339/ | テスカトリポカ (豆瓣)
https://book.douban.com/subject/33438831/ | 梦的宇宙志 (豆瓣)
https://www.google.com/search?q=%E7%99%BD%E9%B9%BF%E5%8E%9F&oq=%E7%99%BD%E9%B9%BF%E5%8E%9F&aqs=chrome.0.0i355i512j46i512j0i512j46i512j0i512l6.2640j0j1&sourceid=chrome&ie=UTF-8 | 白鹿原 - Google Search
https://www.google.com/search?q=%E8%8F%8A%E8%B1%86&oq=%E8%8F%8A%E8%B1%86&aqs=chrome..69i57j0i512l3j46i512j0i512l5.3634j0j1&sourceid=chrome&ie=UTF-8 | 菊豆 - Google Search
https://www.google.com/search?q=%E5%BA%9F%E9%83%BD&oq=%E5%BA%9F%E9%83%BD&aqs=chrome..69i57j46i512l2j0i512l6.5676j0j1&sourceid=chrome&ie=UTF-8 | 废都 - Google Search

https://github.com/AdamCobb/hamiltorch | AdamCobb/hamiltorch: PyTorch-based library for Riemannian Manifold Hamiltonian Monte Carlo (RMHMC) and inference in Bayesian neural networks
https://adamcobb.github.io/journal/hamiltorch.html | hamiltorch: a PyTorch Python package for sampling | Adam Cobb

https://www.bilibili.com/video/BV1Ne4y1W7E3/?spm_id_from=333.1007.tianma.3-2-8.click&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | 【完结】活侠传 | 武侠养成 | 多结局 | 最丑男主 | 潜力新游 | 流程实况合集【Demo】_哔哩哔哩_bilibili

https://book.douban.com/subject/26720073/ | 停在三樓 (豆瓣)
https://www.douban.com/note/839881322/?_i=9590426KLQjbnS | 《推理学导论9》评

https://book.douban.com/subject_search?search_text=%E6%8B%9F%E5%8D%97%E8%8A%A5 | 拟南芥 - 读书 - 豆瓣搜索
https://book.douban.com/subject/36029749/ | 大漠奇闻录 (豆瓣)

https://book.douban.com/subject/35653884/ | 潮汐图 (豆瓣)

https://www.douban.com/note/838649960/?_i=9006047KLQjbnS | 上交社刊《过去之书》《未来之书》个人评价
https://book.douban.com/subject/3783263/ | 日本推理名作选：浜尾四郎（卷一） (豆瓣)
https://book.douban.com/subject/35280426/ | 殘像17：新疫時期的殺意 (豆瓣)
https://www.douban.com/search?q=%E4%B8%A4%E4%BA%AC%E5%8D%81%E4%BA%94%E6%97%A5 | 搜索: 两京十五日

https://github.com/Dotkat-dotcome/OpenPrompt | Dotkat-dotcome/OpenPrompt: An Open-Source Toolkit for Prompt-Learning.
https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247561202&idx=1&sn=4e03162edf39679158611fc0824eaa73&chksm=f9a09d7dced7146ba3b8c3927809946728cb90f792b7814bc0a3952d6ed6a461d8c2a446f73e&mpshare=1&scene=1&srcid=1116KzfqoNjX560k9VVZriuD&sharer_sharetime=1668577228478&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQZh8nbjCyHRzzTn8lE%2Fa65hKWAgIE97dBBAEAAAAAAH8%2BOTdnAy8AAAAOpnltbLcz9gKNyK89dVj05nwTZvI6wCiktfH%2FfbMb1V%2F2OCH9pTBH71xJCohirMuwDjlTuK8%2BZkyhzAPFPZ6cRT6e%2FZx20%2BuWyNpU1OVcCO1jYEkVVr3wwndt8PpK7j5uGPMhnwpwlNM1WSMbBn3M%2Byyl5vAZA%2BfYN80ib%2B5ciiEYs2reZ4iKGBKmqcYLh8dVQIhZVBZx84snBC%2BndTOeg6G%2FdypC2Zj14DxkBvR0Phwv5lt%2F3TmAAA0yUP0cDjkk3fbpY%2FNS94SnDgzXMmu7UwM25iEMT017RuBMdRg86T%2BoLrBwbG7ArwkH%2BH44%2Bf6sa90XaoiJ1rzfDyo9ZRcq&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q21GkvEvW%2F4r7VlMLJ0Qu53&wx_header=0#rd | 顶刊TIP 2022！阿里提出：从分布视角出发理解和提升对抗样本的迁移性
https://mp.weixin.qq.com/s?__biz=MzIzNzU4OTAxMQ==&mid=2247492658&idx=1&sn=0e8ce066af3250de639594d63d2f270b&exportkey=n_ChQIAhIQ6T90Vu79lCHEZhhwCXZbRhKWAgIE97dBBAEAAAAAAO1cIVUxaDUAAAAOpnltbLcz9gKNyK89dVj08l018TKgFHmFqfFDEi91pHRt6gI4hYtHe%2BDdXHcCXlSbpeaoduId6u1NRNg8F%2Bb%2FyhYC%2BryxEAG%2FwZQ802qvuIGXHwtdYhil%2B%2BlM%2B%2BfRxBQLMTVOxMfrZKfvqciXo1WjJZHYuGniT%2FuEJxS9e2mrDpI5EhVoXL9ftPfOjTBUWVYukWxQGc9cGL2vVoysNb2AeIlbLAJs1ReAO2txEs2B%2FmqKHtPMtEIWLeLBvvH9qhOwjpMcSWyEqW1Iqstkq3TF3IF0X%2FvfaerVH65SWiMBqzRf2waSPDcp9n%2FmeNzeFbd6Z7HTfJD4qpK904GMsv31&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q1gvKC2xbFlioiyfsJ0UODN&wx_header=0 | CVPR2022 | Attention机制是为了找最相关的item？中科大团队反其道而行之！
https://mp.weixin.qq.com/s?__biz=Mzk0NDE5Nzg1Ng==&mid=2247504550&idx=1&sn=596768cc4fa51609e24fae65f358fd0f&exportkey=n_ChQIAhIQGLP9agIdyiNmJWfUH89bEBKWAgIE97dBBAEAAAAAAEGAME%2BqmqoAAAAOpnltbLcz9gKNyK89dVj0XvEc%2BYw6BaG3SknrLbZXdJnVGSMwFInAWBOMjn7TO6hubqf3U4hZeQB77L17y3JUpKaYv5eZClEwlSlsC2zgNH6hwr4ByGxVGWNUgplpY76XLOAGSJb%2F%2BFNgXCvqPSZYJHcdqB6J%2FKbL5QJVacp0yoyk59SRgmMIpBkePMX2ZdoJpJwhaiM8EsvmeWqy%2FXG9%2Fd%2BDNmsV6r32b7ugTroXbq98Bm29fqN1XbiOfK5HrWlpNJQK8wdPph9av1xewTmPUVNjJfCJl41jijqwj1jqEnJ6wT2Xxs325%2B2lAkrl1DRSoJ%2FQpgYawii55H4kXjhV&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q2JREvUaxd36WH3Kv19J6NO&wx_header=0 | 针对不平衡问题建模的有趣Loss。
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247519625&idx=1&sn=2ad9d58f8626c18ec7ef5bcd0e75d11e&exportkey=n_ChQIAhIQkHIV%2Fyg6LrO5CI%2BXFvTYAhKWAgIE97dBBAEAAAAAAKRAM8n0uJYAAAAOpnltbLcz9gKNyK89dVj0XCv066TM0ehUIpIgLZlOpj4G59QZckJT5wQaXur8Zz2s0vn6JdCfQlPt0JLudXA%2BZcLJLR4ufgsxqzqScI380nfPYbhWyk9Np6j0VxKrd0VcX%2BS%2FTVw8DupJEI5fRN7m1rPZPNo1S6epLNNaJr1U0mLafkcIxNdbKq7ZEHASwtQSG3uNJJF4IPzuGeWRQ%2BGAfmp1bDwPBj1rMTKhTdOwjdbmN8pUdiOtqC5RQAU%2FUvTmZPQZlouGdwYBV%2BAWv%2BCpXG9QfTm6i5HLKUwoJiKu%2BK4c3XIfPJgyLHXhtJ%2Bb6HdcX3BscdwcbKbJ4N0Ojzu3&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q3bdWILo60AnKxtn2MfZTE3&wx_header=0 | 图灵奖大佬+谷歌团队，为通用人工智能背书！CV 任务也能用 LM 建模！
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247530816&idx=1&sn=78ff024c6dac20bf7e19474b7b9c1b61&exportkey=n_ChQIAhIQtfrl6IpwFofy5rnZ8aC5ixKWAgIE97dBBAEAAAAAAI4XLqn07p4AAAAOpnltbLcz9gKNyK89dVj0aNDPffWThQiQZGDU0vDr6BYX4eBBJGKVO0Wsla8LA2bW2ZeYqEUj4JSLyau7wRjxxh6rXv8qJsBPH6nlrSdjOV16kVaQ8WT8Yur3Xp5yXwB2SbZt5p1q%2BR0WczWIOxQ2dynJCuwHtsZEVaMNWZ4rNIu1JglyoZLx8bgn40K%2B53aRvZlf1Etrjk2nWJaGA%2BjDpMh4tCGthwZxxLDPWTUPtS93MzlLfOV1g5dAHwrBWRE%2BQM8DSqJ3EcpzVuY9SwkgXnL00K1EqE9gp4H1J1u0CIjtDoPZj6JkAwqRy8W7KzvC5MBu5f4Pp8yGXv4XEe5Q&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q15x66qZzt8QojaGoMhMPtJ&wx_header=0 | Pix2Seq：谷歌大脑提出 CV 任务统一接口！
https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247550934&idx=1&sn=895ec412a849e15f13316acde9f53d53&exportkey=n_ChQIAhIQQM3vO3ml90wA9SEtD5zK5xKWAgIE97dBBAEAAAAAAOqXL3nnOQYAAAAOpnltbLcz9gKNyK89dVj07WVg2uWWlLNZWv%2BZND3dfPc14h%2Bve4hDPuwqIVgWgYxjc3rw5MJEaXanqN6C9QTerCCmmlfWY5QN2eH35lm4pL94GBgwNhDpF0XIll70qmnhzThQjXNk%2F81KmkG49dul3nN8fOyBxr3lh0D6%2BkbpITqIGt%2FO2RKkqfSOeeKDXgbaAePZdfqV9DMfAsMNFspLPposTfYtN3CJQALlY1EGZfLx7m83nwzNc%2FaC88vlyS8kpvYiMFxVxn%2F1G%2BgwUD7ntqer%2Bv6Kq5%2FcbXKhEU6yJVKAGv%2FXgY0puiptyx45ouWlklOVewmlY%2BLaVPk56T8T&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q0IsoLB3sN0%2BRzkapHeWeQJ&wx_header=0 | 从ACL 2022 Onsite经历看NLP热点
https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652238017&idx=5&sn=1e292aae780c708ea5b8279f4bc38630&chksm=f1268430c6510d26b92c939f0db2e3addb6e3bea7836db183b68e1aa90577d8690f304709f87&mpshare=1&scene=1&srcid=1116iosEQCYbDKHNQQw7iNgh&sharer_sharetime=1668545671724&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQq7shMG4e04AeookIN3sXFhKIAgIE97dBBAEAAAAAAENFNmVT%2FnAAAAAOpnltbLcz9gKNyK89dVj0hu6wpWjs0m5v3SFiO%2Bwp%2BEScTglxJ4ulRKo4Lpfds6rR0YNjIOEf8MYXaLEfHW3eExFAR9nlkWTHjEa99Z02R5QBJG%2FX9LWGZB7Pv5xVzmzjX5Oq1WYg8%2Fog4o30IMu9RsTQ1G71lQvPcKVYm4bjwkaM5eRopwChCFM5RzNH26Me1rv1yFrUq9jI0VBZ6PtKz4QpSa27IY6%2BI23XhOG3WKEOM30rSJ8kfSc0GXAcqkT4euz5D%2BBN%2FCtd1S7dj4B8iMWTMXwUnuODwdHhNhaUzKdNu9VezxwsLaV057bun552uA%3D%3D&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q2jNoQEbw1Bk8otYTL2sUaJ&wx_header=0#rd | 扩散模型在文本生成领域的应用
https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247631975&idx=3&sn=d10c4f76e95b8fb79d19d46f8a87323b&chksm=e8de7315dfa9fa03d2ced352e5dea6d7033496c21ed8425a0c0406b1ee9b4bf3d8b6fa085b5e&mpshare=1&scene=1&srcid=1116noySzvGcWs86ROpSAyl7&sharer_sharetime=1668544894840&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQ%2FZKRUzriagkJcXivCXKsMhKWAgIE97dBBAEAAAAAAJ8iE5s%2F%2BFMAAAAOpnltbLcz9gKNyK89dVj0F6ULe2fNQ9U5o6c5RcEe07w3k8yGOVskl5Sw%2BvWVyXM8TsPSEk0oDuzJ4iLIFO9J5tbKEG7k4HsCFntU3eL5Yj%2B0ggYWkfvKdYwqPPbjD7q0dVxDwbX%2BIQdh6Zr38ItO1MI%2F%2F%2FjOI6meeF4D8pUOI4Bis4vJ8tA8eNYIaAtqKcxkHo2sf9RFKi%2Bmu6mFO%2FQQqhmX5RHGY2Vft%2BI55ywTv%2BwOsG4AvUYEgpwM0bKiBDFmO%2B1%2BHY7O5CJIqM%2Bb5YyxN2fsWVjw9c2ZNq6ILdsA4ck7H2ecZXOKoXoLeozzj6wN%2BcsMuYz1F5K%2Bi9NPfAYU&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q0jkSGmaCg2ddIBG6lCexyf&wx_header=0#rd | 马毅沈向洋曹颖最新AI综述火了！耗时3月打造，网友：必读论文
https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650847145&idx=2&sn=ebf8ff8f8fb04a8ef4134272eb49ea88&chksm=84e573d7b392fac1536f10ec18963410c68ea5a2c0499cd7219cb781c8d6e167f680da7feb13&mpshare=1&scene=1&srcid=1116iLyaKa63ZA1yFC8YP2an&sharer_sharetime=1668544714998&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQ7YQQxfR7eyaVvBTi8n8sAhKWAgIE97dBBAEAAAAAAIj%2FK0e3yDIAAAAOpnltbLcz9gKNyK89dVj0FBTmqTNpwTXvWk5zawyd66I%2FWIYe6fymfYXUQbuF3atBRV43vVs11OAWJTAFsZmHBg9T8EDstRGOMwvIT5sl5gCGDias4gGyT8wMuYXs0m7bv00VLdBCNjqeM1AHakMgxF9drRYoPhrCMKAhudIZWh67qXO0vFQ4aSzp9IIY1XiPbziypSnSQsgj07SfZYc82dO5Qeqw9TsRNRDHyfdkg27L8pBMHwdbInYV9TxgJrG6J95XhE1IV5lyk7QXG6kUQwxJ3eQMFP%2Fw%2FPN6SM4JpCtkgpSlYx2w19Mut5rU%2BP47vloz3PrrIOrSL0xUbRYr&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q3HMVqoJhO0ed2F5eNYNNF8&wx_header=0#rd | 研究者意外发现DALL-E 2在用自创语言生成图像：全文黑话，人类都看不懂
https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650851425&idx=1&sn=08a31c3e5de9b9ce18defa5a0fbf7f17&exportkey=n_ChQIAhIQOAX16xa8xJzLMg%2BaWr%2F5AxKWAgIE97dBBAEAAAAAAE51Lp%2BHhE4AAAAOpnltbLcz9gKNyK89dVj0%2Fk6Kixt9YrrWHJUmbyL1z2SEGKWIqt0GTkZaz96EQUcz5EXrggIGUkWZ4e%2BhTwS3T3guw6cWa52hyPth3uDN4IKvBJgl5sXZe95QDFWdMZrE%2FgFFEq2DjOHGGpzTQBPwgmQyG5SD1TMklJjigo0V%2F5CDFxBdpbTypfYJVlfmp43kzLucVUB4IFW3T0cr9BEmbABfMGcxp%2BLTon8juUb3jE8fp5Dfynn4RZGCFbok1NiaNhpjT62cTnzw7Jv5N%2F9Vm%2F2LqmhogQvYOH9hkgcq0wMDamCDUS%2BgPaGgTbgLuPgV%2ByIYflNbWOzjrTZDYICO&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q2sjJxlfCuB124tYFeXBNfR&wx_header=0 | 陈天奇、王威廉等人推荐：ACL最佳论文奖得主给新入行研究者的一点建议
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247528370&idx=1&sn=fb3dc251519540cbbb1a4337ed1cabd9&chksm=970f4764a078ce72ca9f641d8574385ebec3a34429d4903d73cf2c69cbeb43ec0492a6a51f40&mpshare=1&scene=1&srcid=1116Bz5j8Zu9768shmqxqoNl&sharer_sharetime=1668544702215&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQiRLPTpUyVBUdNGagRNkZ2BKWAgIE97dBBAEAAAAAANN%2FBBfKY7gAAAAOpnltbLcz9gKNyK89dVj0%2Bme3uCrCARgZru48GM2kVVN%2BmmrzqtzHdkgsPES8%2BVBg3ztOULhnIR4AEkpjnPnqTd5GJVaUqxwEnNA45HpzrmlPGATS7qgwLJVrsstcOAdTCgpB1OoKgq8Q1drxB7bTf7Ba0bW85T5Ngxp1rh74e1sE26Eu6YIzP9DywzFPtSWuErS4p%2BD%2FZlSEPQGvnOhVEO6Vxe62mvgTFJvQuZAc2Q2YdEp15HYmdwmUqjdMYPwmMo6cbA0s1TRo%2Fv0BRKP7mfCh79VcQPxZ2EtwAlpitZSmbbIj1DrCestXK8qvDfEJgeuAoqQchzpYqM73wqf2&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q3cbg2OEnvH0nsPWHeSymKa&wx_header=0#rd | CMU 提出全新 GAN 结构，GAN 自此迈入预训练大军！
https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247640819&idx=1&sn=34048077ddf16734df0163b13c9cba85&exportkey=n_ChQIAhIQC9F%2FCGF5Zbjo1aaRTtrDKBKWAgIE97dBBAEAAAAAADkHLhfbnzkAAAAOpnltbLcz9gKNyK89dVj0J4RiKmK9Fnh6di61v6F%2FnpVShnUEE7oe2psqbsCPSXTgxRKNTlel8C5BPbLEcAGv16KMWLwb5umhIlgLGfAYqH6x4os01TofKCCsUXedDKbv7%2FHhrW9TmfBwjystTn6hRTjy9ttearhs%2BRRacrjl2zjihY9SywrXqaEoCvNLxtTZuSI9hoPxZDhubR8QL9DSVJXxi5fhca7hby7j7%2F%2F%2FY1n9iFOfuTUklbZVQuf0bi7DcuTDfKqSEC98zcCnnLMYqvkh7SNu64OdwLDVGSoIgO0RRWRq7twsD22bDVElj%2FDI9LIzLlAlzcGGXFhVLszJ&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q3TLWhCWfCNW3Y07Q5r96i6&wx_header=0 | “在所有事情上打败所有人”，微软多模态新作横扫12类任务，连纯视觉SOTA都刷新了
https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650859714&idx=2&sn=e065043ceccffdcb67b6d7c7e92b2227&chksm=84e52cbcb392a5aa0c7e430f5449b78360a32e67a36034d447df66418734c9c347e1c6213ba3&mpshare=1&scene=1&srcid=1116AswOu1XHbqOUD3lNdo2l&sharer_sharetime=1668533196137&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQEBDsDAZQgtXrD4lMhY4wmBKWAgIE97dBBAEAAAAAAB%2FDIPux%2BNUAAAAOpnltbLcz9gKNyK89dVj0mWEhPKExecajXImJU%2Fd85dW12sQwfnjepX5ItsXoYj5uE%2Fwa604KdguCRkbTfqXJ0fwutIg5DWLztvPA%2BL7g9tpHjnXAtyJVnE5r1vWwS%2Ftg2KmB13tzGYLnhzn31lIdV8A74FuOzQAuA1Erco2uG0dRLsTwtndd1JwVGkXJ%2BDbwm7vyClZ3yHycbIbZABaV6C8zBNPPxfQKflO7eE1VoJbZJfWcqG8oXsXIE5OXSa34C3PyFTFXPKByKaeVjTv%2FdwExBtuKQcIAp%2FDWBe1CZQz1WDm4ZmvPsTy2WfkxbR6W2vKd3zfnQtn9DxI4H8Q4&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q2UiDB3yzuu6K7%2BWz0dKcU9&wx_header=0#rd | 一行代码12倍加速Bert推理，OpenAI编程语言加持的引擎火了
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247529906&idx=1&sn=f30387b45d8e882adb4044dbd58d5859&exportkey=n_ChQIAhIQCudZbohJE3oYB8H5QNBjwxKWAgIE97dBBAEAAAAAAO72DJxUd6UAAAAOpnltbLcz9gKNyK89dVj0NTjKh7iKftDMbuAfhghIvd2g6L1sR2AOMvgzGKNp25KnhhCgvny6rdPaHRSVwJaL0e5oOaYSRN2e6zOlbbhen4cb2ktdJJ2%2F0FcDNTWFj4ii3swRBcHFOa%2BZ0Fnv2jr%2F5nKsl2axU1P6bwlFmKOx%2BCj7H4sOnM884Fz2%2BU9eFqWTZulbBaEHj2DR1vHfYDYiog%2BDS6JlczxLQk48pmuuM3tpmvEFYHKKtlXNBQkyFHaw1ImRKFOFI%2FrSiyFV31xNpjMFqAcIhbt8JA5iXfObUTe9FPc88eYiEB9zL7FSYi49YMeI%2Fkyf49wSMDz5MMmE&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q3t4hBtrl2cOqzk9ACigs94&wx_header=0 | ACL‘22杰出论文：Prompt范式有bug！
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247531708&idx=1&sn=0dd6bd51624cf3aa21d6679e19ed1796&exportkey=n_ChQIAhIQjpP9RcEikrD%2BOC1NlSKjQhKWAgIE97dBBAEAAAAAAIosJJIPvbwAAAAOpnltbLcz9gKNyK89dVj0L9gs%2BiLUhfFIOzjJrx1LkphvqN7QoKby5pIK959gKUz81hOzNZM%2FvBezCKRCrWco2%2BcZVnSSSnT8EBjOhTztpnQR6T%2BeCDx1dmUiF4vtaPjx3TXs6EVBcA5c6JdkpcY2OBLoiv5ZoMC7yLUQmzUnU6XTM39%2FDU9VqEJAyIaijd1mE62ExjT1ZY76KCEZg%2FB3sPfKcCxhKgLKQI9R5OwmMgrvdPdnzd7xp6QZK8AzSPuaHPQemhZSx5nOFk5COLqJU6LNVo%2FPM7uyA1kozGz456Lf92DyPv%2BSQ%2B9O1kPqIVb%2BdHh74RAWsObEFmIf5oHm&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q3GNbsGIKh6jteJn0S6dXGk&wx_header=0 | MIT指出公开预训练模型不能乱用
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247533143&idx=1&sn=30b7725412f51fc6e049466402999c98&exportkey=n_ChQIAhIQA7UMun1XuKEKg8F0M%2B3%2FIRKWAgIE97dBBAEAAAAAAIXTINfW01YAAAAOpnltbLcz9gKNyK89dVj0G1LRwJ74nv%2FpZNyW%2BhDkaNr5MLC2sFUMKhPJAqesEGjcvxlE9DK4aUzoafqZKnfCFMJUMnHyapzNXBbuBpxSqRDLkS6hrUn4JLwCHe5VsRU5ZqAVGcX%2Fc6GhciMFwSjsAdHa4SM42%2FsOc%2FSBTR7%2BIW3tKKCZddvaxksYd8LGMi9ugEgSSj3VKDmAGWGldjnXUZ8QhRNv955l32Kru%2BTZyIAH7z64FlGkYY9Aony4bKcIl%2FAfucS2lVU6vAy%2BzkDk7fpn%2B%2FnoDZFJiCB513%2FZfiNjDw637PQB8dUDGkZOcpApXXvyt34XBkFhnsC2ccBz&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q3ZgExJjmmZ%2FP%2F%2FPYzQ1lNP&wx_header=0 | GPT-3 泄露了我的真实姓名

https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247531708&idx=1&sn=0dd6bd51624cf3aa21d6679e19ed1796&exportkey=n_ChQIAhIQ7XraOwDDZUo9JAdAQEhp%2FRKWAgIE97dBBAEAAAAAAF0WLQWcV2cAAAAOpnltbLcz9gKNyK89dVj092%2F1BY5VluWiHtdbuuQR1zyTqn9bjzPlu9XYuZGQYSuMlNBySPzus0G3vWcuhC6iRl%2Bz4zBUsmUifs5V4ZJAft6945Mj3APCAeBiOGdRn9aSBhocNIG6uoHaVyLN3zlG4e%2BeKMIYoZRPiAoCdd%2FPu6kiGKmz%2BLbVAihSWzGfyvIzlCjYfXkrzNHZqaA%2BEkkU%2Bp%2BFNggF4h%2B1L25znq0IJpXGVHtgtWEdFypgPdfoD8AxMmkz705jCa%2BIhzH9OEIv%2Bp4ZehS2NhP9bdxViLXonv2ig9sW6wnpI1sk79T5xroqRk7AyFWKsS6dEp%2FgXPKs&acctmode=0&pass_ticket=zUvJ6Bt8HsTNAPBA6U4qajGZ5sn0Pt6WEGVtKUG1A6McGlT1Awcq8l9VBLNEv8Kt&wx_header=0 | MIT指出公开预训练模型不能乱用
https://mp.weixin.qq.com/s?__biz=MzIzNzU4OTAxMQ==&mid=2247493187&idx=1&sn=6e99d3b6e4eba8fec06380af8bcd2050&chksm=e8c4e13fdfb3682927f550094936768586650ab5b6a7d3bc71f1bd0ceccc31432e6775534f5f&mpshare=1&scene=1&srcid=11166sE47MSxzzvkUoTxmhmK&sharer_sharetime=1668532819886&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQpSZbFxroUyu57ilR3nLejBKWAgIE97dBBAEAAAAAAJkSIg4U%2ByAAAAAOpnltbLcz9gKNyK89dVj0HrbZO0aLCzga8hiKwGFWBE50CSIN2t99u%2FpFy80H5sk9k9JS3iIElMjg%2FTYkoV5%2FOo3BdHQX5fQECbw6nRMN16uBOrKwmITooNU5PpYqEWXpw4d9H7FAQvqetCcfWIKyklAh%2F%2BBKYR%2FG9uVytir1hThKxExGHyiorzn9XKgLx1NfYCVQUUbKNsLhmrHceZiiOvd%2BuvRQ%2BSXVCToJH5L%2BVXOO4yjCq2mcRbYBC1BR2phF%2Fpc9euzO1sOJBzCNEYfy9RXp1M4yXMk%2B3P2FEwyo6VDeCYwijhPkzPPu9g7P7FIQnDdlhjvr7FUuTL1nzGGN&acctmode=0&pass_ticket=zUvJ6Bt8HsTNAPBA6U4qajGZ5sn0Pt6WEGVtKUG1A6MYnG3J6GgrtlVrof5z3aOc&wx_header=0#rd | WACV2022 | 一张图片只值五句话吗？UAB提出图像-文本匹配语义的新视角！
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247536935&idx=1&sn=3593e2b71af9d7142b03978a67052f30&exportkey=n_ChQIAhIQvRty0%2BARyeH%2FhQekVI5ZKRKWAgIE97dBBAEAAAAAAOcHODZCFiAAAAAOpnltbLcz9gKNyK89dVj0omvS8NMhsMo%2FVTJxMm0GS3ipPXJ%2BoIFSyt%2F79de4MUhq77Dl1nRIOonso0YHZZ%2FE6aUrw5nL5rnkZUGirFsxgzyesp2Hg2cAvsR8h8V7%2BqUuPCZPGIpqppbpsVqomdMy%2FzMFu3sXj7zM6mcV4qnre0v4UGKSz%2FJ0JW0ZN%2Bql0eN%2Fyu0oI%2FPBuYf%2BqxETmO5YfFjDv1Dfmx379hJ5qC3g2b%2BgGUMb6eFjgoiAV0lVZTL0ttHCPhwkCbAlSGY6gz6cgokzDxUKsRtbvhqlDbKGEFjpzN%2BsP4U8nm3dbbbVKzvB%2BH7IPGkoqJn03c6y80Rw&acctmode=0&pass_ticket=zUvJ6Bt8HsTNAPBA6U4qajGZ5sn0Pt6WEGVtKUG1A6N7LqcDy72qCLyIgoAc8OJ2&wx_header=0 | ICLR 2023 最高分论文被锤抄袭？？
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247536565&idx=1&sn=bf381c5092de28dfc54326cefa7df264&chksm=970f6763a078ee751e41373fa8a34b10245f91a51619e92b156b43eed05080ff02d67825d771&mpshare=1&scene=1&srcid=1116fdPSMiJe3AqPttYaBva4&sharer_sharetime=1668532870713&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQc%2FP4q8UrlGQiZT8I6z4bvRKWAgIE97dBBAEAAAAAAJqvLN%2BmRH0AAAAOpnltbLcz9gKNyK89dVj0jgeUqtax0Pr4rAlLHk8dLiAgiX1O1QfQISlBU3567cdxvvo8pQckA8IQ%2FV3Y6vuabjUelVXrbt9w%2BEo4490j9%2FDxVYv4ahmbBDG87EsIhLWqnLND8eKqz1uclUw%2FD95ULcVpXZRE7KZokqj2aNwg3IAsMd0JcAVYid5dCp5E8LS8DfhxgPzFAYaEeysUpjIUMu5e4w8JZlgl0E61rR%2BiMwHUg3ghdcIpo%2F2rSdODPum03mAtr75yGbxO9DI6Q4TlFZv14ePqFQJ9objFGmgYEBnsygOl1Tz7vg%2BMaJdiaEW2kRyLxqGju8oZa3yGELdC&acctmode=0&pass_ticket=zUvJ6Bt8HsTNAPBA6U4qajGZ5sn0Pt6WEGVtKUG1A6M4VemB5CPEuPfthxbERKwf&wx_header=0#rd | AI取代人类，可以自动生成prompt了
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247536935&idx=1&sn=3593e2b71af9d7142b03978a67052f30&exportkey=n_ChQIAhIQA3oIolGi7kKNEC4aELHqBRKWAgIE97dBBAEAAAAAAEGlA1xy%2FPkAAAAOpnltbLcz9gKNyK89dVj03sw89IITQ%2BLgHRDDPgTW%2BonXNlWcBRzZjU9AdffLwr1Fh46%2Fcv4NNi3Y82ZbnEjNUIyHhqykrQxf1%2BLtvuxb%2FdP9TP%2FE2x0P8RObdvvi4sFMjeDMBxZxBUJVde9q%2Bz6OJl7xBNeVORUqfYC2sQM4U1gBGvtItsN06N2%2BmE%2BHag0AFNj%2ByvbzCHbnVpfbEfds3lrcryisusuztsJjhHl2sgRgvUBU0vgU8p%2B7UWS9dq9gadVb4eVF3d94mTeNglg2s8GJyvpY77XpO%2FN3kzXtBBMnIroqXeUliEnv2ThyXdm4cX%2FDJvNtHeW5hiAey0PP&acctmode=0&pass_ticket=zUvJ6Bt8HsTNAPBA6U4qajGZ5sn0Pt6WEGVtKUG1A6M6WdYO36NrmwEtOS6I%2Fz3o&wx_header=0 | ICLR 2023 最高分论文被锤抄袭？？
https://mp.weixin.qq.com/s?__biz=MzIzNzU4OTAxMQ==&mid=2247493187&idx=1&sn=6e99d3b6e4eba8fec06380af8bcd2050&chksm=e8c4e13fdfb3682927f550094936768586650ab5b6a7d3bc71f1bd0ceccc31432e6775534f5f&mpshare=1&scene=1&srcid=11166sE47MSxzzvkUoTxmhmK&sharer_sharetime=1668532819886&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQpkdfsWvK0kwkcqM9sk0kZBKWAgIE97dBBAEAAAAAABzhFPUs8zgAAAAOpnltbLcz9gKNyK89dVj0UzVbJxU93yyIE%2BAaT2OOeRQVCmQW%2FTnB88ubleERdlfuX%2FZnbZhGu9W5IgYQIZ2CRpOngHXQBI6lphBBlWQXWy4wOsjwbo2c1YjvVTue4AObxbrKv70LWw1nuZNjNX8bjDZt15uZLf6zpbjNFKlm718M9tG1DfBmcXjZ%2B0i3S0GiQcM4QJQ9lQ9W42uDGM2lPyyGWXcAMhIlFfDOrWmJfpkv9ar5Nbfm4LZjP1v8JCkmZE9mKQQ4r9MTLYes3fTWuzFEO8IY5LB9ctRutip57J89AXT4zAzmfjG0nLNgC8JUv26pPAOGJCSVtEczoHD5&acctmode=0&pass_ticket=zUvJ6Bt8HsTNAPBA6U4qajGZ5sn0Pt6WEGVtKUG1A6ObPH4kDzfuf96tj9OMolxN&wx_header=0#rd | WACV2022 | 一张图片只值五句话吗？UAB提出图像-文本匹配语义的新视角！
https://www.cs.princeton.edu/courses/archive/fall22/cos597G/ | COS 597G: Understanding Large Language Models

https://www.douban.com/note/790069051/?_i=8488771KLQjbnS | 【民翻】白井智之 食之信条
https://www.google.com/search?q=%E9%82%AA%E6%95%99%E3%81%AE%E7%A5%9E | 邪教の神 - Google Search
https://book.douban.com/subject/5602441/ | 請勿在此丟棄屍體 (豆瓣)
https://www.douban.com/note/831114616/?_i=8488680KLQjbnS | 【小说翻译】套娃之夜
https://www.99csw.com/article/4835.htm | Spring Has Come_梓崎优_在线阅读_九九藏书网
https://book.douban.com/subject/26653983/ | 乌鸦社 (豆瓣)
https://book.douban.com/subject/5395151/ | 副本 (豆瓣)
https://book.douban.com/subject/11597370/ | 陶偶 (豆瓣)
https://book.douban.com/subject/26900294/ | 红星蓝调 (豆瓣)
https://book.douban.com/subject/34464618/ | 圣天秤星 (豆瓣)
https://book.douban.com/subject/27001144/ | 十三层空间 (豆瓣)
https://www.google.com/search?q=And%20Then%20There%20Were%20(N-One) | And Then There Were (N-One) - Google Search
https://www.google.com/search?q=%E8%8F%9C%E8%8A%B1%E8%9B%87%E7%9A%84%E6%9C%AB%E6%97%A5 | 菜花蛇的末日 - Google Search
https://www.google.com/search?q=%E6%94%AF%E7%A6%BB%E7%A0%B4%E7%A2%8E%E7%9A%84%E8%94%B7%E8%96%87 | 支离破碎的蔷薇 - Google Search
https://www.google.com/search?q=%20%E6%96%A9%E9%A6%96T%E5%AD%97%E4%B9%8B%E8%B0%9C | 斩首T字之谜 - Google Search
https://www.google.com/search?q=%E7%9C%9F%E5%AE%9F%E3%82%92%E8%A6%86%E3%81%84%E9%9A%A0%E3%81%99%E9%9B%A8 | 真実を覆い隠す雨 - Google Search
https://www.google.com/search?q=%E6%B6%88%E5%A4%B1%E7%9A%84%E7%A0%82%E8%9B%BE%E5%AE%B6 | 消失的砂蛾家 - Google Search
https://www.google.com/search?q=%E5%9B%AD%E7%94%B0%E4%BF%AE%E4%B8%80%E9%83%8E%E3%80%8A%20%E4%BD%9C%E8%80%85%E3%82%88%E6%AC%BA%E3%81%8B%E3%82%8B%E3%82%8B%E3%81%AA%E3%81%8B%E3%82%8C%20%E3%80%8B | 园田修一郎《 作者よ欺かるるなかれ 》 - Google Search
https://www.douban.com/note/728081430/?_i=8490432KLQjbnS | 【渣译】我的神秘宫殿by泽村伊智
https://www.google.com/search?q=%E4%BA%95%E4%B8%8A%E7%9C%9F%E4%BC%AA%20%E3%80%8A%E5%9B%9A%E4%BA%BA%E9%A6%86%E7%9A%84%E6%83%A8%E5%89%A7%E3%80%8B | 井上真伪 《囚人馆的惨剧》 - Google Search
https://www.google.com/search?q=%E3%82%AE%E3%82%AC%E3%81%8F%E3%82%89%E3%82%8A%E3%81%AE%E6%AE%BA%E4%BA%BA | ギガくらりの殺人 - Google Search
https://www.google.com/search?q=%0A%E7%99%BD%E4%BA%95%E6%99%BA%E4%B9%8B%20%E3%80%8A%E9%9D%92%E5%B1%81%E8%82%A1%E7%9A%84%E5%B0%B8%E4%BD%93%E3%80%8B | 白井智之 《青屁股的尸体》 - Google Search
https://www.google.com/search?q=%E6%9C%89%E6%A0%96%E5%B7%9D%E6%9C%89%E6%A0%96+%E6%AF%94%E6%B5%B7%E6%9B%B4%E6%B7%B1%E7%9A%84%E6%B2%B3%E5%B7%9D&oq=%E6%9C%89%E6%A0%96%E5%B7%9D%E6%9C%89%E6%A0%96+%E6%AF%94%E6%B5%B7%E6%9B%B4%E6%B7%B1%E7%9A%84%E6%B2%B3%E5%B7%9D&aqs=chrome..69i57j33i160l4.2154j0j1&sourceid=chrome&ie=UTF-8 | 有栖川有栖 比海更深的河川 - Google Search
https://www.google.com/search?q=%0A%E3%80%8A%E8%99%BD%E6%96%AD%E5%A4%B4%E8%80%8C%E4%B8%8D%E6%AD%BB%E7%9A%84%E6%88%91%E4%BB%AC%E7%9A%84%E6%97%A0%E5%A4%B4%E6%9D%80%E4%BA%BA%E4%BA%8B%E4%BB%B6%E3%80%8B | 《虽断头而不死的我们的无头杀人事件》 - Google Search
https://book.douban.com/subject/26809566/ | 相食 (豆瓣)
https://www.google.com/search?q=%E3%82%A6%E3%82%A3%E3%83%B3%E3%83%81%E3%82%A7%E3%82%B9%E3%82%BF%E3%83%BC%E3%83%BB%E3%83%9E%E3%83%BC%E3%83%80%E3%83%BC%E3%83%BB%E3%83%9F%E3%82%B9%E3%83%86%E3%83%AA%E3%83%BC%E3%83%BB%E3%83%8F%E3%82%A6%E3%82%B9%E3%81%AE%E6%AE%BA%E4%BA%BA+%E6%96%9C%E7%BA%BF%E5%A0%82%E6%9C%89%E7%BA%AA&newwindow=1&sxsrf=ALiCzsY22DCy0gy8dhM0p5w9_C_IdngQ8A%3A1668490859025&ei=ayZzY_WSAfXQ5NoP9POHkAc&ved=0ahUKEwi19NnnvK_7AhV1KFkFHfT5AXIQ4dUDCBA&uact=5&oq=%E3%82%A6%E3%82%A3%E3%83%B3%E3%83%81%E3%82%A7%E3%82%B9%E3%82%BF%E3%83%BC%E3%83%BB%E3%83%9E%E3%83%BC%E3%83%80%E3%83%BC%E3%83%BB%E3%83%9F%E3%82%B9%E3%83%86%E3%83%AA%E3%83%BC%E3%83%BB%E3%83%8F%E3%82%A6%E3%82%B9%E3%81%AE%E6%AE%BA%E4%BA%BA+%E6%96%9C%E7%BA%BF%E5%A0%82%E6%9C%89%E7%BA%AA&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIHCAAQHhCiBDIFCAAQogQyBQgAEKIEMgcIABAeEKIESgQIQRgBSgQIRhgAUL0QWMsbYOMiaANwAHgAgAGNBIgB4gSSAQUxLjUtMZgBAKABAcABAQ&sclient=gws-wiz-serp | ウィンチェスター・マーダー・ミステリー・ハウスの殺人 斜线堂有纪 - Google Search
https://www.google.com/search?q=%0A%E3%80%8AQJKJQ%E3%80%8B%E4%BD%90%E8%97%A4%E7%A9%B6 | 《QJKJQ》佐藤究 - Google Search
https://www.google.com/search?q=%E5%A4%A7%E5%B1%B1%E8%AA%A0%E4%B8%80%E9%83%8E+%E4%B8%8D%E9%81%8B%E3%81%AA%E7%8A%AF%E4%BA%BA&newwindow=1&sxsrf=ALiCzsbOKvKLVmUhT1HXupK1bsImheH9cQ%3A1668490953653&ei=ySZzY_qlJ9jn5NoPt5iasA4&ved=0ahUKEwi6rumUva_7AhXYM1kFHTeMBuYQ4dUDCBA&uact=5&oq=%E5%A4%A7%E5%B1%B1%E8%AA%A0%E4%B8%80%E9%83%8E+%E4%B8%8D%E9%81%8B%E3%81%AA%E7%8A%AF%E4%BA%BA&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIECCMQJ0oECEEYAUoECEYYAFCeBFi7DWDtEWgCcAB4AIABVogB6QGSAQEzmAEAoAEBwAEB&sclient=gws-wiz-serp | 大山誠一郎 不運な犯人 - Google Search
https://www.douban.com/note/699540049/?_i=8490981KLQjbnS | 秋露宫超豪华别墅血洗事件
https://www.99csw.com/book/10418/index.htm | 酒徒_刘以鬯_在线阅读_九九藏书网
https://www.google.com/search?q=%0A%E3%80%8A%E3%82%B4%E3%83%BC%E3%82%B9%E3%83%88%E2%89%A0%E3%83%8E%E3%82%A4%E3%82%BA%E3%80%8B | 《ゴースト≠ノイズ》 - Google Search
https://www.google.com/search?q=%0A%E3%80%8A%E6%9C%80%E5%90%8E%E4%B9%9F%E6%98%AF%E6%9C%80%E5%88%9D%E7%9A%84%E5%81%B6%E5%83%8F%E3%80%8B%0A | 《最后也是最初的偶像》 - Google Search
https://www.google.com/search?q=%0A%E3%80%8A%E7%8E%BB%E7%92%83%E4%B9%8B%E5%A1%94%E6%9D%80%E4%BA%BA%E4%BA%8B%E4%BB%B6%E3%80%8B | 《玻璃之塔杀人事件》 - Google Search
https://www.google.com/search?q=%0A%E3%80%8A%E5%A4%9A%E7%B1%B3%E8%AF%BA%E5%B0%91%E5%A5%B3%E3%80%8B | 《多米诺少女》 - Google Search

https://book.douban.com/subject/35585182/ | 不死鳥 (豆瓣)
https://book.douban.com/subject/35515868/ | 塔納托斯的夢境 (豆瓣)
https://book.douban.com/subject/27035205/ | 梅杜莎,看鏡子 (豆瓣)
https://movie.douban.com/subject/26252157/ | 龙三和他的七人党 (豆瓣)
https://book.douban.com/subject/35606758/ | 动物城2333 (豆瓣)
https://book.douban.com/subject/26394917/ | 斜眼少年 (豆瓣)
https://book.douban.com/subject/35544979/ | 侦探往事 (豆瓣)
https://book.douban.com/subject/10344079// | 嫌疑 (豆瓣)
https://book.douban.com/subject/27104036/ | 莫比乌斯的圈套 (豆瓣)
https://www.99csw.com/book/7137/index.htm | 尸体长发之谜·杀人方程式2_绫辻行人_在线阅读_九九藏书网
https://book.douban.com/subject/27165658/ | 三百年の謎匣 (豆瓣)
https://book.douban.com/subject/26728812/ | 涙香迷宮 (豆瓣)
https://book.douban.com/subject/32567077/ | 暗黑女子 (豆瓣)
https://www.52shuku.vip/tuili/hsv.html | 盲人与狗_水天一色【完结】在线阅读_52书库
https://www.sto.cx/book-27321-1.html | 水天一色《校园惨剧》_全文在線閱讀_思兔
https://book.douban.com/subject/25805272/ | 密室推理杰作选——日本卷2 (豆瓣)
https://book.douban.com/subject/4282164/ | 叫びと祈り (豆瓣)
https://book.douban.com/subject/26277997/ | 岛和我们 (豆瓣)
https://book.douban.com/subject/5348099/ | 奇职怪业俱乐部 (豆瓣)
https://book.douban.com/subject/35563806/comments/?start=20&limit=20&status=P&sort=new_score | 隨機死亡 短评
https://book.douban.com/subject/30458315/ | 本店招牌菜 (豆瓣)
https://www.google.com/search?q=%E4%BA%BA%E6%B2%B9%E8%9C%A1%E7%83%9B%20%E5%B0%8F%E9%85%92%E4%BA%95%E4%B8%8D%E6%9C%A8 | 人油蜡烛 小酒井不木 - Google Search
https://www.99csw.com/book/7743/index.htm | 恋爱曲线_小酒井不木_在线阅读_九九藏书网
https://www.99csw.com/book/8043/index.htm | 迷失的雪夜_千山_在线阅读_九九藏书网

https://github.com/dvlab-research/LBGAT | dvlab-research/LBGAT: Learnable Boundary Guided Adversarial Training (ICCV2021)
https://zhuanlan.zhihu.com/p/558286175 | 打开模型Zero-Shot新范式：Instruction Tuning - 知乎
https://zhuanlan.zhihu.com/p/408166011 | Instruction Tuning｜谷歌Quoc V.Le团队提出又一精调范式 - 知乎
https://zhuanlan.zhihu.com/p/422713214 | Fine-tune的替代品？清华P-Tuning v2大幅提升小模型性能，NER也可promp tuning了！ - 知乎
https://arxiv.org/pdf/2010.15980.pdf | AutoPrompt Eliciting Knowledge from Language Models with Automatically Generated Prompts - Arxiv-2010.15980
https://arxiv.org/pdf/2206.14858.pdf | Solving Quantitative Reasoning Problems with Language Models - Arxiv-2206.14858
https://zhuanlan.zhihu.com/p/440169921 | 一文轻松入门Prompt(附代码) - 知乎
https://github.com/THUDM/P-tuning-v2 | THUDM/P-tuning-v2: An optimized deep prompt tuning strategy comparable to fine-tuning across scales and tasks
https://github.com/thunlp/OpenPrompt | thunlp/OpenPrompt: An Open-Source Framework for Prompt-Learning.
https://arxiv.org/pdf/2103.08493.pdf | How Many Data Points is a Prompt Worth? - Arxiv-2103.08493
https://huggingface.co/blog/how_many_data_points/ | How many data points is a prompt worth ?
https://zhuanlan.zhihu.com/p/419215591 | NLP的“第四范式”之prompt learning总结 - 知乎
https://zhuanlan.zhihu.com/p/395115779 | 近代自然语言处理技术发展的“第四范式” - 知乎
https://arxiv.org/pdf/2210.01848.pdf | Explaining Patterns in Data with Language Models via Interpretable Autoprompting - Arxiv-2210.01848
https://github.com/csinva/iprompt | csinva/iprompt: Finding semantically meaningful and accurate prompts
https://github.com/csinva/imodelsX | csinva/imodelsX: Library to explain *a dataset* using natural language and neural networks.
https://github.com/ucinlp/autoprompt | ucinlp/autoprompt: AutoPrompt: Automatic Prompt Construction for Masked Language Models.

https://aclanthology.org/2022.findings-acl.318.pdf | GCPG A General Framework for Controllable Paraphrase Generation - ACL-ACL| Findings-2022_2022.findings-acl.318
https://aclanthology.org/2022.findings-naacl.160.pdf | Learning Structural Information for Syntax-Controlled Paraphrase Generation - ACL-Findings| NAACL-2022_2022.findings-naacl.160
https://arxiv.org/pdf/1903.08855.pdf | Linguistic Knowledge and Transferability of Contextual Representations - Arxiv-1903.08855
https://arxiv.org/pdf/2010.01737.pdf | Transformer-Based Neural Text Generation with Syntactic Guidance - Arxiv-2010.01737
https://arxiv.org/pdf/2108.00104.pdf | Structural Guidance for Transformer Language Models - Arxiv-2108.00104
https://blog.evjang.com/2018/01/nf1.html | Eric Jang: Normalizing Flows Tutorial, Part 1: Distributions and Determinants
https://arxiv.org/pdf/2205.14217.pdf | Diffusion-LM Improves Controllable Text Generation - Arxiv-2205.14217
https://github.com/XiangLi1999/Diffusion-LM | XiangLi1999/Diffusion-LM: Diffusion-LM
https://arxiv.org/abs/2209.11799 | Emb-GAM an Interpretable and Efficient Predictor using Pre-trained Language Models - Arxiv-2209.11799

https://stats.stackexchange.com/questions/390437/variance-of-reparameterization-trick-and-score-function | gradient descent - Variance of reparameterization trick and score function - Cross Validated
https://arxiv.org/pdf/1802.05098.pdf | DiCE The Infinitely Differentiable Monte-Carlo Estimator - Arxiv-1802.05098
https://github.com/alexis-jacq/LOLA_DiCE | alexis-jacq/LOLA_DiCE: Pytorch implementation of LOLA (https://arxiv.org/abs/1709.04326) using DiCE (https://arxiv.org/abs/1802.05098)
https://paperswithcode.com/paper/dice-the-infinitely-differentiable-monte | DiCE: The Infinitely Differentiable Monte-Carlo Estimator | Papers With Code

https://github.com/Verified-Intelligence/auto_LiRPA/blob/master/README.md | auto_LiRPA/README.md at master · Verified-Intelligence/auto_LiRPA
https://arxiv.org/pdf/2002.12920.pdf | Automatic Perturbation Analysis for Scalable Certified Robustness and Beyond - Arxiv-2002.12920
https://aclanthology.org/2022.naacl-main.321.pdf | Informativeness and Invariance Two Perspectives on Spurious Correlations in Natural Language - ACL-NAACL-2022_2022.naacl-main.321
https://arxiv.org/pdf/2104.08735.pdf | Learning with Instance Bundles for Reading Comprehension - Arxiv-2104.08735
https://aclanthology.org/2022.naacl-main.238.pdf | Global Entity Disambiguation with BERT - ACL-NAACL-2022_2022.naacl-main.238
https://arxiv.org/pdf/2109.07022.pdf | How Does Counterfactually Augmented Data Impact Models for Social Computing Constructs? - Arxiv-2109.07022
https://arxiv.org/pdf/2203.12942.pdf | Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets - ACL-ACL-2022_2022.acl-long.190
https://arxiv.org/pdf/2010.01057.pdf | LUKE Deep Contextualized Entity Representations with Entity-aware Self-attention - Arxiv-2010.01057
https://arxiv.org/pdf/2204.13902.pdf | Fast Sampling of Diffusion Models with Exponential Integrator - Arxiv-2204.13902

https://www.google.com/search?q=McNema&sourceid=chrome&ie=UTF-8 | McNema - Google Search
https://www.wikiwand.com/en/McNemar%27s_test | McNemar's test - Wikiwand
https://www.google.com/search?q=mcnemar%27s+test+vs+chi+square&newwindow=1&ei=dnpVY6S_EKim5NoPovyOuAE&oq=McNemar%27s+test+vs&gs_lcp=Cgdnd3Mtd2l6EAEYADIFCAAQgAQyBQgAEIAEMgUIABCABDIGCAAQFhAeMgYIABAWEB4yBggAEBYQHjIGCAAQFhAeMgYIABAWEB4yBQgAEIYDMgUIABCGAzoECAAQRzoECAAQQ0oECE0YAUoECEEYAEoECEYYAFDiBVjyCWDRFWgAcAJ4AIABhwGIAd8CkgEDMC4zmAEAoAEByAEIwAEB&sclient=gws-wiz | mcnemar's test vs chi square - Google Search
https://stats.stackexchange.com/questions/76875/what-is-the-difference-between-mcnemars-test-and-the-chi-squared-test-and-how | r - What is the difference between McNemar's test and the chi-squared test, and how do you know when to use each? - Cross Validated

https://www.wikiwand.com/en/Dirichlet_distribution | Dirichlet distribution - Wikiwand
https://www.cs.cmu.edu/~epxing/Class/10701-08s/recitation/dirichlet.pdf | Dirichlet Distribution, Dirichlet Process and Dirichlet Process Mixture

https://book.douban.com/subject/26436007/ | 意识形态的终结 (豆瓣)
https://www.douban.com/search?q=%E6%B0%91%E4%B8%BB%E7%9A%84%E6%A8%A1%E5%BC%8F | 搜索: 民主的模式
https://book.douban.com/review/12918946/ | “拿起笔，我是自己的神，放下笔，我仍是尘埃，是野草，是炮灰”（仙症）书评
https://book.douban.com/review/12762218/ | “东北文艺复兴三杰”补完计划（生吞）书评
https://reproducedpapers.org/ | Reproduced Papers
https://www.google.com/search?q=%E5%96%84%E7%9A%84%E7%A0%94%E7%A9%B6&oq=%E5%96%84%E7%9A%84%E7%A0%94%E7%A9%B6&aqs=chrome.0.0i355i512j46i512j0i30l2j0i15i30j0i30l2j0i15i30.4192j0j1&sourceid=chrome&ie=UTF-8 | 善的研究 - Google Search

https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb#scrollTo=Ylc3AIdZkFhl | Stable Diffusion with 🧨 diffusers - Colaboratory
https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ | What are Diffusion Models? | Lil'Log
https://yang-song.net/blog/2021/score/ | Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song

https://book.douban.com/subject/35687505/ | 六藏图 (豆瓣)
https://book.douban.com/subject/35501248/ | 战国·白云谣 (豆瓣)

https://arxiv.org/pdf/2204.06340.pdf | Distributionally Robust Models with Parametric Likelihood Ratios - Arxiv-2204.06340
https://arxiv.org/pdf/2104.13478.pdf | Geometric Deep Learning Grids, Groups, Graphs, Geodesics, and Gauges - Arxiv-2104.13478

https://www.bilibili.com/video/BV1ds411B7fR/?p=5&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | 垒球社“暖暖”_哔哩哔哩_bilibili
https://github.com/THUYimingLi/awesome_lists/blob/main/advice.md | awesome_lists/advice.md at main · THUYimingLi/awesome_lists
https://medium.com/@marcotcr/coming-up-with-research-ideas-3032682e5852 | Coming up with research ideas. “What project should I do next?” is a… | by Marco Tulio Ribeiro | Medium
https://github.com/THUYimingLi/machine_unlearning | THUYimingLi/machine_unlearning: Existing Literature about Machine Unlearning
https://arxiv.org/abs/2205.12331 | Certified Robustness Against Natural Language Attacks by Causal Intervention - Arxiv-2205.12331

https://phillipi.github.io/6.s898/materials/notes/05_cnns.pdf | 05_cnns.pdf
https://geometricdeeplearning.com/blogs/ | GDL Blogs
https://geometricdeeplearning.com/lectures/ | GDL Course

https://movie.douban.com/subject/34915462/?from=subject-page | SSSS.电光机王 (豆瓣)
https://www.olehdtv.com/index.php/vod/play/id/25917/sid/1/nid/2.html | SSSS.电光机王_第02集 - 欧乐影院－面向海外华人的在线视频媒体平台,海量高清视频在线观看
https://movie.douban.com/subject/26935251/ | 春宵苦短，少女前进吧！ (豆瓣)

https://www.maofly.com/manga/24804.html | 来自深渊的阿杜_来自深渊的阿杜漫画在线阅读-漫画猫
https://www.manhuagui.com/comic/6808/ | JK女子攻兵漫画_女子攻兵漫画_松本次郎 - 看漫画
https://www.manhuagui.com/comic/35833/ | area51漫画_久正人 - 看漫画
https://www.manhuagui.com/comic/17254/ | 惩役339年漫画_懲役339年漫画_伊势ともか - 看漫画
https://www.zhihu.com/search?q=%E5%AD%A4%E9%AB%98%E4%B9%8B%E4%BA%BA&type=content | 孤高之人 - 搜索结果 - 知乎
https://www.zhihu.com/question/25274459 | 如何评价《晚安布布》？ - 知乎

https://www.zhihu.com/question/453567336/answer/1825995877 | 笔给你，你会怎么写《进击的巨人》结局？ - 知乎

https://www.google.com/search?q=%E5%B0%BC%E9%87%87%E5%B1%B1%E4%B8%8B%E7%9A%84%E6%A0%91&oq=%E5%B0%BC%E9%87%87%E5%B1%B1%E4%B8%8B%E7%9A%84%E6%A0%91&aqs=chrome..69i57j0i546l2.5274j0j1&sourceid=chrome&ie=UTF-8 | 尼采山下的树 - Google Search
https://www.google.com/search?q=%E6%9F%A5%E6%8B%89%E5%9B%BE%E6%96%AF%E7%89%B9%E6%8B%89%E5%A6%82%E6%98%AF%E8%AF%B4&oq=%E6%9F%A5%E6%8B%89%E5%9B%BE%E6%96%AF%E7%89%B9%E6%8B%89%E5%A6%82%E6%98%AF%E8%AF%B4&aqs=chrome..69i57.1404j0j1&sourceid=chrome&ie=UTF-8 | 查拉图斯特拉如是说 - Google Search
https://www.99csw.com/book/2744/83221.htm | 查拉斯图拉如是说 : 第一部 山上的树_尼采_在线阅读_九九藏书网

https://www.bilibili.com/video/BV194411R7UF?spm_id_from=333.999.0.0&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | 【游戏通鉴Vol.10】什么能改变一个人的本质？CRPG名作《异域镇魂曲》通览_哔哩哔哩_bilibili
https://www.youtube.com/watch?v=NuTf6SZul20&list=PLzw_r3FRBpcOPmDhUkdf9YRE9PYpzaWz0&index=4 | (313) Let's Play Planescape Torment - 02 Mortuary Level 2 - YouTube

https://book.douban.com/series/11463 | 卡尔维诺经典
https://book.douban.com/subject/10555509/discussion/637203803/ | 这本书算是散文吗？肯定不是小说
https://book.douban.com/subject/10555550/ | 为什么读经典 (豆瓣)
https://www.douban.com/search?q=%E7%BE%8E%E5%9B%BD%E8%AE%B2%E7%A8%BF | 搜索: 美国讲稿
https://book.douban.com/subject/10555538/ | 美国讲稿 (豆瓣)
https://book.douban.com/review/9648748/ | 《美国讲稿》提到的所有人名与书名（美国讲稿）书评
https://zhuanlan.zhihu.com/p/159241206 | 美国讲稿笔记-序言 - 知乎
https://www.wikiwand.com/en/Charles_Eliot_Norton_Lectures | Charles Eliot Norton Lectures - Wikiwand
https://www.zhihu.com/search?q=%E6%9C%AA%E6%9D%A5%E5%8D%83%E5%B9%B4%E6%96%87%E5%AD%A6%E5%A4%87%E5%BF%98%E5%BD%95&type=content | 未来千年文学备忘录 - 搜索结果 - 知乎
http://www.ruanyifeng.com/calvino/nonfiction/cat-76/ | 未来千年文学备忘录（诺顿讲稿） - 卡尔维诺中文站

https://aclanthology.org/2021.acl-long.346.pdf | Evaluation Examples are not Equally Informative How should that change NLP Leaderboards? - ACL-ACL| IJCNLP-2021_2021.acl-long.346
https://www.reddit.com/r/MachineLearning/comments/vbwe8k/d_yet_another_case_of_plagiarism_in_iccv_the_iccv/ | [D] Yet another case of plagiarism in ICCV. The ICCV 2021 paper "Learnable Boundary Guided Adversarial Training"(arxiv 2011.11164) with the BMVC 2020 paper "Adversarial Concurrent Training: Optimizing Robustness and Accuracy Trade-off of Deep Neural Networks" (arxiv 2008.07015) : MachineLearning
https://arxiv.org/abs/2008.07015 | Adversarial Concurrent Training Optimizing Robustness and Accuracy Trade-off of Deep Neural Networks - Arxiv-2008.07015
https://medium.com/@fahad.sarfraz/plagiarism-by-iccv-2021-paper-learnable-boundary-guided-adversarial-training-404d2ff5ed4e | Plagiarism by ICCV 2021 paper “Learnable Boundary Guided Adversarial Training”? | by Fahad Sarfraz | Jun, 2022 | Medium
https://arxiv.org/pdf/2008.07015.pdf | Adversarial Concurrent Training Optimizing Robustness and Accuracy Trade-off of Deep Neural Networks - Arxiv-2008.07015
https://arxiv.org/pdf/2011.11164.pdf | Learnable Boundary Guided Adversarial Training - Arxiv-2011.11164
https://aclanthology.org/2021.naacl-main.335.pdf | TaxoClass: Hierarchical Multi-Label Text Classification Using Only Class Names
https://github.com/benedekrozemberczki/pytorch_geometric_temporal | benedekrozemberczki/pytorch_geometric_temporal: PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models (CIKM 2021)
https://arxiv.org/pdf/2009.08205.pdf | Generating Label Cohesive and Well-Formed Adversarial Claims - Arxiv-2009.08205
https://arxiv.org/pdf/2106.00872.pdf | On the Efficacy of Adversarial Data Collection for Question Answering Results from a Large-Scale Randomized Study - Arxiv-2106.00872
https://arxiv.org/pdf/2110.08514.pdf | Analyzing Dynamic Adversarial Training Data in the Limit - Arxiv-2110.08514
https://www.google.com/search?q=generalized+inner+loop+meta-learning&oq=generalized+inner+loop+&aqs=chrome.0.0i512j69i57.2930j0j1&sourceid=chrome&ie=UTF-8 | generalized inner loop meta-learning - Google Search

https://arxiv.org/pdf/1707.06347.pdf | Proximal Policy Optimization Algorithms - Arxiv-1707.06347
https://huggingface.co/blog/constrained-beam-search | Guiding Text Generation with Constrained Beam Search in 🤗 Transformers

https://arxiv.org/pdf/1905.05301.pdf | Hierarchically Structured Meta-learning - Arxiv-1905.05301
https://book.douban.com/subject/35192665/ | 平原上的摩西 (豆瓣)
https://book.douban.com/subject_search?search_text=%E5%8F%8C%E9%9B%AA%E6%B6%9B | 双雪涛 - 读书 - 豆瓣搜索
https://book.douban.com/subject/26881768/ | 我的朋友安德烈 (豆瓣)

https://philosophy.stackexchange.com/questions/90518/how-is-pure-intuition-possible-according-to-kant | philosophy of mathematics - How is 'Pure Intuition' possible according to Kant? - Philosophy Stack Exchange
https://www.reddit.com/r/askphilosophy/comments/5g95ue/kants_prolegomena_to_any_future_metaphysics_that/ | Kant's Prolegomena to Any Future Metaphysics That Will Be Able to Present Itself as a Science, ch. "Main transcendental problem 2: How is pure natural science possible?", Note III - What is he trying to say? : askphilosophy
https://www.wikiwand.com/en/Prolegomena_to_Any_Future_Metaphysics | Prolegomena to Any Future Metaphysics - Wikiwand
http://people.wku.edu/jan.garrett/303/kantprop.htm | Notes on Kant's Prolegomena
https://hume.ucdavis.edu/phi023/kantLEC.HTM | Lectures on Immanuel Kant

https://movie.douban.com/subject/1301021/ | 盲兽 (豆瓣)
https://www.zhihu.com/question/269926545 | 叔本华在《作为意志和表象的世界》一书中经常谈到的充足理由律是什么? - 知乎
https://book.douban.com/subject/35703665/ | 作为意欲和表象的世界（第2卷） (豆瓣)
https://www.reddit.com/r/LearnJapanese/comments/98dxbv/what_does_this_subreddit_think_of_bunprojp/ | What does this subreddit think of bunpro.jp? : LearnJapanese

https://github.com/allegro/allRank | allegro/allRank: allRank is a framework for training learning-to-rank neural models based on PyTorch.
https://zhuanlan.zhihu.com/p/111636490 | Learning to Rank： pointwise 、 pairwise 、 listwise - 知乎
https://zhuanlan.zhihu.com/p/214242589 | SIGIR20|最佳论文：通往公平、公正的Learning to Rank！ - 知乎
https://www.zhihu.com/question/389068269 | (24 封私信 / 81 条消息) Learning To Rank的pair wise方法如何得到全局排序结果呢？ - 知乎
https://zhuanlan.zhihu.com/p/64952093 | Learning to Rank读书笔记--排序评价指标 - 知乎
https://zhuanlan.zhihu.com/p/149310341 | 【萝卜日记第32期】十年后仍是虚构作品—机动警察剧场版&真人版 - 知乎
https://zhuanlan.zhihu.com/p/149309480 | 【萝卜日记第31期】所到之处，寸草不生——机动警察PATLABOR - 知乎
https://www.zhihu.com/question/305055900/answer/1177727168 | (24 封私信 / 81 条消息) 如何评价2020年4月23日Netflix原创动画《攻壳机动队SAC_2045》？ - 知乎

https://zhuanlan.zhihu.com/p/172121380 | 深入浅出Yolo系列之Yolov5核心基础知识完整讲解 - 知乎
https://zhuanlan.zhihu.com/p/143747206 | 深入浅出Yolo系列之Yolov3&Yolov4&Yolov5&Yolox核心基础知识完整讲解 - 知乎
https://docs.ultralytics.com/ | YOLOv5 Documentation
https://github.com/ultralytics/yolov5#pretrained-checkpoints | ultralytics/yolov5: YOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLite
https://zhuanlan.zhihu.com/p/94986199 | 写给小白的YOLO介绍 - 知乎
https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab | Understanding SSD MultiBox — Real-Time Object Detection In Deep Learning | by Eddie Forson | Towards Data Science
https://developers.arcgis.com/python/guide/how-ssd-works/ | How single-shot detector (SSD) works? | ArcGIS Developer
https://www.zhihu.com/search?q=ssd%20algorithm&type=content | ssd algorithm - 搜索结果 - 知乎
https://zhuanlan.zhihu.com/p/33544892 | 目标检测|SSD原理与实现 - 知乎
https://arxiv.org/abs/1708.02002 | Focal Loss for Dense Object Detection - Arxiv-1708.02002
https://link.zhihu.com/?target=https%3A//www.cnblogs.com/xuanyuyt/p/7222867.html | 知乎 - 安全中心
https://www.zhihu.com/search?q=ssd%20%E6%A3%80%E6%B5%8B&type=content | ssd 检测 - 搜索结果 - 知乎
https://gullayeshwantkumarruler.medium.com/single-shot-detector-ssd-a299f437f6ef | Single Shot Detector (SSD). What is SSD? | by Yeshwant Kumar | Medium
https://blog.actorsfit.com/a?ID=00001-38d70c78-3466-4c1b-bbff-26805f5a4ea4 | Detailed SSD object detection algorithm - actorsfit
https://zhuanlan.zhihu.com/p/183261974 | 你一定从未看过如此通俗易懂的YOLO系列(从v1到v5)模型解读 (上) - 知乎
https://movie.douban.com/subject/3041806/ | 叶问 (豆瓣)

https://www.google.com/search?q=%E4%B8%9C%E9%82%AA%E8%A5%BF%E6%AF%92&sourceid=chrome&ie=UTF-8 | 东邪西毒 - Google Search

https://www.google.com/search?q=%E5%91%BC%E5%90%B8+%E7%89%B9%E5%BE%B7%E5%A7%9C&oq=%E5%91%BC%E5%90%B8te+de+jiang+%7C&aqs=chrome.1.69i57j0i333.8110j0j1&sourceid=chrome&ie=UTF-8 | 呼吸 特德姜 - Google Search
https://book.douban.com/review/12163001/ | 《呼吸》各篇短评（呼吸）书评
https://www.google.com/search?q=learn2learn+kronecker&oq=learn2learn+kronecker&aqs=chrome..69i57j69i60.4601j0j1&sourceid=chrome&ie=UTF-8 | learn2learn kronecker - Google Search
https://joeddav.github.io/blog/2020/05/29/ZSL.html | Zero-Shot Learning in Modern NLP | Joe Davison Blog
https://www.google.com/search?q=%E7%AA%84%E9%97%A8&oq=%E7%AA%84%E9%97%A8&aqs=chrome..69i57j69i59l2.2012j0j1&sourceid=chrome&ie=UTF-8 | 窄门 - Google Search

https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/5/ | 标签 生成模型 下的文章 - 科学空间|Scientific Spaces
https://kexue.fm/archives/5776/comment-page-3#comments | 细水长flow之NICE：流模型的基本概念与实现 - 科学空间|Scientific Spaces
http://www.cs.toronto.edu/~duvenaud/courses/csc2541/ | index

https://www.99csw.com/book/10305/371569.htm | 呼吸 : 焦虑是自由引起的眩晕_特德·姜_在线阅读_九九藏书网
https://book.douban.com/review/13890377/ | 分节感想，严重剧透（呼吸）书评
https://book.douban.com/review/12245641/ | 焦虑是自由引起的眩晕（呼吸）书评
https://www.zhihu.com/people/ai-yu-16-30/posts | Liewschild - 知乎
https://zhuanlan.zhihu.com/p/263554045 | StyleGAN 和 StyleGAN2 的深度理解 - 知乎
https://www.zhihu.com/people/kong-gu-91/posts | 圆圆要学习 - 知乎

https://kexue.fm/archives/6549 | 从DCGAN到SELF-MOD：GAN的模型架构发展一览 - 科学空间|Scientific Spaces
https://kexue.fm/archives/8757 | WGAN新方案：通过梯度归一化来实现L约束 - 科学空间|Scientific Spaces
https://kexue.fm/archives/8244 | WGAN的成功，可能跟Wasserstein距离没啥关系 - 科学空间|Scientific Spaces
https://kexue.fm/archives/7466 | 泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练 - 科学空间|Scientific Spaces
https://kexue.fm/archives/7234 | 对抗训练浅谈：意义、方法和思考（附Keras实现） - 科学空间|Scientific Spaces
https://kexue.fm/archives/7105 | 级联抑制：提升GAN表现的一种简单有效的方法 - 科学空间|Scientific Spaces
https://kexue.fm/archives/6139 | WGAN-div：一个默默无闻的WGAN填坑者 - 科学空间|Scientific Spaces
https://kexue.fm/archives/7210 | Designing GANs：又一个GAN生产车间 - 科学空间|Scientific Spaces
https://kexue.fm/archives/4439 | 互怼的艺术：从零直达WGAN-GP - 科学空间|Scientific Spaces
https://kexue.fm/archives/6016 | f-GAN简介：GAN模型的生产车间 - 科学空间|Scientific Spaces
https://kexue.fm/archives/6214 | BiGAN-QP：简单清晰的编码&生成模型 - 科学空间|Scientific Spaces
https://kexue.fm/archives/6110 | RSGAN：对抗模型中的“图灵测试”思想 - 科学空间|Scientific Spaces
https://www.zhihu.com/search?q=Snorkel&type=content | Snorkel - 搜索结果 - 知乎

https://www.wikiwand.com/en/Hungarian_algorithm | Hungarian algorithm - Wikiwand
https://www.zhihu.com/people/marisa.moe | 圆角骑士魔理沙 - 知乎
https://github.com/pyg-team/pytorch_geometric/issues/1365 | What is the relationship between DGL and PyG? · Issue #1365 · pyg-team/pytorch_geometric
https://posts.careerengine.us/p/6196f6ecae2a98248ad5e6c0 | 图神经网络框架-PyTorch Geometric（PyG）的使用及踩坑
https://www.spaces.ac.cn/archives/6620 | 函数光滑化杂谈：不可导函数的可导逼近 - 科学空间|Scientific Spaces

https://zhuanlan.zhihu.com/p/239929601 | [Meta-Learning]对Reptile的深度解析 - 知乎
https://github.com/bamos/HowToTrainYourMAMLPytorch | bamos/HowToTrainYourMAMLPytorch: The original code for the paper "How to train your MAML" along with a replication of the original "Model Agnostic Meta Learning" (MAML) paper in Pytorch.
https://github.com/facebookresearch/higher/issues/56 | Meaning of stop-gradient · Issue #56 · facebookresearch/higher
https://github.com/google-research/sam | google-research/sam
http://learn2learn.net/tutorials/anil_tutorial/ANIL_tutorial/ | Feature Reuse with ANIL - learn2learn
http://learn2learn.net/docs/learn2learn.nn/ | learn2learn.nn - learn2learn
https://arxiv.org/pdf/1910.13603.pdf | When MAML Can Adapt Fast and How to Assist When It Cannot | PDF
https://github.com/Sha-Lab/kfo | Sha-Lab/kfo: Code release for "When MAML Can Adapt Fast and How to Assist When It Cannot", AISTATS 2021.
http://seba1511.net/posters/ | Posters
https://www.google.com/search?q=maml+%2B%2B&oq=maml+%2B%2B&aqs=chrome..69i57j69i65j69i61.1197j0j1&sourceid=chrome&ie=UTF-8 | maml ++ - Google Search
https://arxiv.org/abs/1810.09502 | How to train your MAML | Abstract

https://xingyuzhou.org/blog/notes/strong-convexity#mjx-eqn-eqdef | Strong convexity · Xingyu Zhou's blog
https://www.zhihu.com/question/22426561 | 如何理解Bregman divergence？ - 知乎
http://mark.reid.name/blog/meet-the-bregman-divergences.html | Meet the Bregman Divergences ← Inductio Ex Machina ← Mark Reid
https://www2.cs.uic.edu/~zhangx/teaching/bregman.pdf | bregman.pdf
http://users.cecs.anu.edu.au/~xzhang/teaching/bregman.pdf | bregman.pdf

https://arxiv.org/pdf/1410.8516.pdf | NICE: Non-linear Independent Components Estimation | PDF
https://github.com/bojone/flow/blob/master/nice.py | flow/nice.py at master · bojone/flow
https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650745032&idx=1&sn=a889433dd4c4d9f62bfab347909d9d28&chksm=871aecb6b06d65a02625abdf4b21a2116251e311a49508db587b76ae8f76d7a9e03d4a6ab80a&scene=27#wechat_redirect | 下一个GAN？OpenAI提出可逆生成模型Glow
https://github.com/paultsw/nice_pytorch | paultsw/nice_pytorch: Nonlinear Independent Components Estimation (Dinh et al, 2014) in PyTorch.
https://opensourcelibs.com/lib/pytorch-nice | Pytorch Nice - Implementation of non-linear independent components estimation (NICE) in pytorch - (pytorch-nice)
https://spaces.ac.cn/archives/5776/comment-page-1#comments | 细水长flow之NICE：流模型的基本概念与实现 - 科学空间|Scientific Spaces
https://github.com/fmu2/NICE | fmu2/NICE: PyTorch implementation of NICE
https://github.com/DakshIdnani/pytorch-nice | DakshIdnani/pytorch-nice: Implementation of non-linear independent components estimation (NICE) in pytorch
https://kexue.fm/archives/5807 | 细水长flow之RealNVP与Glow：流模型的传承与升华 - 科学空间|Scientific Spaces
https://kexue.fm/tag/flow/ | 标签 flow 下的文章 - 科学空间|Scientific Spaces
https://www.zhihu.com/question/444045409 | 如何评价王国之心系列今天宣布登录EPIC？ - 知乎
https://www.91m.cc/vodplay/15679-3-11.html | 在线播放巴克·亚罗_BACK ARROW第11集-手机高清免费流畅观看-樱花动漫

https://spaces.ac.cn/archives/7180/comment-page-1 | 从几何视角来理解模型参数的初始化策略 - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/8620 | 浅谈Transformer的初始化、参数化与标准化 - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/8069 | 你可能不需要BERT-flow：一个线性变换媲美BERT-flow - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/7681 | L2正则没有想象那么好？可能是“权重尺度偏移”惹的祸 - 科学空间|Scientific Spaces
https://spaces.ac.cn/category/Mathematics/2/ | 分类 数学研究 下的文章 - 科学空间|Scientific Spaces
https://spaces.ac.cn/archives/8444 | 我们可以无损放大一个Transformer模型吗（一） - 科学空间|Scientific Spaces

https://berkeleytime.com/catalog | Berkeleytime
https://edusalsa.com/ | Edusalsa - Discover Your Stanford

https://www.99csw.com/book/1166/33767.htm | 窄门 : 第二章_安德烈·纪德_在线阅读_九九藏书网
https://www.zhihu.com/question/28961306 | 如何评价王小波《绿毛水怪》这部作品? - 知乎
https://www.google.com/search?q=%E5%9C%B0%E4%B9%85%E5%A4%A9%E9%95%BF+%E7%8E%8B%E5%B0%8F%E6%B3%A2&newwindow=1&sxsrf=AOaemvLKO6u5iqifyQu5bh3oX-R_CRls-g%3A1635914834852&ei=UhSCYZa8M9zI0PEPz4mpsAk&oq=%E5%9C%B0%E4%B9%85%E5%A4%A9%E9%95%BF+%E7%8E%8B%E5%B0%8F%E6%B3%A2&gs_lcp=Cgdnd3Mtd2l6EAM6BwgAEEcQsAM6BQgAEIAEOgUILhCABDoHCAAQgAQQDDoFCAAQzQJKBAhBGABQoAlY-BlgtxpoAXACeACAAcUDiAGmFpIBCTAuMi41LjMuMZgBAKABAcgBCMABAQ&sclient=gws-wiz&ved=0ahUKEwiWrfTUsfvzAhVcJDQIHc9ECpYQ4dUDCA4&uact=5 | 地久天长 王小波 - Google Search
https://www.99csw.com/book/1970/index.htm | 地久天长_王小波_在线阅读_九九藏书网

https://lilianweng.github.io/lil-log/2020/01/29/curriculum-for-reinforcement-learning.html | Curriculum for Reinforcement Learning

https://perso.telecom-paristech.fr/tchamker/SI221/2020f/ | Index of /tchamker/SI221/2020f
https://www.manhuacat.com/manga/5565/464372.html | 放学后失眠的你漫画第20话在线阅读-オジロマコト - 漫画猫
https://www.zhihu.com/question/309212728 | 如何评价《我想吃掉你的胰脏》的原作小说以及衍生作品（漫画、真人电影、动画电影）？ - 知乎
https://zhuanlan.zhihu.com/p/56166459 | 【轻吐槽】回应先知，关于《胰脏》中的校园阶级那些事 - 知乎
https://zhuanlan.zhihu.com/p/55581506 | 对《我想吃掉你的胰脏》的纯粹非理性批判 - 知乎

https://spaces.ac.cn/archives/6409/comment-page-1 | O-GAN：简单修改，让GAN的判别器变成一个编码器！ - 科学空间|Scientific Spaces
https://github.com/eriklindernoren/PyTorch-GAN | eriklindernoren/PyTorch-GAN: PyTorch implementations of Generative Adversarial Networks.
https://spaces.ac.cn/archives/6280/comment-page-1#mjx-eqn-eq%3Astrong-dual | 从Wasserstein距离、对偶理论到WGAN - 科学空间|Scientific Spaces
https://vincentherrmann.github.io/blog/wasserstein/ | Wasserstein GAN and the Kantorovich-Rubinstein Duality - Vincent Herrmann
https://spaces.ac.cn/archives/6051 | 深度学习中的Lipschitz约束：泛化与生成模型 - 科学空间|Scientific Spaces

https://www.zhihu.com/column/c_171450570 | 生成对抗网络 - 知乎
https://zhuanlan.zhihu.com/p/34635690 | 生成对抗网络系列(2)——GAN提高 - 知乎
https://github.com/nocotan/pytorch-lightning-gans | nocotan/pytorch-lightning-gans: Collection of PyTorch Lightning implementations of Generative Adversarial Network varieties presented in research papers.

https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#noise-contrastive-estimation-nce | Learning Word Embedding

https://blog.csdn.net/a1015553840/article/details/51043019 | 機器學習基石(Machine Learning Foundations) 机器学习基石 作业二 课后习题解答_Mac Jiang的博客-CSDN博客
https://github.com/Doraemonzzz/ML-Foundation-and-ML-Techniques | Doraemonzzz/ML-Foundation-and-ML-Techniques: 台大机器学习课程作业详解
https://github.com/ppaquay/Learning-from-Data-Solutions/blob/master/Problems_Chap2.pdf | Learning-from-Data-Solutions/Problems_Chap2.pdf at master · ppaquay/Learning-from-Data-Solutions
https://github.com/Doraemonzzz/Learning-from-data/blob/master/Chapter2/Chapter%202%20Training%20versus%20Testing.pdf | Learning-from-data/Chapter 2 Training versus Testing.pdf at master · Doraemonzzz/Learning-from-data
https://github.com/niuers/Learning-From-Data-A-Short-Course/blob/master/Solutions%20to%20Chapter%202%20Training%20versus%20Testing.ipynb | Learning-From-Data-A-Short-Course/Solutions to Chapter 2 Training versus Testing.ipynb at master · niuers/Learning-From-Data-A-Short-Course

https://www.google.com/search?q=%E6%8B%BE%E5%8F%88%E4%B9%8B%E5%9B%BD&oq=%E6%8B%BE%E5%8F%88%E4%B9%8B%E5%9B%BD&aqs=chrome..69i57&sourceid=chrome&ie=UTF-8 | 拾又之国 - Google Search
https://www.google.com/search?q=%E6%97%A5%E6%9C%88%E5%90%8C%E9%94%99&oq=%E6%97%A5%E6%9C%88%E5%90%8C%E9%94%99&aqs=chrome..69i57&sourceid=chrome&ie=UTF-8 | 日月同错 - Google Search
https://www.zhihu.com/search?q=%E9%95%BF%E5%AE%89%E7%9D%A3%E6%AD%A6%E5%8F%B8&type=content | 长安督武司 - 搜索结果 - 知乎

https://www.zhihu.com/search?q=%E5%AD%98%E5%9C%A8%E4%B8%8E%E8%99%9A%E6%97%A0&type=content | 存在与虚无 - 搜索结果 - 知乎
https://book.douban.com/subject/25939476/ | 存在与时间 (豆瓣)

https://www.zxzj.me/video/775-1-4.html | 《火花》第4集在线观看- 在线之家
https://www.kanunu8.com/book3/6631/116245.html | 赡养人类_刘慈欣中短篇科幻作品_刘慈欣 小说在线阅读
https://search.cn-ki.net/search?keyword=%E8%BF%9C%E5%B1%B1%E6%B7%A1%E5%BD%B1&db=CDMD | 远山淡影-iData知识搜索

https://project.hupili.net/tutorial/hu2012-matrix-calculus/hu2012matrix-calculus.pdf | hu2012matrix-calculus.pdf
https://cdn-uploads.piazza.com/paste/kstd6rn7gfj2xa/d141b3471443145c7413a7dd8ddea576fd8c0710a292de74c429364ed60ac129/Vector_and_Matrix_Derivatives-1.pdf | Vector_and_Matrix_Derivatives-1.pdf
https://www.bilibili.com/video/BV1F54y197GW?p=2 | 【今 敏】妄想代理人到底讲了什么 | 个人解析 | 合集_哔哩哔哩_bilibili

https://wzyboy.im/post/1317.html | 使用 Beancount 记录证券投资 | wzyboy’s blog
https://www.zhihu.com/question/265457580 | 如何评价漫画《少女终末旅行》42话？ - 知乎

https://www.zhihu.com/column/c_1387802014982762496 | 觉醒年代中的好文章整理 - 知乎
https://www.doc88.com/p-0062457643161.html?r=1 | 奇文赏析 民主至上——评陈独秀的《爱国心与自觉心》 - 道客巴巴
https://www.zhihu.com/search?q=%E4%B8%AD%E5%9B%BD%E8%BF%91%E4%BB%A3%E5%8F%B2%20%E8%92%8B%E5%BB%B7%E9%BB%BB&type=content | 中国近代史 蒋廷黻 - 搜索结果 - 知乎

https://socket.io/get-started/ | Get started | Socket.IO
https://www.zhihu.com/question/22198714 | 如何解读芥川龙之介的《地狱变》一文？ - 知乎

https://www.reddit.com/r/reinforcementlearning/comments/a4qwva/metalearning_learning_to_learn_fast_lilian_weng/ | "Meta-Learning: Learning to Learn Fast", Lilian Weng [metric learning, MANN & meta networks, MAML/REPTILE] : reinforcementlearning
https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html | Meta Reinforcement Learning
https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html | Meta-Learning: Learning to Learn Fast

https://zhuanlan.zhihu.com/p/149725307 | 【萝卜日记第38期】Follow me,follow you——苍穹的法芙娜EXODUS - 知乎
https://zhuanlan.zhihu.com/p/149724367 | 【萝卜日记第37期】我存在于此——苍穹的法芙娜（OVA&剧场版） - 知乎

https://zhuanlan.zhihu.com/p/110955275 | 李航统计学习方法（第四章） - 知乎
https://zhuanlan.zhihu.com/p/149890569 | "Linformer" 拍了拍 "被吊打的Transformers 后浪们" - 知乎
https://www.zhihu.com/question/349958732/answer/945349902 | 有哪些令你印象深刻的魔改transformer？ - 知乎
https://zhuanlan.zhihu.com/p/259765593 | 高效Transformer层出不穷，谷歌团队综述文章一网打尽 - 知乎
https://www.zhihu.com/question/319339652 | transformer中为什么使用不同的K 和 Q， 为什么不能使用同一个值？ - 知乎
https://zhuanlan.zhihu.com/p/223430086 | Longformer: 局部Attention和全局attention的混搭 - 知乎
https://zhuanlan.zhihu.com/p/208134502 | Reformer: 搞笑（高效）的transformer结构(2020年2月Google) - 知乎
https://zhuanlan.zhihu.com/c_1213397558586257408 | 李航统计学习方法 - 知乎

https://zhuanlan.zhihu.com/p/107944440 | 李航统计学习方法（第一章） - 知乎
https://www.zhihu.com/column/c_1213397558586257408 | 李航统计学习方法 - 知乎
https://zhuanlan.zhihu.com/p/114284754 | 李航统计学习方法（第六章） - 知乎
https://zhuanlan.zhihu.com/p/115631923 | 广义线性模型（第六章补充） - 知乎
