https://zhuanlan.zhihu.com/p/261412153 | Ê±ÇÈÅì‰πã‰∫∫Ôºå‰∏çÈóÆÂØíÊöëÔºàÂÖ≠Ôºâ - Áü•‰πé
https://zhuanlan.zhihu.com/p/355523266 | ÊúÄÁÆÄÂçïÁöÑself-supervisedÊñπÊ≥ï - Áü•‰πé
https://www.google.com/search?q=BYOL&oq=BYOL&aqs=chrome..69i57j69i61&sourceid=chrome&ie=UTF-8 | BYOL - Google Search
https://zhuanlan.zhihu.com/p/150358540 | Ëá™ÁõëÁù£ÈªëÈ©¨SimCLRv2Êù•‰∫ÜÔºÅÊèêÂá∫Ëí∏È¶èÊñ∞ÊÄùË∑ØÔºåÂèØËøÅÁßªËá≥Â∞èÊ®°ÂûãÔºåÊÄßËÉΩÁ≤æÂ∫¶Ë∂ÖË∂äÊúâÁõëÁù£ - Áü•‰πé
https://zhuanlan.zhihu.com/p/334732028 | Ëá™ÁõëÁù£ÂØπÊØîÂ≠¶‰π†ÔºàContrastive LearningÔºâÁªºËø∞+‰ª£Á†Å - Áü•‰πé
https://ai.googleblog.com/2021/06/extending-contrastive-learning-to.html | Extending Contrastive Learning to the Supervised Setting ‚Äì Google AI Blog
https://zhuanlan.zhihu.com/p/205636123 | Ëá™ÁõëÁù£Â≠¶‰π†BYOL‰∏≠È≠îÈ¨ºBN - Áü•‰πé
https://www.zhihu.com/question/402452508 | (42 Â∞ÅÁßÅ‰ø° / 80 Êù°Ê∂àÊÅØ) Â¶Ç‰ΩïËØÑ‰ª∑DeepmindËá™ÁõëÁù£Êñ∞‰ΩúBYOLÔºü - Áü•‰πé
https://generallyintelligent.com/blog/2020-08-24-understanding-self-supervised-contrastive-learning/ | Understanding Self-Supervised and Contrastive Learning with "Bootstrap Your Own Latent" (BYOL) - generally intelligent
https://aclanthology.org/2022.coling-1.222.pdf | COPNER Contrastive Learning with Prompt Guiding for Few-shot Named Entity Recognition - ACL-COLING-2022_2022.coling-1.222
https://arxiv.org/pdf/2202.06417.pdf | A Contrastive Framework for Neural Text Generation - Arxiv-2202.06417
https://arxiv.org/pdf/2106.06823.pdf | Prompting Contrastive Explanations for Commonsense Reasoning Tasks - Arxiv-2106.06823
https://arxiv.org/pdf/2204.10298.pdf | DiffCSE Difference-based Contrastive Learning for Sentence Embeddings - Arxiv-2204.10298
https://arxiv.org/pdf/2210.09150.pdf | Prompting GPT-3 To Be Reliable - Arxiv-2210.09150
https://github.com/NoviScl/GPT3-Reliability | NoviScl/GPT3-Reliability
https://arxiv.org/pdf/1806.10348.pdf | Learning Visually-Grounded Semantics from Contrastive Adversarial Samples - Arxiv-1806.10348
https://proceedings.mlr.press/v162/saunshi22a/saunshi22a.pdf | Understanding Contrastive Learning Requires Incorporating Inductive Biases - PMLR-2022-saunshi22a
https://arxiv.org/pdf/2104.08812.pdf | Contrastive Out-of-Distribution Detection for Pretrained Transformers - Arxiv-2104.08812
https://arxiv.org/abs/2210.03162 | Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models - Arxiv-2210.03162
https://proceedings.mlr.press/v162/zhou22l/zhou22l.pdf | Contrastive Learning with Boosted Memorization - PMLR-2022-zhou22l
https://openaccess.thecvf.com/content/CVPR2022/papers/Yao_PCL_Proxy-Based_Contrastive_Learning_for_Domain_Generalization_CVPR_2022_paper.pdf | PCL Proxy-Based Contrastive Learning for Domain Generalization - CVPR-2022_31606908
https://arxiv.org/abs/2204.07596 | Perfectly Balanced Improving Transfer and Robustness of Supervised Contrastive Learning - Arxiv-2204.07596
https://arxiv.org/pdf/2210.15097.pdf | Contrastive Decoding Open-ended Text Generation as Optimization - Arxiv-2210.15097
https://arxiv.org/pdf/2111.00899.pdf | Equivariant Contrastive Learning - Arxiv-2111.00899
https://www.google.com/search?q=non-linguistic+supervision+for+contrastive+learning+of+sentence+embeddings&oq=Non-Linguistic+Supervision+for+Contrastive+Learning+of+Sentence+Embeddings&aqs=chrome.0.0i512.250j0j1&sourceid=chrome&ie=UTF-8 | non-linguistic supervision for contrastive learning of sentence embeddings - Google Search
https://arxiv.org/abs/2204.10298 | DiffCSE Difference-based Contrastive Learning for Sentence Embeddings - Arxiv-2204.10298
https://arxiv.org/abs/2007.15651 | Contrastive Learning for Unpaired Image-to-Image Translation - Arxiv-2007.15651
https://zhuanlan.zhihu.com/p/334772391 | Noise Contrastive Estimation Ââç‰∏ñ‰ªäÁîü‚Äî‚Äî‰ªé NCE Âà∞ InfoNCE - Áü•‰πé
https://zhuanlan.zhihu.com/p/357071960 | CVPR2021Ëá™ÁõëÁù£Â≠¶‰π†ËÆ∫Êñá: ÁêÜËß£ÂØπÊØîÊçüÂ§±ÁöÑÊÄßË¥®‰ª•ÂèäÊ∏©Â∫¶Á≥ªÊï∞ÁöÑ‰ΩúÁî® - Áü•‰πé
https://lilianweng.github.io/posts/2021-05-31-contrastive/ | Contrastive Representation Learning | Lil'Log
https://lilianweng.github.io/posts/2019-11-10-self-supervised/ | Self-Supervised Representation Learning | Lil'Log
https://mail.google.com/mail/u/0/#inbox/FMfcgzGslkrfDlrxHBqgQwNMgfCXlMJC | Authenticate Your Email Address - 1999j0615une@gmail.com - Gmail

https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00434/108865/Self-Diagnosis-and-Self-Debiasing-A-Proposal-for | Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP | Transactions of the Association for Computational Linguistics | MIT Press
https://openreview.net/forum?id=CQsmMYmlP5T | Git Re-Basin Merging Models modulo Permutation Symmetries - OR-ICLR-2023_CQsmMYmlP5T
https://arxiv.org/abs/2209.04836 | Git Re-Basin Merging Models modulo Permutation Symmetries - OR-ICLR-2023_CQsmMYmlP5T
https://arxiv.org/abs/1910.03065 | Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations - Arxiv-1910.03065
https://github.com/samuela/git-re-basin | samuela/git-re-basin: Code release for "Git Re-Basin: Merging Models modulo Permutation Symmetries"
https://arxiv.org/abs/2209.11055 | Efficient Few-Shot Learning Without Prompts - Arxiv-2209.11055
http://proceedings.mlr.press/v97/chen19g/chen19g.pdf | Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels - PMLR-2019-chen19g
https://arxiv.org/abs/2205.11558 | Using Natural Language and Program Abstractions to Instill Human Inductive Biases in Machines - Arxiv-2205.11558
https://arxiv.org/abs/2206.08496 | Self-Supervised Contrastive Pre-Training For Time Series via Time-Frequency Consistency - Arxiv-2206.08496
https://arxiv.org/abs/2110.15943 | MetaICL Learning to Learn In Context - Arxiv-2110.15943
https://www.google.com/search?q=meta-learning+via+language+model+in-context+tuning&newwindow=1&sxsrf=ALiCzsa3EDwkzoxpax8RkM0P836TR7dAzg%3A1669920709812&ei=xfeIY-OVMdXW5NoPh_yhgA0&oq=meta+learning+via+in+c&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAxgAMgYIABAWEB4yBQgAEIYDMgUIABCGAzIFCAAQhgMyBQgAEIYDMgUIABCGAzoKCAAQRxDWBBCwAzoECCMQJzoICC4QsQMQgwE6EQguEIAEELEDEIMBEMcBENEDOggIABCxAxCDAToICC4QgwEQsQM6CwgAEIAEELEDEIMBOgsILhCxAxCDARDUAjoLCC4QgAQQsQMQgwE6BQgAEIAEOg4ILhCABBCxAxDHARDRAzoRCC4QgAQQsQMQgwEQ5QQQ1AI6CAgAEIAEELEDOggIABCABBDLAToHCAAQgAQQCkoECEEYAEoECEYYAFCnMFiFSmC3T2gEcAF4AYABZIgBgg6SAQQyMS4xmAEAoAEByAEIwAEB&sclient=gws-wiz-serp | meta-learning via language model in-context tuning - Google Search
https://scholar.google.com/scholar?q=%20Language%20models%20(mostly)%20know%20what%20they%20know | Google Scholar
https://arxiv.org/pdf/2109.15103.pdf | Scalable Rule-Based Representation Learning for Interpretable Classification - Arxiv-2109.15103
https://zhuanlan.zhihu.com/p/385866470 | Ëß£ËØªÊ®°ÂûãÂéãÁº©9ÔºöÊó†ÈúÄÊï∞ÊçÆÁöÑÁ•ûÁªèÁΩëÁªúÂéãÁº©ÊäÄÊúØ (‰∏Ä) - Áü•‰πé
https://zhuanlan.zhihu.com/p/308301901 | 3WÂ≠óÈïøÊñáÂ∏¶‰Ω†ËΩªÊùæÂÖ•Èó®ËßÜËßâtransformer - Áü•‰πé
https://zhuanlan.zhihu.com/p/160206075 | Knowledge DistillationÔºàÁü•ËØÜËí∏È¶èÔºâReview--20ÁØápaperÂõûÈ°æ - Áü•‰πé
https://posts.careerengine.us/p/5e040074089a4c71be7da859 | ‰∏ÄÊñáÊÄªËßàÁü•ËØÜËí∏È¶èÊ¶ÇËø∞
https://github.com/Guang000/Awesome-Dataset-Distillation | Guang000/Awesome-Dataset-Distillation: Awesome Dataset Distillation Papers
https://arxiv.org/pdf/2102.09559.pdf | CReST A Class-Rebalancing Self-Training Framework for Imbalanced Semi-Supervised Learning - Arxiv-2102.09559
https://arxiv.org/abs/2208.09392 | Cold Diffusion Inverting Arbitrary Image Transforms Without Noise - Arxiv-2208.09392
https://arxiv.org/pdf/2204.11790.pdf | Can Rationalization Improve Robustness? - Arxiv-2204.11790
https://github.com/SforAiDl/KD_Lib | SforAiDl/KD_Lib: A Pytorch Knowledge Distillation library for benchmarking and extending works in the domains of Knowledge Distillation, Pruning, and Quantization.
https://arxiv.org/pdf/2012.15699.pdf | Better Robustness by More Coverage Adversarial Training with Mixup Augmentation for Robust Fine-tuning - Arxiv-2012.15699
https://github.com/LJY-HY/MentorMix_pytorch | LJY-HY/MentorMix_pytorch: [MentorMix] "Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels" implemented in the PyTorch version.
https://ai.googleblog.com/2020/08/understanding-deep-learning-on.html | Understanding Deep Learning on Controlled Noisy Labels ‚Äì Google AI Blog
https://arxiv.org/pdf/1911.09781.pdf | Beyond Synthetic Noise Deep Learning on Controlled Noisy Labels - Arxiv-1911.09781
https://aclanthology.org/2022.acl-long.201.pdf | Sequence-to-Sequence Knowledge Graph Completion and Question Answering - ACL-ACL-2022_2022.acl-long.201
https://arxiv.org/pdf/2112.08348.pdf | Prompt Waywardness The Curious Case of Discretized Interpretation of Continuous Prompts - Arxiv-2112.08348
https://arxiv.org/pdf/2210.12517.pdf | Exploring The Landscape of Distributional Robustness for Question Answering Models - Arxiv-2210.12517
https://arxiv.org/pdf/2205.12507.pdf | Re-Examining Calibration The Case of Question Answering - Arxiv-2205.12507
https://arxiv.org/pdf/2207.00746.pdf | INSCIT Information-Seeking Conversations with Mixed-Initiative Interactions - Arxiv-2207.00746
https://arxiv.org/abs/1708.03999 | ZOO Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models - Arxiv-1708.03999

https://zhuanlan.zhihu.com/p/316865623 | 2020Âπ¥9ÊúàË∞∑Ê≠åÁ†îÁ©∂ÁªôÂá∫ÁöÑÁªºËø∞‚ÄúEfficient Transformers: A Survey‚Äù - Áü•‰πé
https://zhuanlan.zhihu.com/p/366744794 | Ê∑±Â∫¶Â≠¶‰π†‰∏≠‰∏çÂêåÁ±ªÂûãÂç∑ÁßØÁöÑÁªºÂêà‰ªãÁªçÔºö2DÂç∑ÁßØ„ÄÅ3DÂç∑ÁßØ„ÄÅËΩ¨ÁΩÆÂç∑ÁßØ„ÄÅÊâ©Âº†Âç∑ÁßØ„ÄÅÂèØÂàÜÁ¶ªÂç∑ÁßØ„ÄÅÊâÅÂπ≥Âç∑ÁßØ„ÄÅÂàÜÁªÑÂç∑ÁßØ„ÄÅÈöèÊú∫ÂàÜÁªÑÂç∑ÁßØ„ÄÅÈÄêÁÇπÂàÜÁªÑÂç∑ÁßØÁ≠âpytorch‰ª£Á†ÅÂÆûÁé∞ÂíåËß£Êûê„ÄÇ - Áü•‰πé
https://zhuanlan.zhihu.com/p/357628257 | Reformer: ‰∏Ä‰∏™Âú®ËÆ≠ÁªÉÈò∂ÊÆµÂ≠òÂÇ®ÊûÅËá¥ÂéãÁº©ÁöÑTransformerÊ®°Âûã - Áü•‰πé
https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#ncaloss | Losses - PyTorch Metric Learning
https://css-tricks.com/bem-101/ | BEM 101 | CSS-Tricks - CSS-Tricks
https://www.spaces.ac.cn/archives/7708/comment-page-1 | ÂÜçË∞àÁ±ªÂà´‰∏çÂπ≥Ë°°ÈóÆÈ¢òÔºöË∞ÉËäÇÊùÉÈáç‰∏éÈ≠îÊîπLossÁöÑÂØπÊØîËÅîÁ≥ª - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://posts.careerengine.us/p/60c2ebb480e090697baeb2a3 | TransformerÊ®°ÂûãÊúâÂ§öÂ∞ëÁßçÂèò‰ΩìÔºüÂ§çÊó¶ÈÇ±Èî°ÈπèÊïôÊéàÂõ¢ÈòüÂÅö‰∫ÜÂÖ®Èù¢ÁªºËø∞
https://arxiv.org/pdf/2106.04554.pdf | A Survey of Transformers - Arxiv-2106.04554
https://arxiv.org/abs/2111.07624 | Attention Mechanisms in Computer Vision A Survey - Arxiv-2111.07624
https://arxiv.org/pdf/2001.04451.pdf | Reformer The Efficient Transformer - OR-ICLR-2020_rkgNKkHtvB
https://github.com/xmu-xiaoma666/External-Attention-pytorch#re-parameter-series | xmu-xiaoma666/External-Attention-pytorch: üçÄ Pytorch implementation of various Attention Mechanisms, MLP, Re-parameter, Convolution, which is helpful to further understand papers.‚≠ê‚≠ê‚≠ê
https://zhuanlan.zhihu.com/p/109342043 | GANËØÑ‰ª∑ÊåáÊ†áÊúÄÂÖ®Ê±áÊÄª - Áü•‰πé
https://arxiv.org/pdf/1802.03446.pdf | Pros and Cons of GAN Evaluation Measures - Arxiv-1802.03446
https://wryou.xyz/2021/01/16/Evaluating-GANs.html#fnref:2 | Evaluating GANs - Wonryong Ryou
https://zhuanlan.zhihu.com/p/115741192 | ÂØπReformerÁöÑÊ∑±ÂÖ•Ëß£ËØª - Áü•‰πé
https://zhuanlan.zhihu.com/p/139220925 | üí°Illustrating the Reformer - Áü•‰πé
https://www.jiqizhixin.com/articles/2019-03-23-5 | ÂõõÂùóGPUÂç≥ÂèØËÆ≠ÁªÉBigGANÔºö„ÄåÂÆòÊñπÁâà„ÄçPyTorchÂÆûÁé∞Âá∫ÁÇâ | Êú∫Âô®‰πãÂøÉ
https://arxiv.org/pdf/2111.06091.pdf | A Survey of Visual Transformers - Arxiv-2111.06091
https://www.zhihu.com/question/410332622 | (42 Â∞ÅÁßÅ‰ø° / 80 Êù°Ê∂àÊÅØ) Ê®°ÂûãÁöÑRobustnessÂíåGeneralizationÊòØ‰ªÄ‰πàÂÖ≥Á≥ªÔºü - Áü•‰πé
https://arxiv.org/pdf/2008.10032.pdf | Seesaw Loss for Long-Tailed Instance Segmentation - Arxiv-2008.10032
https://arxiv.org/pdf/2106.01465.pdf | Ethical-Advice Taker Do Language Models Understand Natural Language Interventions? - Arxiv-2106.01465
https://github.com/hbaniecki/adversarial-explainable-ai#attacks-on-explainability-and-fairness | hbaniecki/adversarial-explainable-ai: üí° Adversarial attacks on explanations and how to defend them
https://arxiv.org/abs/2106.08367 | What Context Features Can Transformer Language Models Use? - Arxiv-2106.08367
https://arxiv.org/abs/2106.03993 | Lexicon Learning for Few-Shot Neural Sequence Modeling - Arxiv-2106.03993
https://arxiv.org/abs/2106.00737 | Implicit Representations of Meaning in Neural Language Models - Arxiv-2106.00737
https://arxiv.org/abs/2201.11114 | Natural Language Descriptions of Deep Visual Features - Arxiv-2201.11114
http://proceedings.mlr.press/v139/wong21a.html | Leveraging Language to Learn Program Abstractions and Search Heuristics - PMLR-2021-wong21a
https://arxiv.org/abs/2110.01517 | Skill Induction and Planning with Latent Language - ACL-ACL-2022_2022.acl-long.120
https://openreview.net/forum?id=NudBMY-tzDr | Natural Language Descriptions of Deep Visual Features - Arxiv-2201.11114
https://arxiv.org/abs/2202.01771 | Pre-Trained Language Models for Interactive Decision-Making - Arxiv-2202.01771
https://www.bilibili.com/video/BV18G41137Ws/?spm_id_from=333.1007.tianma.2-3-6.click&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | ÈáëÂ∫∏Áúã‰∫ÜÈÉΩÊëáÂ§¥Ôºå‰ºòÁßÄÁöÑÊ≠¶‰æ†Ê∏∏ÊàèËØ•ÊòØÊÄéÊ†∑ÁöÑÔºü_ÂìîÂì©ÂìîÂì©bilibili
https://www.google.com/search?q=feedback+transformer&oq=feedback+transformer&aqs=chrome..69i57j0i512l6j69i65.2237j0j1&sourceid=chrome&ie=UTF-8 | feedback transformer - Google Search
https://github.com/lucidrains/x-transformers | lucidrains/x-transformers: A simple but complete full-attention transformer with a set of promising experimental features from various papers
https://github.com/lucidrains/feedback-transformer-pytorch | lucidrains/feedback-transformer-pytorch: Implementation of Feedback Transformer in Pytorch
https://github.com/thumbe3/label-noise-nlp | thumbe3/label-noise-nlp
https://github.com/haiphanNJIT/StoBatch | haiphanNJIT/StoBatch: Scalable Differential Privacy with Certified Robustness in Adversarial Learning (ICML'2020)
https://github.com/zbchern/awesome-machine-learning-reliability | zbchern/awesome-machine-learning-reliability: A curated list of awesome resources regarding machine learning reliability.
https://scholar.google.com/scholar?start=30&hl=en&as_sdt=40000005&sciodt=0,22&as_ylo=2022&cites=1496995241845170163&scipsc= | Petroni: Language models as knowledge bases? - Google Scholar
https://arxiv.org/pdf/2210.08536.pdf | Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding - Arxiv-2210.08536
https://arxiv.org/pdf/2211.05994.pdf | A Survey of Knowledge-Enhanced Pre-trained Language Models - Arxiv-2211.05994
https://watermark.silverchair.com/tacl_a_00454.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAr0wggK5BgkqhkiG9w0BBwagggKqMIICpgIBADCCAp8GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMKs95iKdDJGdDrL9VAgEQgIICcNfeMNiFpbXfGI3q1Jsl3omjxqCVTUeZsYS8SWBBXbBaL1QR5thZl5tswYU3mK-cyK90CjjaDZjR3tVxYWJkM0rruX7zYO2naxllHpFfIqNLIG0kQgxWqvPQSjlZ2CFu5ES54Y7pMLrxBlfsurfebCj5biuqk6Y0EUK6xhIxzwTkII9HKz1d7DywS9Mfrp56yCwmDy8eDW0TZXTY8mDnzWN4S3HElwa-kFZ7pf_dPr6HS_PszYIuRfms_JpjcDEof41qpJIGEMyKud8C_GxFxhi_ub_77xTNSKToykDRPWaQoy-PHqSo4jfR1FHbA8MS_yUETWqvSLzopo5vh_Tz3WbSLOKpeR8VuUuookYr-u_je82LFcIieLYn9NAZj7kl_JrrnoyYMw0Oyrb3LAO_gaZXlOd0LbYxDPSi6klKLLLuMPFBWWVTe61_MgbJ98UJdAWHkq9ZvMDRIVzYD2DkyM4686cDZMZiGZrvy1Tlivh67jybFvKJouvhAfl4Wj4gFlDyf0HCgADDmSTO4D6BMPNxKjT4Dpir40qagfJ1HXFTowGIbkjRH8pHHknbICAiH5kiE1XXISuUpmX02qKoccb4LnORIWy_Q7L85oO_3Ki2eY3it95pwoWA5aWdPff01JIpvS4M6wtmhMt5ynZUC0Z1HXhpnJ2plRlLhf-CBNBq1bLAbwLBP05kzUBtEStUvel0Wn25fu7nFYzIkGWIi8Ki3Z7y54HF8kiYNY6R_1tnHLkYRGX9MONYi0e8m4-tXOLaU6CrTVWk5Io61eCRnyeLV3QpEZ1n9wrfqlgYDn1X-S5oDMbV9rXhEMqnV_2jfQ | https://watermark.silverchair.com/tacl_a_00454.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAr0wggK5BgkqhkiG9w0BBwagggKqMIICpgIBADCCAp8GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMKs95iKdDJGdDrL9VAgEQgIICcNfeMNiFpbXfGI3q1Jsl3omjxqCVTUeZsYS8SWBBXbBaL1QR5thZl5tswYU3mK-cyK90CjjaDZjR3tVxYWJkM0rruX7zYO2naxllHpFfIqNLIG0kQgxWqvPQSjlZ2CFu5ES54Y7pMLrxBlfsurfebCj5biuqk6Y0EUK6xhIxzwTkII9HKz1d7DywS9Mfrp56yCwmDy8eDW0TZXTY8mDnzWN4S3HElwa-kFZ7pf_dPr6HS_PszYIuRfms_JpjcDEof41qpJIGEMyKud8C_GxFxhi_ub_77xTNSKToykDRPWaQoy-PHqSo4jfR1FHbA8MS_yUETWqvSLzopo5vh_Tz3WbSLOKpeR8VuUuookYr-u_je82LFcIieLYn9NAZj7kl_JrrnoyYMw0Oyrb3LAO_gaZXlOd0LbYxDPSi6klKLLLuMPFBWWVTe61_MgbJ98UJdAWHkq9ZvMDRIVzYD2DkyM4686cDZMZiGZrvy1Tlivh67jybFvKJouvhAfl4Wj4gFlDyf0HCgADDmSTO4D6BMPNxKjT4Dpir40qagfJ1HXFTowGIbkjRH8pHHknbICAiH5kiE1XXISuUpmX02qKoccb4LnORIWy_Q7L85oO_3Ki2eY3it95pwoWA5aWdPff01JIpvS4M6wtmhMt5ynZUC0Z1HXhpnJ2plRlLhf-CBNBq1bLAbwLBP05kzUBtEStUvel0Wn25fu7nFYzIkGWIi8Ki3Z7y54HF8kiYNY6R_1tnHLkYRGX9MONYi0e8m4-tXOLaU6CrTVWk5Io61eCRnyeLV3QpEZ1n9wrfqlgYDn1X-S5oDMbV9rXhEMqnV_2jfQ
https://arxiv.org/pdf/2204.06031.pdf | A Review on Language Models as Knowledge Bases - Arxiv-2204.06031
https://arxiv.org/pdf/2201.09680.pdf | Relational Memory Augmented Language Models - Arxiv-2201.09680
https://arxiv.org/pdf/2211.08332.pdf | Versatile Diffusion Text, Images and Variations All in One Diffusion Model - Arxiv-2211.08332
https://arxiv.org/abs/2211.05783 | Unifying Flow, Stereo and Depth Estimation - Arxiv-2211.05783
https://arxiv.org/abs/2211.07292 | Fast Text-Conditional Discrete Denoising on Vector-Quantized Latent Spaces - Arxiv-2211.07292
https://arxiv.org/pdf/2211.07830.pdf | Prompting Language Models for Linguistic Structure - Arxiv-2211.07830
https://aclanthology.org/2022.naacl-main.59.pdf | Explaining Toxic Text via Knowledge Enhanced Text Generation - ACL-NAACL-2022_2022.naacl-main.59
https://arxiv.org/pdf/2203.10652.pdf] | Continual Sequence Generation with Adaptive Compositional Modules - Arxiv-2203.10652
https://www.manhuagui.com/comic/11223/ | ËµîÂëΩÈáëÊº´Áîª_Ë°ÄÂÅøÈáëÊº´Áîª_Ê≤ôÊùëÂπøÊòé - ÁúãÊº´Áîª
https://www.google.com/search?q=%E9%A9%BE%E7%AC%BC%E7%9C%9F%E5%A4%AA%E9%83%8E | È©æÁ¨ºÁúüÂ§™ÈÉé - Google Search
https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00511/113490 | Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond | Transactions of the Association for Computational Linguistics | MIT Press
https://proceedings.neurips.cc/paper/2021/file/22b1f2e0983160db6f7bb9f62f4dbb39-Paper.pdf | How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness? - NeurIPS-2021_22b1f2e0
https://arxiv.org/pdf/2101.00288.pdf | Polyjuice Generating Counterfactuals for Explaining, Evaluating, and Improving Models - Arxiv-2101.00288
https://www.google.com/search?q=%E5%9C%B0%E9%9B%B7%E9%9C%87&sourceid=chrome&ie=UTF-8 | Âú∞Èõ∑Èúá - Google Search
https://syntaxgym.org/ | SyntaxGym
https://arxiv.org/pdf/2104.03474.pdf | Revisiting Simple Neural Probabilistic Language Models - Arxiv-2104.03474
https://arxiv.org/pdf/1506.05254.pdf | Gradient Estimation Using Stochastic Computation Graphs - Arxiv-1506.05254
https://arxiv.org/abs/2104.07000 | IGA An Intent-Guided Authoring Assistant - Arxiv-2104.07000
https://arxiv.org/abs/2103.15335 | Changing the Mind of Transformers for Topically-Controllable Language Generation - Arxiv-2103.15335
https://arxiv.org/abs/2203.10053 | RELIC Retrieving Evidence for Literary Claims - Arxiv-2203.10053
https://arxiv.org/abs/2205.09726 | RankGen Improving Text Generation with Large Ranking Models - Arxiv-2205.09726
https://scholar.google.com/scholar?start=10&hl=en&as_sdt=40000005&sciodt=0,22&as_ylo=2021&cites=3065323231297656090&scipsc= | Krishna: Reformulating unsupervised style transfer... - Google Scholar
https://arxiv.org/pdf/2109.05554.pdf | No True State-of-the-Art? OOD Detection Methods are Inconsistent across Datasets - Arxiv-2109.05554
https://yobibyte.github.io/files/paper_notes/dice.pdf | dice.pdf
https://arxiv.org/pdf/2208.00005.pdf | Testing Relational Understanding in Text-Guided Image Generation - Arxiv-2208.00005
https://arxiv.org/pdf/2107.06278.pdf | Per-Pixel Classification is Not All You Need for Semantic Segmentation - Arxiv-2107.06278
https://aclanthology.org/2022.repl4nlp-1.23.pdf | Towards Improving Selective Prediction Ability of NLP Systems - ACL-ACL| RepL4NLP| WS-2022_2022.repl4nlp-1.23
https://www.google.com/search?q=stochastic+computational+graph&oq=stochastic+computational+graph&aqs=chrome..69i57j0i22i30j0i390l5.4455j0j1&sourceid=chrome&ie=UTF-8 | stochastic computational graph - Google Search

http://web.cs.ucla.edu/~patricia.xiao/files/CS_260_Cheatsheet_version2.pdf | CS_260_Cheatsheet_version2.pdf

https://www.google.com/search?q=lime+learning+inductive+bias+for+primitives+of+mathematical+reasoning&oq=lime+learning+ind&aqs=chrome.4.69i57j0i546l2j0i546i649j0i546l2.5197j0j1&sourceid=chrome&ie=UTF-8 | lime learning inductive bias for primitives of mathematical reasoning - Google Search
https://www.google.com/search?q=taskmatrix+ai&newwindow=1&sxsrf=APwXEdcrWxju921Ogk05hGsm8rYznMVB2w%3A1680311507938&ei=04QnZMfuOJif5NoPq8uasAM&ved=0ahUKEwiHpKuUwIf-AhWYD1kFHaulBjYQ4dUDCBA&uact=5&oq=taskmatrix+ai&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIGCAAQDRADMgYIABANEAMyCAgAEIoFEIYDMggIABCKBRCGAzIICAAQigUQhgMyCAgAEIoFEIYDOgoIABBHENYEELADOgcIIxDqAhAnOg0IABCPARDqAhC0AhgBOg0ILhCPARDqAhC0AhgBOgQIIxAnOggIABCKBRCRAjoNCAAQigUQsQMQgwEQQzoHCAAQigUQQzoLCC4QgAQQsQMQgwE6CwgAEIoFELEDEIMBOg4ILhCABBCxAxDHARDRAzoQCC4QigUQsQMQxwEQ0QMQQzoRCC4QgAQQsQMQgwEQxwEQ0QM6CggAEIAEEBQQhwI6BwguEIoFEEM6BQgAEIAEOhAIABCABBAUEIcCELEDEIMBOgQIABADOg4ILhCABBCxAxCDARDUAjoLCAAQgAQQsQMQgwE6EwguEIoFELEDEIMBENQCEEMQ6gQ6EAguEIMBENQCELEDEIoFEEM6CggAEIoFELEDEEM6EwguEIAEELEDEIMBENQCEAoQ6gQ6DQgAEIAEELEDEIMBEAo6DQguEIAEELEDEIMBEAo6EAguEIMBENQCELEDEIAEEAo6CggAEIAEELEDEAo6EAguEIAEELEDEIMBENQCEAo6BggAEAoQAzoHCAAQgAQQCjoICAAQHhAPEAo6BwgAEA0QgAQ6BggAEB4QDToICAAQHhANEA86BggAEBYQHjoKCAAQFhAeEA8QCkoECEEYAFDbBFjdkwFgr5UBaAJwAXgAgAHEAYgBrwuSAQM1LjiYAQCgAQGwARTIAQfAAQHaAQYIARABGAo&sclient=gws-wiz-serp | taskmatrix ai - Google Search
https://arxiv.org/abs/2206.10139 | Insights into Pre-training via Simpler Synthetic Tasks - Arxiv-2206.10139
https://arxiv.org/abs/2303.16434 | TaskMatrix.AI Completing Tasks by Connecting Foundation Models with Millions of APIs - Arxiv-2303.16434
https://www.google.com/search?q=visual+chatggpt&oq=visual+chatggpt&aqs=chrome..69i57j0i512j0i10i512j0i512l7.1720j0j1&sourceid=chrome&ie=UTF-8 | visual chatggpt - Google Search
https://www.google.com/search?q=your+diffusion+model+is+secretly+a+zero+shot+classifier&oq=your+diffusion+model+is+sec&aqs=chrome.0.0i3j69i57j0i390i650l3j69i61l3.6085j0j1&sourceid=chrome&ie=UTF-8 | your diffusion model is secretly a zero shot classifier - Google Search
https://www.google.com/search?q=whose+opinion+do+language+models+reflect&oq=whose+opinion+do+language+models+reflect&aqs=chrome..69i57j33i160l2.14039j0j1&sourceid=chrome&ie=UTF-8 | whose opinion do language models reflect - Google Search
https://paperswithcode.com/paper/whose-opinions-do-language-models-reflect | Whose Opinions Do Language Models Reflect? | Papers With Code
https://www.google.com/search?q=scene+graph&oq=scene+g&aqs=chrome.2.69i57j0i433i512j0i512l4j69i60l2.10906j1j1&sourceid=chrome&ie=UTF-8 | scene graph - Google Search
https://www.google.com/search?q=3DB+paper&oq=3DB+paper&aqs=chrome..69i57j33i160.2130j0j1&sourceid=chrome&ie=UTF-8 | 3DB paper - Google Search
https://www.google.com/search?q=sparks+of+artificial+general+intelligence&oq=sparks+of+&aqs=chrome.2.0i3j0i512j0i3j69i57j0i512j46i433i512j0i3j0i512l2j0i3.2379j0j1&sourceid=chrome&ie=UTF-8 | sparks of artificial general intelligence - Google Search
https://www.google.com/search?q=label+efficient+semantic+segmentation&oq=label+efficient&aqs=chrome.0.0i512j69i57j0i512j0i22i30l3j0i390i650l2.2768j0j1&sourceid=chrome&ie=UTF-8 | label efficient semantic segmentation - Google Search

https://dandanzan.net/dianying/20230132.html | „ÄäÁΩëÁªúË∞úË∏™2„Äã2023Âπ¥ÁæéÂõΩÂâßÊÉÖÊÉäÊÇöÁîµÂΩ±Âú®Á∫øËßÇÁúã - ËõãËõãËµûÂΩ±Èô¢

https://arxiv.org/pdf/2203.01543.pdf | QaNER Prompting Question Answering Models for Few-shot Named Entity Recognition - Arxiv-2203.01543
https://arxiv.org/pdf/2303.04132.pdf | Exploiting Asymmetry for Synthetic Training Data Generation SynthIE and the Case of Information Extraction - Arxiv-2303.04132
https://arxiv.org/pdf/2011.01549.pdf | DAGA Data Augmentation with a Generation Approach for Low-resource Tagging Tasks - Arxiv-2011.01549
https://laion.ai/blog/open-flamingo/ | Announcing OpenFlamingo: An open-source framework for training vision-language models with in-context learning | LAION
https://github.com/hwchase17/langchain | hwchase17/langchain: ‚ö° Building applications with LLMs through composability ‚ö°
https://python.langchain.com/en/latest/ | Welcome to LangChain ‚Äî ü¶úüîó LangChain 0.0.125
https://python.langchain.com/en/latest/modules/chains/index_examples/summarize.html | Summarization ‚Äî ü¶úüîó LangChain 0.0.126
https://python.langchain.com/en/latest/use_cases/personal_assistants.html | Personal Assistants ‚Äî ü¶úüîó LangChain 0.0.125
https://python.langchain.com/en/latest/use_cases/question_answering.html | Question Answering over Docs ‚Äî ü¶úüîó LangChain 0.0.126
https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation | SolidGoldMagikarp (plus, prompt generation) - LessWrong
https://github.com/teticio/audio-diffusion | teticio/audio-diffusion: Apply diffusion models using the new Hugging Face diffusers package to synthesize music instead of images.
https://openreview.net/pdf?id=68EuccCtO5i | Differentially Private Model Compression - OR-NeurIPS-2022_68EuccCtO5i
https://arxiv.org/pdf/2201.00971.pdf | Submix Practical Private Prediction for Large-Scale Language Models - Arxiv-2201.00971
https://arxiv.org/pdf/2205.13621.pdf | Differentially Private Decoding in Large Language Models - Arxiv-2205.13621
https://arxiv.org/pdf/2303.16203.pdf | Your Diffusion Model is Secretly a Zero-Shot Classifier - Arxiv-2303.16203

https://arxiv.org/pdf/2212.14024.pdf | Demonstrate-Search-Predict Composing retrieval and language models for knowledge-intensive NLP - Arxiv-2212.14024
https://arxiv.org/pdf/2302.07371.pdf | AutoBiasTest Controllable Sentence Generation for Automated and Open-Ended Social Bias Testing in Language Models - Arxiv-2302.07371
https://arxiv.org/pdf/2110.05679.pdf | Large Language Models Can Be Strong Differentially Private Learners - Arxiv-2110.05679
https://arxiv.org/pdf/2209.01975.pdf | Selective Annotation Makes Language Models Better Few-Shot Learners - Arxiv-2209.01975
https://arxiv.org/pdf/2112.08633.pdf | Learning To Retrieve Prompts for In-Context Learning - ACL-NAACL-2022_2022.naacl-main.191
https://github.com/OhadRubin/EPR | OhadRubin/EPR
https://arxiv.org/pdf/2109.05620.pdf | RockNER A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models - Arxiv-2109.05620
https://arxiv.org/pdf/2110.08454.pdf | Good Examples Make A Faster Learner Simple Demonstration-based Learning for Low-resource NER - Arxiv-2110.08454
https://www.ijcai.org/proceedings/2022/0590.pdf | Low-Resource NER by Data Augmentation With Prompting - IJCAI-2022_590
https://arxiv.org/abs/2302.08659 | Uncertainty-aware Self-training for Low-resource Neural Sequence Labeling - Arxiv-2302.08659

https://scholar.googleusercontent.com/scholar.bib?q=info:hG0iVOrOguoJ:scholar.google.com/&output=citation&scisdr=CgXNZnReEMjhx75lVt4:AAGBfm0AAAAAZB5jTt7SRB_TwcZ8biXT0B62OO1vyH06&scisig=AAGBfm0AAAAAZB5jTmF2ANL3ww7bgYzANmQFSQpWUMdp&scisf=4&ct=citation&cd=-1&hl=en | https://scholar.googleusercontent.com/scholar.bib?q=info:hG0iVOrOguoJ:scholar.google.com/&output=citation&scisdr=CgXNZnReEMjhx75lVt4:AAGBfm0AAAAAZB5jTt7SRB_TwcZ8biXT0B62OO1vyH06&scisig=AAGBfm0AAAAAZB5jTmF2ANL3ww7bgYzANmQFSQpWUMdp&scisf=4&ct=citation&cd=-1&hl=en
https://poe.com/claude | Poe - Claude
https://scholar.google.com/scholar?as_ylo=2022&q=in+context+learning+selection+exemplar&hl=en&as_sdt=0,22&as_vis=1 | in context learning selection exemplar - Google Scholar
https://web.mit.edu/canvas/ | MIT Canvas - Login
https://docs.google.com/spreadsheets/d/1zks-bDD4LPzV7wZ78UXVLn27nLvXqiM7rcl_L6OUoxg/edit#gid=0 | LLM Class Groups - Google Ë°®Ê†º
https://www.google.com/search?q=auto+chain+of+thought&oq=auto+chain+of+thought&aqs=chrome.0.0i512j0i15i22i30j0i22i30l5j69i60.3282j0j1&sourceid=chrome&ie=UTF-8 | auto chain of thought - Google Search
https://github.com/Timothyxxx/Chain-of-ThoughtsPapers | Timothyxxx/Chain-of-ThoughtsPapers: A trend starts from "Chain of Thought Prompting Elicits Reasoning in Large Language Models".
https://arxiv.org/pdf/2210.03493.pdf | Automatic Chain of Thought Prompting in Large Language Models - Arxiv-2210.03493
https://arxiv.org/pdf/2211.04486.pdf | Active Example Selection for In-Context Learning - Arxiv-2211.04486
https://arxiv.org/pdf/2302.12246.pdf | Active Prompting with Chain-of-Thought for Large Language Models - Arxiv-2302.12246
https://www.google.com/search?q=zero+shot+reasonser&oq=zero+shot+reasonser&aqs=chrome..69i57j0i13i512j0i390i650l5.1945j0j1&sourceid=chrome&ie=UTF-8 | zero shot reasonser - Google Search
https://arxiv.org/pdf/2205.11916.pdf | Large Language Models are Zero-Shot Reasoners - Arxiv-2205.11916
https://www.google.com/search?q=self+verification+llm&oq=self+verification+llm&aqs=chrome..69i57.2476j0j1&sourceid=chrome&ie=UTF-8 | self verification llm - Google Search
https://arxiv.org/pdf/2212.09561.pdf | Large Language Models are reasoners with Self-Verification - Arxiv-2212.09561
https://n.derek.ma/2_literature/bias | bias - Derek Ma

https://www.bilibili.com/video/BV1UD4y137vs/?spm_id_from=333.337.search-card.all.click&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | Êó•Âºè‚ÄúÂÖãËãèÈ≤Å‚ÄùÂèóÊÆãÁßΩÂΩ±Âìç ÁñØÁãÇÂêéÊùÄÊéâ7‰∏™Â©¥ÂÑø „ÄäÊÆãÁßΩ.‰∏çÂèØ‰ª•‰ΩèÁöÑÊàøÈó¥„ÄãÂâßÊÉÖÁ≤æËÆ≤/ÁªÜËäÇËß£Êûê_ÂìîÂì©ÂìîÂì©_bilibili

https://akariasai.github.io/files/llm_memorization.pdf | llm_memorization.pdf
https://arxiv.org/pdf/2201.05575.pdf | Reasoning Through Memorization Nearest Neighbor Knowledge Graph Embeddings - Arxiv-2201.05575
https://arxiv.org/pdf/2205.12674.pdf | Training Language Models with Memory Augmentation - Arxiv-2205.12674
https://github.com/Timothyxxx/RetrivalLMPapers | Timothyxxx/RetrivalLMPapers: Paper collections of retrieval-based(augmented) language model.
https://antoniolonga.github.io/Pytorch_geometric_tutorials/ | Pytorch Geometric Tutorial
https://arxiv.org/pdf/1906.05664.pdf | Calibration, Entropy Rates, and Memory in Language Models - Arxiv-1906.05664
https://arxiv.org/abs/2202.05262 | Locating and Editing Factual Associations in GPT - Arxiv-2202.05262
https://openreview.net/forum?id=gJcEM8sxHK | Mapping Language Models to Grounded Conceptual Spaces | OpenReview
https://arxiv.org/abs/2106.00737 | Implicit Representations of Meaning in Neural Language Models - Arxiv-2106.00737
https://roberttlange.github.io/posts/2020/06/lottery-ticket-hypothesis/ | The Lottery Ticket Hypothesis: A Survey - Rob‚Äôs Homepage
http://proceedings.mlr.press/v119/malach20a/malach20a.pdf | malach20a.pdf
https://arxiv.org/pdf/2208.03299.pdf | Atlas Few-shot Learning with Retrieval Augmented Language Models - Arxiv-2208.03299
https://arxiv.org/pdf/2301.12652.pdf | REPLUG Retrieval-Augmented Black-Box Language Models - Arxiv-2301.12652
https://github.com/tloen/alpaca-lora | tloen/alpaca-lora: Instruct-tune LLaMA on consumer hardware
https://github.com/AlexTMallen/adaptive-retrieval | AlexTMallen/adaptive-retrieval

https://arxiv.org/pdf/2212.10534.pdf | DISCO Distilling Phrasal Counterfactuals with Large Language Models - Arxiv-2212.10534
https://arxiv.org/pdf/2110.02467.pdf | BadPre Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models - Arxiv-2110.02467
https://openreview.net/forum?id=PS3IMnScugk | Learning to Recombine and Resample Data For Compositional Generalization | OpenReview
https://arxiv.org/pdf/2302.07452.pdf | How to Train Your DRAGON Diverse Augmentation Towards Generalizable Dense Retrieval - Arxiv-2302.07452
https://arxiv.org/pdf/2208.08984.pdf | Open-Vocabulary Panoptic Segmentation with MaskCLIP - Arxiv-2208.08984
https://harvard.zoom.us/rec/play/9QSw6sWzuS7nruMMCdNjNBf0IMrZdyIxfvw1tYTZRz7ch9NsaRfKHZ-jjPDww7gpy-USE0LUf5lYnBj6.TkHXcTuNtxDjEQ9V?continueMode=true&_x_zm_rtaid=Y2dSpXH6SkOzxD_UfsWg2A.1677764513694.6e677a81f0c314953edb7716ccdb6cf2&_x_zm_rhtaid=327 | Boaz Barak's Zoom Meeting - Zoom

https://www.google.com/search?q=diffusion+instance+segmentation&oq=diffusion+instance+segmentation&aqs=chrome..69i57j0i22i30j0i390l4.10158j0j1&sourceid=chrome&ie=UTF-8 | diffusion instance segmentation - Google Search
https://arxiv.org/abs/2212.02773 | [2212.02773] DiffusionInst: Diffusion Model for Instance Segmentation
https://github.com/ant-research/diffusion-model-for-instance-segmentation | ant-research/diffusion-model-for-instance-segmentation: This repository is the code of the paper "DiffusionInst: Diffusion Model for Instance Segmentation".

https://arxiv.org/pdf/2303.04803.pdf | Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models - Arxiv-2303.04803
https://jerryxu.net/ODISE/ | Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models
https://huggingface.co/spaces/xvjiarui/ODISE/blob/main/app.py | app.py ¬∑ xvjiarui/ODISE at main
https://github.com/lllyasviel/ControlNet | lllyasviel/ControlNet: Let us control diffusion models!
https://arxiv.org/pdf/2302.05543.pdf | Adding Conditional Control to Text-to-Image Diffusion Models - Arxiv-2302.05543
https://www.google.com/search?q=ControlNet | ControlNet - Google Search
https://github.com/facebookresearch/Mask2Former | facebookresearch/Mask2Former: Code release for "Masked-attention Mask Transformer for Universal Image Segmentation"
https://arxiv.org/pdf/2301.13188.pdf | Extracting Training Data from Diffusion Models - Arxiv-2301.13188
https://arxiv.org/pdf/2012.07805.pdf | Extracting Training Data from Large Language Models - Arxiv-2012.07805
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=q4qDvAoAAAAJ&citation_for_view=q4qDvAoAAAAJ:eQOLeE2rZwMC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=q4qDvAoAAAAJ&citation_for_view=q4qDvAoAAAAJ:Se3iqnhoufwC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=q4qDvAoAAAAJ&citation_for_view=q4qDvAoAAAAJ:KlAtU1dfN6UC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=q4qDvAoAAAAJ&citation_for_view=q4qDvAoAAAAJ:dhFuZR0502QC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=q4qDvAoAAAAJ&citation_for_view=q4qDvAoAAAAJ:kNdYIx-mwKoC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=q4qDvAoAAAAJ&citation_for_view=q4qDvAoAAAAJ:ZeXyd9-uunAC | View article
https://www.google.com/search?q=machine+unlearning&oq=machine+unlearning&aqs=chrome..69i57j0i512l6j0i22i30l3.2395j0j1&sourceid=chrome&ie=UTF-8 | machine unlearning - Google Search
https://arxiv.org/pdf/2209.02299.pdf | A Survey of Machine Unlearning - Arxiv-2209.02299
https://arxiv.org/pdf/2110.05223.pdf | Continual Learning with Differential Privacy - Arxiv-2110.05223
https://arxiv.org/pdf/1902.06497.pdf | Differentially Private Continual Learning - Arxiv-1902.06497
https://www.google.com/search?q=differential+privacy+fairness&newwindow=1&sxsrf=AJOqlzWuZGngWQiV7PJB0o3v6ePaCSBNZw%3A1679123878679&ei=pmUVZM6IKYWr5NoPsNGKgAQ&ved=0ahUKEwjO88ny9-T9AhWFFVkFHbCoAkAQ4dUDCBA&uact=5&oq=differential+privacy+fairness&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIFCAAQgAQyBggAEBYQHjIFCAAQhgMyBQgAEIYDOgoIABBHENYEELADOgcIABCwAxBDOg0IABDkAhDWBBCwAxgBOgwILhDIAxCwAxBDGAI6CggAEIAEEBQQhwI6BAgAEEM6BQguEIAESgQIQRgAUMUBWIIJYIwKaAFwAXgAgAFYiAHQBZIBATmYAQCgAQHIARHAAQHaAQYIARABGAnaAQYIAhABGAg&sclient=gws-wiz-serp | differential privacy fairness - Google Search
https://openreview.net/forum?id=zAxuIJLb38 | Knowledge Unlearning for Mitigating Privacy Risks in Language Models | OpenReview
https://openreview.net/pdf?id=zAxuIJLb38 | pdf

https://openreview.net/pdf?id=qiaRo_7Zmug | pdf
https://arxiv.org/pdf/2211.09527.pdf | Ignore Previous Prompt Attack Techniques For Language Models - Arxiv-2211.09527
https://aclanthology.org/2022.naacl-main.191.pdf | Learning To Retrieve Prompts for In-Context Learning - ACL-NAACL-2022_2022.naacl-main.191
https://arxiv.org/pdf/2212.10509.pdf | Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions - Arxiv-2212.10509
https://arxiv.org/pdf/2302.12813.pdf | Check Your Facts and Try Again Improving Large Language Models with External Knowledge and Automated Feedback - Arxiv-2302.12813
https://arxiv.org/abs/2211.07636 | EVA Exploring the Limits of Masked Visual Representation Learning at Scale - Arxiv-2211.07636

https://arxiv.org/abs/1702.08591 | The Shattered Gradients Problem If resnets are the answer, then what is the question? - Arxiv-1702.08591
https://twitter.com/SeonghyeonYe/status/1580170684466360323 | https://twitter.com/SeonghyeonYe/status/1580170684466360323
https://twitter.com/SeonghyeonYe/status/1617041418857611267 | (1) Seonghyeon Ye on Twitter: "Flipped learning paper is accepted to #ICLR2023 ! Check out our paper if you are interested in large language models &amp; instruction tuning! Paper: https://t.co/IYF1FLtape Demo: https://t.co/SobchsSOmv Special thanks to coauthors as well üòÑ" / Twitter
https://github.com/OpenBioLink/ThoughtSource | OpenBioLink/ThoughtSource: A central, open resource for data and tools related to chain-of-thought reasoning in large language models. Developed @ Samwald research group: https://samwald.info/
https://arxiv.org/abs/2302.10149 | Poisoning Web-Scale Training Datasets is Practical - Arxiv-2302.10149
https://twitter.com/omarsar0/status/1628034003776204800 | (1) elvis on Twitter: "ChatGPT is the biggest buzz in AI today. ChatGPT demonstrates remarkable capabilities so there is a high interest to replicate it. Colossal-AI just open-sourced a solution that replicates ChatGPT training process: https://t.co/XvFXjqeqZF" / Twitter
https://twitter.com/GuillaumeLample/status/1629151231800115202 | (1) Guillaume Lample on Twitter: "Today we release LLaMA, 4 foundation models ranging from 7B to 65B parameters. LLaMA-13B outperforms OPT and GPT-3 175B on most benchmarks. LLaMA-65B is competitive with Chinchilla 70B and PaLM 540B. The weights for all models are open and available at https://t.co/q51f2oPZlE 1/n https://t.co/DPyJFBfWEq" / Twitter
https://github.com/tloen/llama-int8 | tloen/llama-int8: Quantized inference code for LLaMA models
https://github.com/tatsu-lab/stanford_alpaca | tatsu-lab/stanford_alpaca: Code and documentation to train Stanford's Alpaca models, and generate the data.
https://twitter.com/cwolferesearch/status/1612099963013529609 | (1) Cameron R. Wolfe on Twitter: "OPT-IML (just released by @MetaAI) is a version of OPT-175B that's fine-tuned on a benchmark (OPT-IML Bench) of ~2000 instruction-based tasks. Prior work indicated that instruction-tuning is useful for LLMs, but we learn more details about this with OPT-IML... üßµ[1/8]" / Twitter
https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15 | Understanding the Open Pre-Trained Transformers (OPT) Library
https://docs.google.com/presentation/d/1W-c7CYEpByKNDOmGj8pZ_otY-u2yFcw0ZztsVF4vHYk/edit#slide=id.g21c5ed5ef8c_0_10 | Instruction Attack - Google ÂπªÁÅØÁâá
https://arxiv.org/pdf/2210.11416.pdf | Scaling Instruction-Finetuned Language Models - Arxiv-2210.11416
https://huggingface.co/google/flan-ul2 | google/flan-ul2 ¬∑ Hugging Face
https://arxiv.org/abs/2302.00093 | Large Language Models Can Be Easily Distracted by Irrelevant Context - Arxiv-2302.00093
https://github.com/thunlp/StyleAttack/blob/main/experiments/prepare_probingdata.py | StyleAttack/prepare_probingdata.py at main ¬∑ thunlp/StyleAttack
https://github.com/martiansideofthemoon/style-transfer-paraphrase | martiansideofthemoon/style-transfer-paraphrase: Official code and data repository for our EMNLP 2020 long paper "Reformulating Unsupervised Style Transfer as Paraphrase Generation" (https://arxiv.org/abs/2010.05700).

https://arxiv.org/pdf/2302.09170.pdf | KILM Knowledge Injection into Encoder-Decoder Language Models - Arxiv-2302.09170

https://arxiv.org/pdf/2105.10123.pdf | Backdoor Attacks on Self-Supervised Learning - Arxiv-2105.10123
https://github.com/uclanlp/awesome-fairness-papers | uclanlp/awesome-fairness-papers: Papers on fairness in NLP
https://aclanthology.org/2021.acl-long.330/ | Societal Biases in Language Generation: Progress and Challenges - ACL Anthology
https://scholar.google.com/scholar?hl=en&as_sdt=0%2C22&as_ylo=2022&as_vis=1&q=continual+learning+experience+replay+instruction&btnG= | continual learning experience replay instruction - Google Scholar
https://arxiv.org/pdf/2301.12314.pdf | Progressive Prompts Continual Learning for Language Models - Arxiv-2301.12314
https://arxiv.org/pdf/2211.12701.pdf | Continual Learning of Natural Language Processing Tasks A Survey - Arxiv-2211.12701
https://arxiv.org/pdf/2205.02014.pdf | On Continual Model Refinement in Out-of-Distribution Data Streams - Arxiv-2205.02014
https://arxiv.org/pdf/2010.05595.pdf | Rethinking Experience Replay: a Bag of Tricks for Continual Learning | PDF

https://drive.google.com/drive/folders/0B4E10azXECctWnl4NG11bmhiOTQ?resourcekey=0-SBKd66q-dDlNrQuEjQV_lg | compound - Google ‰∫ëÁ´ØÁ°¨Áõò
https://twitter.com/_jasonwei/status/1516445376835776518 | (1) Jason Wei on Twitter: "In the spirit of the PhD admissions season ending, I'm making my state of purpose public. I learned a lot from reading @nelsonfliu and @ssgrn's SoPs, and so I'd like to pay it forward. https://t.co/awYteBgHow https://t.co/MhJQc5nBnP Prior SoPs below:" / Twitter
https://boards.greenhouse.io/thealleninstitute/jobs/2171573 | Job Application for Predoctoral Young Investigator at The Allen Institute for AI
https://boards.greenhouse.io/thealleninstitute/jobs/2171612 | Job Application for Research Internship at The Allen Institute for AI
https://boards.greenhouse.io/thealleninstitute/jobs/2137393 | Job Application for Research Scientist at The Allen Institute for AI
https://boards.greenhouse.io/thealleninstitute/jobs/2171617 | Job Application for Young Investigator at The Allen Institute for AI
https://boards.greenhouse.io/thealleninstitute/jobs/2171324 | Job Application for Research Internship at The Allen Institute for AI
https://boards.greenhouse.io/thealleninstitute/jobs/2167055 | Job Application for Research Scientist at The Allen Institute for AI
https://boards.greenhouse.io/thealleninstitute/jobs/2170237 | Job Application for Young Investigator at The Allen Institute for AI
https://boards.greenhouse.io/thealleninstitute/jobs/2171532 | Job Application for Research Internship at The Allen Institute for AI
https://www.zhihu.com/people/xuan-jiu-ye/posts?page=2 | (42 Â∞ÅÁßÅ‰ø° / 80 Êù°Ê∂àÊÅØ) ÁéÑÁéñÁà∑ - Áü•‰πé
https://pan.baidu.com/s/1qWH1uYK#list/path=%2F | ÁéãÊô¥Â∑ù-ÊÉäÈπ§ÊΩúÈæôËÆ∞.txt_ÂÖçË¥πÈ´òÈÄü‰∏ãËΩΩ|ÁôæÂ∫¶ÁΩëÁõò-ÂàÜ‰∫´Êó†ÈôêÂà∂
https://search.bilibili.com/all?keyword=flowers%20for%20algernon | flowers for algernon-ÂìîÂì©ÂìîÂì©_Bilibili
https://wx.tianyabooks.com/book/xdwx1/ | Áü≥Ê¶¥ËÆ∞Â∞èËØ¥Âú®Á∫øÈòÖËØª - Â∞èÊ§¥ - Ê≠¶‰æ†Â∞èËØ¥ÁΩë
https://www.kanunu8.com/book3/6346/index.html | ÂÇ≤ÂêõÂàÄ - È©¨Ëà∏ - Â∞èËØ¥Âú®Á∫øÈòÖËØª - Âä™Âä™‰π¶Âùä
https://www.kanunu8.com/book3/6344/index.html | ÂπªÁúüÁºò - È©¨Ëà∏ - Â∞èËØ¥Âú®Á∫øÈòÖËØª - Âä™Âä™‰π¶Âùä
https://www.kanunu8.com/book/4486/ | Ê≠§Èó¥ÁöÑÂ∞ëÂπ¥ - Ê±üÂçó - Â∞èËØ¥Âú®Á∫øÈòÖËØª - Âä™Âä™‰π¶Âùä
https://book.douban.com/subject/1824256/ | Á†¥ÈòµÂ≠ê¬∑ÈæôÂêü (Ë±ÜÁì£)
https://book.douban.com/subject/2342840/ | ËãèÊó∑‰º†Â•á (Ë±ÜÁì£)

https://arxiv.org/pdf/2105.03659.pdf | Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text - Arxiv-2105.03659
https://arxiv.org/pdf/2212.08410.pdf | Teaching Small Language Models to Reason - Arxiv-2212.08410
https://arxiv.org/pdf/2203.05115.pdf | Internet-augmented language models through few-shot prompting for open-domain question answering - Arxiv-2203.05115
https://arxiv.org/pdf/2205.10770.pdf | Memorization Without Overfitting Analyzing the Training Dynamics of Large Language Models - Arxiv-2205.10770
https://arxiv.org/pdf/2302.04931.pdf | In-Context Learning with Many Demonstration Examples - Arxiv-2302.04931
https://arxiv.org/pdf/2210.06726.pdf | Explanations from Large Language Models Make Small Reasoners Better - Arxiv-2210.06726
https://aclanthology.org/2022.emnlp-main.174.pdf | Iteratively Prompt Pre-trained Language Models for Chain of Thought - ACL-EMNLP-2022_2022.emnlp-main.174
https://arxiv.org/pdf/2212.00193.pdf | Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions - Arxiv-2212.00193
https://dreambooth.github.io/ | DreamBooth
https://arxiv.org/pdf/2208.12242.pdf | DreamBooth Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation - Arxiv-2208.12242
https://huggingface.co/spaces | Spaces - Hugging Face
https://huggingface.co/new-space | Hugging Face ‚Äì The AI community building the future.
https://gas.graviti.com/dataset/graviti/RAF_DB | Graviti Open Datasets/RAF-DB | Graviti
http://www.whdeng.cn/raf/li_RAFDB_2017_CVPR.pdf | www.whdeng.cn/raf/li_RAFDB_2017_CVPR.pdf
https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720639.pdf | 136720639.pdf
https://github.com/toharl/soft/blob/main/rafdb_noisy/inject0.3noise_asym_cmat.txt | soft/inject0.3noise_asym_cmat.txt at main ¬∑ toharl/soft

https://arxiv.org/pdf/2104.14690.pdf | Entailment as Few-Shot Learner - Arxiv-2104.14690
https://arxiv.org/pdf/2106.13353.pdf | Cutting Down on Prompts and Parameters Simple Few-Shot Learning with Language Models - Arxiv-2106.13353
https://arxiv.org/pdf/2111.08284.pdf | Few-Shot Self-Rationalization with Natural Language Prompts - Arxiv-2111.08284
https://arxiv.org/pdf/2205.14704.pdf | Decoupling Knowledge from Memorization Retrieval-augmented Prompt Learning - Arxiv-2205.14704
https://arxiv.org/abs/2101.06804 | What Makes Good In-Context Examples for GPT-$3$? | Abstract
https://arxiv.org/abs/2209.01975 | Selective Annotation Makes Language Models Better Few-Shot Learners - Arxiv-2209.01975
https://arxiv.org/abs/2107.08251 | Generative Pretraining for Paraphrase Evaluation - Arxiv-2107.08251

https://www.google.com/search?q=Mc-.%20Candlish%20et%20al.%20(2018)%20study%20the%20impact%20of%20gradient%20variance%20on%20scaling%20efficiency.%20By%20averaging%20the%20relative%20gradient%20variance%20over%20the%C2%A0. | Mc-. Candlish et al. (2018) study the impact of gradient variance on scaling efficiency. By averaging the relative gradient variance over the¬†. - Google Search
https://www.google.com/search?q=An+empirical+model+of+large-batch+training&sourceid=chrome&ie=UTF-8 | An empirical model of large-batch training - Google Search
https://boazbk.github.io/mltheoryseminar/ | CS229br Foundations of Deep Learning (aka Topics in the Foundations of Machine Learning)
https://arxiv.org/pdf/1905.11604.pdf | SGD on Neural Networks Learns Functions of Increasing Complexity - Arxiv-1905.11604
https://app.perusall.com/courses/compsci-229br-topics-in-the-foundations-of-machine-learning/_/dashboard/assignments/kBcxBctLood6fZn95 | COMPSCI 229BR: Topics in the Foundations of Machine Learning - Perusall
https://app.perusall.com/courses/compsci-229br-topics-in-the-foundations-of-machine-learning/chinchilla?assignmentId=yDMhjDMMtTK4fgMTC&part=1 | chinchilla - COMPSCI 229BR: Topics in the Foundations of Machine Learning - Perusall
https://www.google.com/search?q=nakkiran+kaplun+kalimeris&oq=nakkiran+kaplun+kalimeris&aqs=chrome..69i57j33i160l3.4321j0j1&sourceid=chrome&ie=UTF-8 | nakkiran kaplun kalimeris - Google Search
https://twitter.com/rasbt/status/1621522841417170946?lang=en | Sebastian Raschka on Twitter: "After putting together a lecture on multi-GPU training paradigms, I thought it might be a good idea to catch up with the recent ‚ÄúCramming: Training a Language Model on a Single GPU in One Day‚Äù paper (https://t.co/sv3VMPEDAd). An interesting read with lots of insights! 1/8" / Twitter
https://twitter.com/memdotai/status/1608715108972109824 | Mem on Twitter: "@Ilxcondottiero @LChoshen @jonasgeiping @tomgoldsteincs Saved! Here's the compiled thread: https://t.co/jP4nSN3iHp ü™Ñ AI-generated summary: "This paper explores various ways to make training more efficient when limited to one GPU and one day. The authors suggest that scaling up is helpful, but scaling down is also..." / Twitter
https://mobile.twitter.com/mindkosh/status/1608756646528053250 | Twitter
https://medium.com/geekculture/paper-dive-cramming-training-a-language-model-on-a-single-gpu-in-one-day-7965f47f7e8d | Paper Dive: ‚ÄúCramming: Training a Language Model on a Single GPU in One Day‚Äù | by Tudor Surdoiu | Geek Culture | Medium
https://www.google.com/search?q=deep+boostrap+ICLR+2021&newwindow=1&sxsrf=AJOqlzV19jinmo_PEYBwX6BR877Yw9lXtQ%3A1678400489969&ei=6VsKZInbOtahptQP9--tsAs&ved=0ahUKEwjJ8_qH8c_9AhXWkIkEHfd3C7YQ4dUDCBA&uact=5&oq=deep+boostrap+ICLR+2021&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIHCCEQoAEQCjIHCCEQoAEQCjIHCCEQoAEQCjoECCMQJzoFCC4QkQI6EQguEIMBEMcBELEDENEDEIAEOggIABCxAxCDAToOCC4QgAQQsQMQxwEQ0QM6CwgAEIAEELEDEIMBOgUIABCRAjoFCAAQgAQ6CAguELEDEIMBOgUILhCABDoICC4QgAQQsQM6CwguEIAEELEDEIMBOggIABCABBCxAzoLCC4QgAQQxwEQrwE6DgguEIAEELEDEIMBENQCOgcIABCABBAKOgcILhCABBAKOggIABCABBDLAToGCAAQFhAeOggIABAWEB4QDzoHCAAQDRCABDoGCAAQHhANOggIABAFEB4QDToKCAAQBRAeEA0QCjoICAAQCBAeEA06CggAEAgQHhANEA86BQgAEIYDOgcIIRCrAhAKOgUIIRCrAjoICCEQFhAeEB1KBAhBGAFQ3hRYjjRggzVoA3AAeACAAa8CiAHpEZIBCDIxLjIuMC4xmAEAoAEBwAEB&sclient=gws-wiz-serp | deep boostrap ICLR 2021 - Google Search
https://openreview.net/forum?id=guetrIHLFGI | The Deep Bootstrap Framework Good Online Learners are Good Offline Generalizers - OR-ICLR-2021_guetrIHLFGI
https://openreview.net/pdf?id=guetrIHLFGI | The Deep Bootstrap Framework Good Online Learners are Good Offline Generalizers - OR-ICLR-2021_guetrIHLFGI
https://arxiv.org/pdf/2106.09685.pdf | LoRA Low-Rank Adaptation of Large Language Models - Arxiv-2106.09685
https://the-decoder.com/metas-llama-language-model-shows-that-parameters-are-not-everything/ | Metas "LLaMA" language model shows that parameters are not everything
https://twitter.com/rasbt/status/1629496764808953857 | https://twitter.com/rasbt/status/1629496764808953857
https://twitter.com/yoavgo/status/1629221991797465088 | https://twitter.com/yoavgo/status/1629221991797465088
https://www.google.com/search?q=LLaMA+ppaer&oq=LLaMA+ppaer&aqs=chrome..69i57j0i3i13j0i13i512l5j0i13i30l3.1864j0j1&sourceid=chrome&ie=UTF-8 | LLaMA ppaer - Google Search
https://arxiv.org/pdf/2302.13971.pdf | LLaMA Open and Efficient Foundation Language Models - Arxiv-2302.13971
https://twitter.com/neuro_kim/status/1633866866463592455 | https://twitter.com/neuro_kim/status/1633866866463592455
https://docs.google.com/presentation/d/1evX4saCZ7AA3QWqEJX5G1YOHiKAaZ-4UW3TK6K7dO4k/edit?resourcekey=0-Miu5vScd8wKTuZ7_OcfeZA#slide=id.g138b2e57ee4_0_1420 | RL tutorial [cosyne] - Google ÂπªÁÅØÁâá
https://colab.research.google.com/drive/1jEiDNA1q98n1Wrw_uBEFvpuV9BGW_yxW | CosyneTutorial_RL_2023_Stachenfeld.ipynb - Colaboratory

https://arxiv.org/pdf/2205.10625.pdf | Least-to-Most Prompting Enables Complex Reasoning in Large Language Models - Arxiv-2205.10625
https://arxiv.org/pdf/2203.11171.pdf | Self-Consistency Improves Chain of Thought Reasoning in Language Models - OR-ICLR-2023_1PL1NIMMrw
https://www.google.com/search?q=1.+Differentiable+prompt+makes+pre-trained+488+language+models+better+few-shot+learners&sourceid=chrome&ie=UTF-8 | 1. Differentiable prompt makes pre-trained 488 language models better few-shot learners - Google Search
https://arxiv.org/abs/2108.13161 | Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners - Arxiv-2108.13161
https://www.google.com/search?q=the+capacity+for+moral+self-correction&oq=the+capacity+for+moral+self-correction&aqs=chrome..69i57j69i64l2.8347j0j1&sourceid=chrome&ie=UTF-8 | the capacity for moral self-correction - Google Search
https://arxiv.org/abs/2302.07459 | The Capacity for Moral Self-Correction in Large Language Models - Arxiv-2302.07459
https://www.google.com/search?q=christopher+potts&oq=christopher+potts&aqs=chrome.0.0i355i512j46i512j0i512l2j46i175i199i512j0i512l2j46i175i199i512j0i512l2.2615j0j1&sourceid=chrome&ie=UTF-8 | christopher potts - Google Search
https://web.stanford.edu/~cgpotts/ | Christopher Potts
https://www.google.com/search?q=diff+pruning&oq=diff+pruning&aqs=chrome.0.0i512j0i22i30l9.1225j0j1&sourceid=chrome&ie=UTF-8 | diff pruning - Google Search
https://yaofu.notion.site/GPT-3-5-360081d91ec245f29029d37b54573756 | ÊãÜËß£ËøΩÊ∫Ø GPT-3.5 ÂêÑÈ°πËÉΩÂäõÁöÑËµ∑Ê∫ê
https://hub.baai.ac.cn/view/23787 | ChatGPTÈÄö‰øóÁ¨îËÆ∞Ôºö‰ªéGPT-N„ÄÅRL‰πãPPOÁÆóÊ≥ïÂà∞InstructGPT„ÄÅChatGPT - Êô∫Ê∫êÁ§æÂå∫
https://blog.csdn.net/weixin_42370153/article/details/128678051 | instructGPTÁöÑÂâç‰∏§Èò∂ÊÆµÊ†∏ÂøÉËÆ≠ÁªÉËøáÁ®ãpytorchËØ¶ÁªÜ‰ª£Á†ÅÂ±ïÁ§∫_ÂÄ™‰∏çËÇâÁöÑÂçöÂÆ¢-CSDNÂçöÂÆ¢
https://www.google.com/search?q=ruiqi+zhong&oq=ruiqi+zhong&aqs=chrome..69i57j0i512j0i22i30j0i390.2009j0j1&sourceid=chrome&ie=UTF-8 | ruiqi zhong - Google Search
https://ruiqi-zhong.github.io/ | Ruiqi Zhong
https://arxiv.org/abs/2210.10960 | Diffusion Models already have a Semantic Latent Space - Arxiv-2210.10960
https://openreview.net/forum?id=PS3IMnScugk | Learning to Recombine and Resample Data For Compositional Generalization | OpenReview
https://zhuanlan.zhihu.com/p/594107879 | 2022Âπ¥Â∫¶Â∞èÁªìÔºöÁßëÁ†î„ÄÅChatGPT‰∏éÁñ´ÊÉÖ - Áü•‰πé
https://promptperfect.jina.ai/ | PromptPerfect - Elevate your prompts to perfection
https://www.google.com/search?q=ameet+deshpande&oq=ameet+deshpande&aqs=chrome..69i57j0i22i30l9.2366j0j1&sourceid=chrome&ie=UTF-8 | ameet deshpande - Google Search
https://ameet-1997.github.io/ | Ameet Deshpande
https://www.google.com/search?q=mina+lee&oq=mina+lee&aqs=chrome..69i57j35i39l2j46i67l2j0i131i433i512l2j46i199i433i465i512j46i199i465i512j46i512.829j0j1&sourceid=chrome&ie=UTF-8 | mina lee - Google Search
https://minalee.info/ | Mina Lee ‚Äì Ph.D. Candidate at Stanford
https://www.google.com/search?q=huaxiu+yao&oq=huaxiu+yao&aqs=chrome..69i57j0i512.1537j0j1&sourceid=chrome&ie=UTF-8 | huaxiu yao - Google Search
https://huaxiuyao.mystrikingly.com/ | Huaxiu Yao's Personal Website on Strikingly
https://twitter.com/search?q=academic%20job%20market&src=typed_query&f=user | academic job market - Twitter Search / Twitter
https://www.google.com/search?q=prithviraj+ammanabrolu&oq=prithviraj+ammanabrolu&aqs=chrome..69i57.5505j0j1&sourceid=chrome&ie=UTF-8 | prithviraj ammanabrolu - Google Search
https://www.google.com/search?q=lei+li&newwindow=1&sxsrf=AJOqlzVnfMJxNYEpGTgbRkFziL9-Yq6Z_g%3A1678395767359&ei=d0kKZLzMFemhptQPiJqHoAE&ved=0ahUKEwj844W838_9AhXpkIkEHQjNARQQ4dUDCBA&uact=5&oq=lei+li&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIECCMQJzILCC4QgAQQsQMQgwEyBAgAEEMyBQgAEIAEMgUILhCABDIFCAAQgAQyBAgAEEMyBQguEIAEMgUIABCABDIFCAAQgAQ6CggAEEcQ1gQQsAM6BwgAELADEENKBAhBGABQhgRYhgRgkAZoAXABeACAAWqIAWqSAQMwLjGYAQCgAQHIAQnAAQE&sclient=gws-wiz-serp | lei li - Google Search
https://sites.cs.ucsb.edu/~lilei/student.html | Lei LI's Software
https://sites.cs.ucsb.edu/~william/ | William Wang, UC Santa Barbara Computer Science
https://sites.cs.ucsb.edu/~yuxiangw/ | Yu-Xiang WANG's Homepage
https://code-terminator.github.io/ | Shiyu Chang | UC Santa Barbara

https://www.csail.mit.edu/events | Events | MIT CSAIL

https://proceedings.neurips.cc/paper/2001/file/7b7a53e239400a13bd6be6c91c4f6c4e-Paper.pdf | On Discriminative vs. Generative Classifiers A comparison of logistic regression and naive Bayes - NeurIPS-2001_7b7a53e2
https://arxiv.org/pdf/2302.13007.pdf | ChatAug Leveraging ChatGPT for Text Data Augmentation - Arxiv-2302.13007
https://www.bilibili.com/video/BV1T8411T7yw/?spm_id_from=333.337.search-card.all.click&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | ‰∏∫‰ªÄ‰πàÊàë‰ª¨Ê¥ªÂæóËøô‰πàÁ¥ØÔºü‰∏Ä‰∏™ËßÜÈ¢ëÔºåÁúãÊáÇÂºÇÂåñÁêÜËÆ∫ÂçÉÂπ¥ÊµÅÂèò_ÂìîÂì©ÂìîÂì©_bilibili
https://arxiv.org/pdf/2303.01469.pdf | Consistency Models - Arxiv-2303.01469
https://arxiv.org/pdf/2303.01580.pdf | Mixture of Soft Prompts for Controllable Data Generation - Arxiv-2303.01580

https://n.derek.ma/2_literature/RLHF | RLHF - Derek Ma
https://arxiv.org/pdf/2204.05239.pdf | Exploring the Universal Vulnerability of Prompt-based Learning Paradigm | PDF
https://arxiv.org/pdf/2012.01274.pdf | How Robust are Randomized Smoothing based Defenses to Data Poisoning? | PDF
https://github.com/akshaymehra24/PoisoningCertifiedDefenses | akshaymehra24/PoisoningCertifiedDefenses: How Robust are Randomized Smoothing based Defenses to Data Poisoning? (CVPR 2021)
https://github.com/kohpangwei/data-poisoning-release/blob/master/generate_or_process_bounds.py | data-poisoning-release/generate_or_process_bounds.py at master ¬∑ kohpangwei/data-poisoning-release
https://aclanthology.org/2021.findings-emnlp.369.pdf | https://aclanthology.org/2021.findings-emnlp.369.pdf
https://github.com/neulab/RIPPLe/issues/6 | Run Error ¬∑ Issue #6 ¬∑ neulab/RIPPLe
https://github.com/neulab/RIPPLe/blob/paul_refactor/manifestos/example_manifesto.yaml | RIPPLe/example_manifesto.yaml at paul_refactor ¬∑ neulab/RIPPLe
https://proceedings.mlr.press/v120/zhang20b.html | Online Data Poisoning Attacks
https://openreview.net/pdf?id=v6UimxiiR78 | https://openreview.net/pdf?id=v6UimxiiR78
https://github.com/lightly-ai/lightly | lightly-ai/lightly: A python library for self-supervised learning on images.
https://arxiv.org/pdf/2004.07401.pdf | Poisoning Attacks on Algorithmic Fairness - Arxiv-2004.07401
https://arxiv.org/pdf/2105.12837.pdf | Fooling Partial Dependence via Data Poisoning | PDF
https://openreview.net/forum?id=rYLMJ6zX3RF | [Re] Exacerbating Algorithmic Bias through Fairness Attacks | OpenReview
https://openreview.net/pdf?id=rYLMJ6zX3RF | https://openreview.net/pdf?id=rYLMJ6zX3RF
https://github.com/toliz/fairness-attacks | toliz/fairness-attacks: Re-implementation of the paper "Exacerbating Algorithmic Bias through Fairness Attacks"
https://www.google.com/search?q=Adversarial+example+generation+with+syntactically+controlled+paraphrase+networks&sourceid=chrome&ie=UTF-8 | Adversarial example generation with syntactically controlled paraphrase networks - Google Search
https://arxiv.org/pdf/2011.10369.pdf | ONION: A Simple and Effective Defense Against Textual Backdoor Attacks | PDF
https://github.com/leileigan/clean_label_textual_backdoor_attack | leileigan/clean_label_textual_backdoor_attack
https://github.com/leileigan/clean_label_textual_backdoor_attack/blob/master/scripts/run_bert_sst_samples_gen.sh | clean_label_textual_backdoor_attack/run_bert_sst_samples_gen.sh at master ¬∑ leileigan/clean_label_textual_backdoor_attack
https://github.com/leileigan/clean_label_textual_backdoor_attack/blob/master/attack/poison_examples_gen.py | clean_label_textual_backdoor_attack/poison_examples_gen.py at master ¬∑ leileigan/clean_label_textual_backdoor_attack
https://github.com/leileigan/clean_label_textual_backdoor_attack/blob/56e3a96f6a4eeaf30b90a275685f37cc7e7b3c7c/attack/poison_examples_gen.py#L275 | clean_label_textual_backdoor_attack/poison_examples_gen.py at 56e3a96f6a4eeaf30b90a275685f37cc7e7b3c7c ¬∑ leileigan/clean_label_textual_backdoor_attack
https://github.com/leileigan/clean_label_textual_backdoor_attack/blob/56e3a96f6a4eeaf30b90a275685f37cc7e7b3c7c/OpenAttack/attackers/genetic.py#L96 | clean_label_textual_backdoor_attack/genetic.py at 56e3a96f6a4eeaf30b90a275685f37cc7e7b3c7c ¬∑ leileigan/clean_label_textual_backdoor_attack
https://openreview.net/pdf?id=H4lzChGmhCK | https://openreview.net/pdf?id=H4lzChGmhCK
https://openreview.net/forum?id=H4lzChGmhCK | On the reproducibility of "Exacerbating Algorithmic Bias through Fairness Attacks" | OpenReview
https://www.google.com/search?q=On+the+reproducibility+of+%22Exacerbating+Algorithmic+Bias+through+Fairness+Attacks%22&sourceid=chrome&ie=UTF-8 | On the reproducibility of "Exacerbating Algorithmic Bias through Fairness Attacks" - Google Search
https://arxiv.org/pdf/2012.08723.pdf | Exacerbating Algorithmic Bias through Fairness Attacks | PDF
https://github.com/kohpangwei/influence-release | kohpangwei/influence-release
https://aclanthology.org/2020.acl-main.249.pdf | https://aclanthology.org/2020.acl-main.249.pdf
https://github.com/wronnyhuang/metapoison/blob/master/main.py | metapoison/main.py at master ¬∑ wronnyhuang/metapoison
https://github.com/wronnyhuang/metapoison/blob/master/meta.py | metapoison/meta.py at master ¬∑ wronnyhuang/metapoison
https://arxiv.org/pdf/2106.01494.pdf | Knowing More About Questions Can Help: Improving Calibration in Question Answering | PDF
https://arxiv.org/pdf/2004.00225.pdf | MetaPoison: Practical General-purpose Clean-label Data Poisoning | PDF
https://arxiv.org/pdf/2005.00191.pdf | Bullseye Polytope: A Scalable Clean-Label Poisoning Attack with Improved Transferability | PDF
https://openreview.net/pdf?id=SJeYe0NtvH | https://openreview.net/pdf?id=SJeYe0NtvH
https://arxiv.org/pdf/2202.11203.pdf | Label-Smoothed Backdoor Attack | PDF
https://arxiv.org/pdf/2103.15543.pdf | Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models | PDF
https://github.com/wronnyhuang/metapoison#algorithm-overview | wronnyhuang/metapoison: Craft poisoned data using MetaPoison
https://aclanthology.org/2020.conll-1.48.pdf | https://aclanthology.org/2020.conll-1.48.pdf
https://arxiv.org/abs/2212.10717 | [2212.10717] Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks
https://arxiv.org/abs/1903.09860 | [1903.09860] Data Poisoning against Differentially-Private Learners: Attacks and Defenses
https://worksheets.codalab.org/worksheets/0xbdd35bdd83b14f6287b24c9418983617/ | CodaLab Worksheets
https://arxiv.org/pdf/1706.03691.pdf | Certified Defenses for Data Poisoning Attacks | PDF
https://www.google.com/search?q=certified+defense+for+data+poisioning+attacks&newwindow=1&sxsrf=ALiCzsZAvWLMkcNthxxT8acwMJQiWs7QoQ%3A1672965002579&ei=imu3Y77pIvuk5NoPkPWWuAo&ved=0ahUKEwi-1syl2LH8AhV7ElkFHZC6BacQ4dUDCBA&uact=5&oq=certified+defense+for+data+poisioning+attacks&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIGCAAQFhAeMgUIABCGAzIFCAAQhgMyBQgAEIYDMgUIABCGAzoKCAAQRxDWBBCwAzoECCMQJzoFCAAQgAQ6BQgAEKIEOgcIABAeEKIEOgUIIRCgAToFCCEQqwI6CAghEBYQHhAdOgoIIRAWEB4QDxAdOgcIIRCgARAKSgQIQRgASgQIRhgAUIADWNIxYJ0yaAFwAXgBgAGaAYgB1h2SAQUxMS4yNJgBAKABAcgBCMABAQ&sclient=gws-wiz-serp | certified defense for data poisioning attacks - Google Search
https://arxiv.org/abs/1706.03691 | [1706.03691] Certified Defenses for Data Poisoning Attacks
https://arxiv.org/pdf/1505.05424.pdf | https://arxiv.org/pdf/1505.05424.pdf
https://arxiv.org/pdf/1708.06733.pdf | https://arxiv.org/pdf/1708.06733.pdf
https://arxiv.org/pdf/1912.02292.pdf | https://arxiv.org/pdf/1912.02292.pdf
https://arxiv.org/abs/2007.08432 | [2007.08432] Data Poisoning Attacks Against Federated Learning Systems
https://github.com/Cinofix/beta_poisoning | Cinofix/beta_poisoning: Official implementation of 'The Hammer and the Nut: Is Bilevel Optimization Really Needed to Poison Linear Classifiers?' [Submitted to IJCNN 2021]
https://arxiv.org/pdf/2206.12654.pdf | https://arxiv.org/pdf/2206.12654.pdf
https://arxiv.org/pdf/2110.07831.pdf | https://arxiv.org/pdf/2110.07831.pdf
https://github.com/THUYimingLi/backdoor-learning-resources | THUYimingLi/backdoor-learning-resources: A list of backdoor learning resources
https://github.com/THUYimingLi/BackdoorBox | THUYimingLi/BackdoorBox: The open-sourced Python toolbox for backdoor attacks and defenses.
https://arxiv.org/pdf/2201.02993.pdf | https://arxiv.org/pdf/2201.02993.pdf
https://arxiv.org/pdf/2206.01832.pdf | https://arxiv.org/pdf/2206.01832.pdf
https://arxiv.org/pdf/2202.05749.pdf | https://arxiv.org/pdf/2202.05749.pdf
https://arxiv.org/abs/2111.14309 | [2111.14309] A General Framework for Defending Against Backdoor Attacks via Influence Graph
https://web.cs.ucdavis.edu/~hpirsiav/papers/backdoor_ssl_cvpr22.pdf | https://web.cs.ucdavis.edu/~hpirsiav/papers/backdoor_ssl_cvpr22.pdf
https://arxiv.org/pdf/1912.02771/ | https://arxiv.org/pdf/1912.02771/
https://github.com/thunlp/OpenBackdoor | thunlp/OpenBackdoor: An open-source toolkit for textual backdoor attack and defense (NeurIPS 2022 D&B, Spotlight)
https://arxiv.org/abs/1911.07116 | https://arxiv.org/abs/1911.07116
https://arxiv.org/pdf/2111.08429.pdf | https://arxiv.org/pdf/2111.08429.pdf
https://arxiv.org/pdf/2111.14309.pdf | https://arxiv.org/pdf/2111.14309.pdf
https://arxiv.org/pdf/2010.08138.pdf | https://arxiv.org/pdf/2010.08138.pdf
https://arxiv.org/abs/1708.03999 | https://arxiv.org/abs/1708.03999
https://arxiv.org/pdf/2201.10055.pdf | https://arxiv.org/pdf/2201.10055.pdf
https://aclanthology.org/2022.acl-long.386/ | https://aclanthology.org/2022.acl-long.386/
https://arxiv.org/pdf/2104.09667.pdf | https://arxiv.org/pdf/2104.09667.pdf
https://www.google.com/search?q=model+extraction+attacks&newwindow=1&sxsrf=APq-WBu4aWF2mc_RjTaQeUsTFpfXgX59aQ%3A1646786838384&ei=FvknYtWDF6CP9PwPw_qXwAw&ved=0ahUKEwjV5paE57f2AhWgB50JHUP9BcgQ4dUDCA8&uact=5&oq=model+extraction+attacks&gs_lcp=Cgdnd3Mtd2l6EAMyBQgAEIAEMgUIABCABDIFCAAQgAQyBggAEBYQHjoHCAAQRxCwA0oECEEYAEoECEYYAFDmAljZGmCcG2gCcAF4AIAB7AKIAbQTkgEHMC4zLjcuMZgBAKABAcgBBcABAQ&sclient=gws-wiz | model extraction attacks - Google Search
https://github.com/Lorraine333/smoothed_box_embedding | Lorraine333/smoothed_box_embedding: smoothed box embedding code
https://github.com/yasumasaonoe/Box4Types/blob/main/box4et/README.md | Box4Types/README.md at main ¬∑ yasumasaonoe/Box4Types

https://www.bilibili.com/video/BV1h54y1u74A/?spm_id_from=333.1007.tianma.1-3-3.click&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | ËøôÈÉ®Âêç‰Ωú‰∏ç‰ªÖÈ™óËøáÂÖ®ÈÉ®ËßÇ‰ºóÔºÅËøòËÆ©‰æ¶Êé¢Ê≤¶‰∏∫ÁäØ‰∫∫ÁöÑÁé©Áâ©ÔºüÔºÅÂÖ∂ËÆ≤Ëø∞‰Ωï‰∏∫ÁúüÊ≠£ÁöÑÈ™óÂ±ÄÔºÅ_ÂìîÂì©ÂìîÂì©_bilibili
https://www.bilibili.com/video/BV1JM4y1Z7Bt/?spm_id_from=333.1007.tianma.2-2-5.click&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | ÊàëÊÅ®„ÄäÊ∑∑Ê≤åÊ≠¶Â£´„ÄãÔºåÂÆÉÂâ•Â§∫‰∫ÜÊàëÂøçËÄêÂπ≥Â∫∏Âä®ÁîªÁöÑËÉΩÂäõ„ÄêÈì∂Â±èÁ≥ª„Äë‰∏®Êú∫Ê†∏_ÂìîÂì©ÂìîÂì©_bilibili

https://book.douban.com/subject/3121820/ | ÊÆâÊïô„Ç´„ÉÜ„É™„ÉäËªäËº™ (Ë±ÜÁì£)
https://book.douban.com/subject/5986239/ | Á†ÇÊº†„ÅÆËñîËñá (Ë±ÜÁì£)
https://book.douban.com/subject/3077658/comments/ | „Éü„Çπ„ÉÜ„É™„Éª„Ç™„Éö„É©‚ÄïÂÆøÂëΩÂüéÊÆ∫‰∫∫‰∫ã‰ª∂ Áü≠ËØÑ
https://lockedroom.net/blog/?p=1632 | Â±±Áî∞Ê≠£Á¥Ä„Äé„Éü„Çπ„ÉÜ„É™„Éª„Ç™„Éö„É©‚ÄïÂÆøÂëΩÂüéÊÆ∫‰∫∫‰∫ã‰ª∂„Äè(2001) | Fang's Mystery Blog

https://arxiv.org/pdf/2212.10560.pdf | Self-Instruct Aligning Language Model with Self Generated Instructions - Arxiv-2212.10560
https://twitter.com/SeonghyeonYe/status/1580170684466360323 | (1) Seonghyeon Ye on Twitter: "*Flipping* the instruction and label space makes stronger zero-shot LM! üôÉ 0-shot üåü FLIPPED (3B, 11B) üåü outperforms 0-shot T0 (11B), 3-shot GPT3 (175B), 0-shot PaLM (540B) on BIG-bench. üí™¬†[1/9] w/@Doe_Young_Kim @jang_yoel Joongbo Shin @seo_minjoon üìú: https://t.co/IYF1FLtape https://t.co/LyOmxQlg6l" / Twitter
https://github.com/bigscience-workshop/promptsource | bigscience-workshop/promptsource: Toolkit for creating, sharing and using natural language prompts.
https://www.zhihu.com/search?q=beit&type=content | (42 Â∞ÅÁßÅ‰ø° / 80 Êù°Ê∂àÊÅØ) beit - ÊêúÁ¥¢ÁªìÊûú - Áü•‰πé
https://arxiv.org/pdf/2302.12173.pdf | More than you've asked for A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models - Arxiv-2302.12173
https://github.com/greshake/llm-security | greshake/llm-security: New ways of breaking app-integrated LLMs
https://people.csail.mit.edu/cloudygoose/ | Tianxing He
https://twitter.com/TianxingH | (1) Tianxing He (@TianxingH) / Twitter
https://tsvetshop.github.io/people.html | : People
https://homes.cs.washington.edu/~yuliats/ | Yulia Tsvetkov
https://www.google.com/search?q=goosehe+mit&oq=goosehe+mit&aqs=chrome..69i57j0i546l5.2850j0j1&sourceid=chrome&ie=UTF-8 | goosehe mit - Google Search
https://arxiv.org/pdf/2212.10020.pdf | On the Blind Spots of Model-Based Evaluation Metrics for Text Generation - Arxiv-2212.10020
https://question406.github.io/ | Jiabao Ji (ËÆ°ÂÆ∂ÂÆù)
https://code-terminator.github.io/ | Shiyu Chang | UC Santa Barbara
https://www.google.com/search?q=graham+neubig&oq=graham+neubig&aqs=chrome.0.0i355i512j46i512j0i512l7j0i390.3552j0j1&sourceid=chrome&ie=UTF-8 | graham neubig - Google Search
https://scholar.google.com/citations?hl=en&user=r21asW4AAAAJ&view_op=list_works&sortby=pubdate | ‚Ä™Shiyu Chang‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
https://code-terminator.github.io/index.html#students | Shiyu Chang | UC Santa Barbara
https://people.eecs.berkeley.edu/~klein/ | Dan Klein's Home Page
http://sameersingh.org/group.html | Sameer Singh: Group
https://www.cs.washington.edu/people/faculty/lsz/ | Luke Zettlemoyer | Paul G. Allen School of Computer Science & Engineering
https://arxiv.org/pdf/2110.04366.pdf | Towards a Unified View of Parameter-Efficient Transfer Learning - Arxiv-2110.04366
https://scholar.google.com/citations?hl=en&user=UjpbO6IAAAAJ&view_op=list_works&sortby=pubdate | ‚Ä™Luke Zettlemoyer‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
https://scholar.google.com/citations?user=euc0GX4AAAAJ&hl=en | ‚Ä™Karthik Narasimhan‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
http://rush-nlp.com/ | Main
https://yoonholee.com/ | Yoonho Lee
https://scholar.google.com/citations?hl=en&user=vfPE6hgAAAAJ&view_op=list_works&sortby=pubdate | ‚Ä™Chelsea Finn‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
https://irislab.stanford.edu//people.html | People
https://scholar.google.com/citations?hl=en&user=euc0GX4AAAAJ&view_op=list_works&sortby=pubdate | ‚Ä™Karthik Narasimhan‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
https://scholar.google.com/scholar?cites=17392009296900150762&as_sdt=40000005&sciodt=0,22&hl=en | Google Scholar
https://www.cs.utexas.edu/~eunsol/html_pages/group.html | Eunsol Choi
https://scholar.google.com/citations?hl=en&user=kV9XRxYAAAAJ&view_op=list_works&sortby=pubdate | ‚Ä™Samuel R. Bowman‚Ä¨ - ‚Ä™Google Scholar‚Ä¨

https://www.cs.princeton.edu/~karthikn/ | Karthik Narasimhan
https://arxiv.org/abs/2202.09318 | DataMUX Data Multiplexing for Neural Networks - Arxiv-2202.09318
https://people.csail.mit.edu/fisch/ | Adam Fisch
https://people.csail.mit.edu/fisch/assets/pdf/research.pdf | research.pdf
https://people.csail.mit.edu/tommi/ | Tommi Jaakkola
https://ysymyth.github.io/ | About ‚Äì Shunyu Yao ‚Äì ÂßöÈ°∫Èõ®
https://scholar.google.com/citations?user=qJBXk9cAAAAJ | ‚Ä™Shunyu Yao‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
https://twitter.com/ShunyuYao12 | https://twitter.com/ShunyuYao12
https://scholar.google.com/citations?hl=en&user=-hGZC54AAAAJ&view_op=list_works&sortby=pubdate | ‚Ä™Sameer Singh‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=euc0GX4AAAAJ&sortby=pubdate | ‚Ä™Karthik Narasimhan‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
https://mila.quebec/en/person/hugo-larochelle/ | Hugo Larochelle - Mila
https://scholar.google.ca/citations?hl=en&user=U89FHq4AAAAJ&view_op=list_works&sortby=pubdate | ‚Ä™Hugo Larochelle‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
https://scholar.google.com/citations?user=gnox0EsAAAAJ&hl=en | ‚Ä™Runzhe Yang‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
https://twitter.com/karthik_r_n | https://twitter.com/karthik_r_n
https://scholar.google.com/citations?user=LYRkQhMAAAAJ&hl=en | ‚Ä™Adam Fisch‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
https://www.google.com/search?q=Jeremy%20Wohlwend%20%20 | Jeremy Wohlwend - Google Search
https://twitter.com/adamjfisch?lang=en | https://twitter.com/adamjfisch?lang=en
https://robinjia.github.io/ | Robin Jia
https://www.google.com/search?q=usc+faculty&oq=usc+faul&aqs=chrome.1.69i57j0i10i512l9.3149j0j1&sourceid=chrome&ie=UTF-8 | usc faculty - Google Search

https://yoavartzi.com/ | Yoav Artzi
https://scholar.google.com/citations?user=XuQW7ogAAAAJ&hl=en | ‚Ä™Yoav Artzi‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
https://www.google.com/search?q=UW+cs+faulty&oq=UW&aqs=chrome.0.69i59j69i57j35i39j69i59j46i131i199i433i465i512j0i131i433i512j69i60l2.2167j0j1&sourceid=chrome&ie=UTF-8 | UW cs faulty - Google Search
https://scholar.google.com/citations?hl=en&user=UjpbO6IAAAAJ&view_op=list_works&sortby=pubdate | ‚Ä™Luke Zettlemoyer‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
https://nasmith.github.io/ | Noah A. Smith
https://noahs-ark.github.io/people/#phd | Researchers in Noah‚Äôs ARK - Noah‚Äôs ARK
https://noahs-ark.github.io/people/#postdoc | Researchers in Noah‚Äôs ARK - Noah‚Äôs ARK
https://maartensap.com/ | Maarten Sap - Home
https://scholar.google.com/citations?hl=en&user=gFN4QUYAAAAJ&view_op=list_works&sortby=pubdate | ‚Ä™Maarten Sap‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
https://swabhs.com/ | Swabha Swayamdipta
https://jflanigan.github.io/index.html | Jeffrey Flanigan
https://people.ischool.berkeley.edu/~dbamman/ | David Bamman
http://brenocon.com/ | Brendan T. O'Connor - UMass Amherst, Computer Science
https://people.cs.georgetown.edu/nschneid/ | Nathan Schneider
https://noahs-ark.github.io/people/#masters | Researchers in Noah‚Äôs ARK - Noah‚Äôs ARK
https://noahs-ark.github.io/people/#other | Researchers in Noah‚Äôs ARK - Noah‚Äôs ARK
https://personal.ntu.edu.sg/wangwy/ | Wenya WANG (NTU)
https://homes.cs.washington.edu/~hannaneh/ | Hannaneh Hajishirzi - University of Washington
https://noahs-ark.github.io/ | About Noah‚Äôs ARK - Noah‚Äôs ARK
https://ofir.io/ | Ofir Press
https://homes.cs.washington.edu/~rahuln/ | https://homes.cs.washington.edu/~rahuln/
https://alisawuffles.github.io/ | Alisa Liu
https://www.amazon.science/search | Search - Amazon Science
https://homes.cs.washington.edu/~jkasai/ | Jungo
https://yushi-hu.github.io/ | Yushi Hu
https://twitter.com/yanaiela | https://twitter.com/yanaiela
https://blog.allenai.org/from-interviewee-to-interviewer-68c36593b305 | From Interviewee To Interviewer - A New Type of Odyssey | by Yanai Elazar | AI2 Blog
https://ranjaykrishna.com/index.html | Ranjay Krishna - Home
https://sarahwie.github.io/ | Sarah Wiegreffe ‚Äì Personal website (in the works)
https://sneha-rk.github.io/ | Sneha Kudugunta's Website
https://dwadden.github.io/ | David Wadden
https://homes.cs.washington.edu/~mrsalehi/ | Reza Salehi
https://bhargaviparanjape.github.io/ | Bhargavi Paranjape ‚Äì PhD student at University of Washington
https://liujch1998.github.io/ | Jiacheng (Gary) Liu
https://homes.cs.washington.edu/~yizhongw/ | Yizhong Wang - University of Washington
http://ellenmellon.github.io/ | About Me | Ellen (Zeqiu) Wu
https://danielkhashabi.com/index.html#themes | Daniel Khashabi
https://www.clsp.jhu.edu/faculty/ | Faculty - Center for Language and Speech Processing
https://www.cis.upenn.edu/~ccb/ | Chris Callison-Burch

http://www.ikonstas.net/ | Yannis Konstas - Research
http://www.janmbuys.com/ | Jan Buys
https://yonatanbisk.com/ | Yonatan Bisk
https://rudinger.github.io/ | Rachel Rudinger
https://jmhessel.com/ | Jack Hessel's Homepage
https://www.cs.ubc.ca/~vshwartz/ | Vered Shwartz - Department of Computer Science - UBC
https://swabhs.com/ | Swabha Swayamdipta
https://danielkhashabi.com/ | Daniel Khashabi
https://yj-yu.github.io/home/ | Youngjae Yu. Personal Homepage
https://maartensap.com/ | Maarten Sap - Home
https://atcbosselut.github.io/ | Antoine Bosselut
https://hrashkin.github.io/ | https://hrashkin.github.io
https://people.cs.umass.edu/~xiangl/ | Xiang Lorraine Li
https://scholar.google.com/citations?hl=en&user=SRgRwSoAAAAJ | ‚Ä™Xiang Lorraine Li‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
http://prithvirajva.com/ | Prithviraj (Raj) Ammanabrolu
http://people.csail.mit.edu/tommi/ | Tommi Jaakkola
http://people.csail.mit.edu/tommi/people.html | Tommi Jaakkola
https://dmelis.github.io/ | David Alvarez-Melis | Home
https://scholar.google.com/citations?user=XsxZrYYAAAAJ&hl=en | ‚Ä™David Alvarez-Melis‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
https://thashim.github.io/ | Tatsunori Hashimoto | Home

https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/ | CSC2541 Winter 2022
https://arxiv.org/pdf/2205.05638.pdf | Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning - Arxiv-2205.05638
https://arxiv.org/pdf/2208.11857.pdf | Shortcut Learning of Large Language Models in Natural Language Understanding A Survey - Arxiv-2208.11857
https://piazza.com/mit/spring2023/67830/resources | 6.7830 | Class Profile | Piazza
https://piazza.com/class_profile/get_resource/ldjefz5a4yu69n/leogz9qsbffkp | leogz9qsbffkp
https://github.com/dangkhoasdc/awesome-ai-residency | dangkhoasdc/awesome-ai-residency: List of AI Residency Programs
http://www.cs.toronto.edu/~rgrosse/teaching.html | www.cs.toronto.edu/~rgrosse/teaching.html
https://subercui.github.io/csc2515/Lectures.html | CSC2515 Winter 2021- University of Toronto Computer Science
https://duvenaud.github.io/sta414/ | STA414
https://jsc370.github.io/ | JSC370 Data Science
https://probmlcourse.github.io/csc412/ | CSC412 Winter 2020: Probabilsitic Machine Learning
https://duvenaud.github.io/learn-discrete/ | Learning Discrete Latent Structure
http://www.cs.toronto.edu/~duvenaud/courses/csc2541/index.html | index
https://duvenaud.github.io/learning-to-search/ | Learning to Search
https://www.cs.toronto.edu/~cmaddis/courses/sta4273_w21/ | Minimizing Expectations
https://aclanthology.org/2022.acl-long.429.pdf | Can Explanations Be Useful for Calibrating Black Box Models? - ACL-ACL-2022_2022.acl-long.429
https://www.mitgenaisummit.com/s-projects-side-by-side | Agenda & Speakers | MIT GenAI Summit
https://web.mit.edu/webcast/mitgenaisummit/s23/ | LIVE WEBCAST | MIT Generative AI Summit Live Webcast

https://scholar.google.com/citations?user=QMkbFp8AAAAJ&hl=en | ‚Ä™Shibani Santurkar‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
https://arxiv.org/pdf/1811.02553.pdf | A Closer Look at Deep Policy Gradients - Arxiv-1811.02553
https://people.eecs.berkeley.edu/~klein/ | Dan Klein's Home Page
https://jsteinhardt.stat.berkeley.edu/ | Jacob Steinhardt
https://scholar.google.com/citations?hl=en&user=LKv32bgAAAAJ&view_op=list_works&sortby=pubdate | ‚Ä™Jacob Steinhardt‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
https://thashim.github.io/ | Tatsunori Hashimoto | Home
https://mlfoundations.org/#opportunities | Harvard ML Foundations
https://carat.fas.harvard.edu/applicant/apply?type=A&publicSite=N | CARAT
http://web.mit.edu/jda/www/teaching/6.884/ | Neuro-symbolic Models for NLP (6.884)
http://web.mit.edu/jda/www/teaching/6.864/ | web.mit.edu/jda/www/teaching/6.864/
https://web.stanford.edu/~jurafsky/ | Dan Jurafsky - Home Page
https://homes.cs.washington.edu/~yuliats/ | Yulia Tsvetkov
https://people.csail.mit.edu/cloudygoose/papers/defense.pdf | defense.pdf
http://tensorlab.cms.caltech.edu/users/anima/ | Anima AI + Science Lab
https://www.google.com/search?q=anima+anandkumar&oq=Anima+Anandkumar&aqs=chrome.0.0i131i355i433i512j46i131i433i512j0i67j0i512l4j69i60.239j0j1&sourceid=chrome&ie=UTF-8 | anima anandkumar - Google Search
http://tensorlab.cms.caltech.edu/users/anima/group.html | Anima AI + Science Lab
http://starai.cs.ucla.edu/members/ | UCLA StarAI Lab - Members
https://www.google.com/search?q=Guy%20Van%20den%20Broeck | Guy Van den Broeck - Google Search
https://www.google.com/search?q=%0A%0AFollow%0ALianhui%20Qin | Follow Lianhui Qin - Google Search
https://web.cs.ucla.edu/~weiwang/ | Wei Wang's Home Page
https://khhuang.me/ | Kuan-Hao Huang

https://dill-lab.github.io/ | DILL Lab üåø
https://swabhs.com/ | Swabha Swayamdipta
https://robinjia.github.io/ | Robin Jia
https://nlp.usc.edu/ | USC NLP
https://sghosh73.github.io/ | Sayan Ghosh
https://scholar.google.co.uk/citations?hl=en&user=IBlMTLwAAAAJ&view_op=list_works&sortby=pubdate | ‚Ä™Dani Yogatama‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
https://watermark.silverchair.com/tacl_a_00476.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAArowggK2BgkqhkiG9w0BBwagggKnMIICowIBADCCApwGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMrMWUP0kg7j0vgkcSAgEQgIICbY1GYlKAzckFFbfgbgEWAG9XIzMahpcJkW648VAKfnqsP91mu3AlFOpcXZmDMifWoBCMjk4afPcXlnAaU86M8zXxBh0vvVk28eQb2BLn1Yrg1PkGlyUg_Y3fIw8EYGurfvXTxTih41u4fIZhyzu63TCLvWAp-K1iLaUtT20VTwJmf57ovW2rCD1hM-ZgA75QXdYalpXdp6tLtQZOeSOpZgxeER5wIzX11w4ITjSF-TxFFQvc9V-0zaEyNf0Mwbx1Zmp9GWbsr6rqZkABgTuxhLNRkO8vFFuQDdnB5_6am77SbQpZ7wKa33WDrQTHCDAD54sWMDkfou45EBxeZNiaAqT2JTn1-xraI2vtw9OR-i2EV_Bpj3HEnu1XpdbzKuuCjs5j6sslrp8Q1_3dxiUA3B_elaKk9IId8RMrfyWKUrs40UaTllYecy-NK2d2TxMwP0Vbgkfs2Lxjk-d3NGWisl6AYVrIrtSc9WxLR_HUtGvj5xdNnDTjXtCmjMVgmj-wsKQwnRdZrw8jfhAEdMhiMIkSJLIZCHk7gZPRnQcex8IenBGuQiYEHhQC9irriZYTa96SFWiIhFInUGdteuv3KK9Iq_VuKixDNNB15aT7j5BsgVhPO0LaV0oORVb3iSlXV9d7XJ0pzq88gUUkOpiaCtqENTEeyrSo8d-w7grSWTg-lB5Z7FpB7Pbfxz4uvaRWblMOyiTvIijtqOSZHU7XV8Dk-qOagb8255ss6DmKacIJKIhiWQ2sdiQwN3OgUwsZIe5xMxtsTulSOsO2DR4ofU4RVxt3zuJGMFuKjbR0hxGjUIJobS27PS-f3TFu1A | tacl_a_00476.pdf
https://arxiv.org/pdf/2203.01311.pdf | HighMMT Towards Modality and Task Generalization for High-Modality Representation Learning - Arxiv-2203.01311
https://www.google.com/search?q=Joshua%20Robinson | Joshua Robinson - Google Search
https://arxiv.org/pdf/2212.10378.pdf | Careful Data Curation Stabilizes In-context Learning - Arxiv-2212.10378
https://arxiv.org/pdf/2211.15718.pdf | CoNAL Anticipating Outliers with Large Language Models - Arxiv-2211.15718
https://www.1point3acres.com/bbs/thread-969179-1-1.html | 1st rej stanford|‰∏Ä‰∫©‰∏âÂàÜÂú∞ÂΩïÂèñÊ±áÊä•ÔºöÁ†îÁ©∂ÁîüÁâà
https://posts.careerengine.us/p/60fb91c8bde42948f9b51064 | Áî≥ËØ∑ÂåóÁæéÂçöÂ£´ÔºåÊöëÁ†îÁ´üÁÑ∂Ëøô‰πàÂä†ÂàÜÔºü
https://lijinzhang.com/post/2022-03-03-tutorial-phd-app/ | ÂåóÁæéÂçöÂ£´Áî≥ËØ∑ÊîªÁï• - Lijin Zhang
https://github.com/dangkhoasdc/awesome-ai-residency | dangkhoasdc/awesome-ai-residency: List of AI Residency Programs
https://dyogatama.github.io/ | Dani Yogatama
https://leuchine.github.io/ | Qi Liu's Homepage

https://arxiv.org/pdf/2204.07705.pdf | Super-NaturalInstructions Generalization via Declarative Instructions on 1600+ NLP Tasks - Arxiv-2204.07705
https://instructions.apps.allenai.org/ | Learning From Instructions
https://arxiv.org/pdf/2205.03401.pdf | The Unreliability of Explanations in Few-Shot In-Context Learning - Arxiv-2205.03401
https://arxiv.org/abs/2010.05607 | The elephant in the interpretability room Why use attention as explanation when we have saliency methods? - Arxiv-2010.05607
https://arxiv.org/pdf/2204.02329.pdf | Can language models learn from explanations in context? - Arxiv-2204.02329

https://www.reddit.com/r/MachineLearning/comments/1080c3d/p_evaluating_several_topic_modeling/ | (2) [P] Evaluating several topic modeling implementations. What's the current best practice? BERTopic? OpenAI Ada-002? : MachineLearning
https://github.com/MilaNLProc/contextualized-topic-models#id9 | MilaNLProc/contextualized-topic-models: A python package to run contextualized topic modeling. CTMs combine contextualized embeddings (e.g., BERT) with topic models to get coherent topics. Published at EACL and ACL 2021.
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247529719&idx=1&sn=e282c8dc4a235992241813f9b88e7e7c&exportkey=n_ChQIAhIQzlxFJyQcqV3DdMuHZ6eL8BKWAgIE97dBBAEAAAAAADsTAQcK5esAAAAOpnltbLcz9gKNyK89dVj0Gw%2FaIHS%2ByF8kjmlLyB%2FHHjuvW%2BIDTyXDFNiPKDxIn%2BiStHRu0yl1dtD61NZfa4fhWHte%2FdvfP9vBYPYKuCc3b%2B%2BA0UEvXu24%2B0T%2BMz3DltccB%2F7cFwzli%2BM4xP9Oj4ExyczYCLXd4PFwSeoT87TxoL6FBmF%2BR2U9OKfHmFX%2B%2BIbJY3fBUsoUQSoAKsg3X2oUDSQVxRF1akGRx%2FG1pA1%2BfYdm6T7ST5fQ8GwzHQ%2BXd4EtTrkQI5hVK3Ah6j%2FJPCYUUaUr1ZA7luYsXuWU4NBipVj6BRHL2%2BlqAGnGns49njsmZvI1KN1jKpRdQaTUef16&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q0xYaDUhGjWF2iTn%2BxYLPlk&wx_header=0 | BERTopicÔºöNLP‰∏ªÈ¢òÊ®°ÂûãÁöÑÊú™Êù•ÔºÅ
https://maartengr.github.io/BERTopic/api/bertopic.html | BERTopic - BERTopic
https://twitter.com/nazneenrajani/status/1618737741453479939 | (1) Nazneen Rajani on Twitter: "You can create your own chatbot by fine-tuning pre-trained causal LLM to follow instructions ü§ñ Here is a list of datasets on @huggingface hub that you can use for Instruction fine-tuning (IFT) üßµ /0 https://t.co/uumHYdbQ1S" / Twitter
https://github.com/allenai/natural-instructions | allenai/natural-instructions: Expanding natural instructions
https://github.com/allenai/natural-instructions/pull/557 | print contributed authors by danyaljj ¬∑ Pull Request #557 ¬∑ allenai/natural-instructions
https://github.com/yizhongw/Tk-Instruct | yizhongw/Tk-Instruct: Tk-Instruct is a Transformer model that is tuned to solve many NLP tasks by following instructions.
https://instructions.apps.allenai.org/ | Learning From Instructions
https://www.paperdigest.org/2023/01/recent-papers-on-chatgpt/ | Paper Digest: Recent Papers on ChatGPT ‚Äì Paper Digest
https://shamulent.github.io/CS_Stat184_Fall22.html | CS/Stat 184 Intro to RL
https://arxiv.org/pdf/2109.07830.pdf | Reframing Instructional Prompts to GPTk's Language - Arxiv-2109.07830
https://arxiv.org/pdf/2205.00049.pdf | Prompt Consistency for Zero-Shot Task Generalization - Arxiv-2205.00049

https://arxiv.org/pdf/2204.05239.pdf | Exploring the Universal Vulnerability of Prompt-based Learning Paradigm - Arxiv-2204.05239
https://arxiv.org/pdf/2211.01910.pdf | Large Language Models Are Human-Level Prompt Engineers - Arxiv-2211.01910
https://arxiv.org/pdf/2302.07842.pdf | Augmented Language Models a Survey - Arxiv-2302.07842
https://scholar.google.com/scholar?q=+Memorization+without+overftiting:+Analyzing+the+training+dynamics+of+large+language+models&hl=en&as_sdt=0,22 | Memorization without overftiting: Analyzing the... - Google Scholar
https://arxiv.org/pdf/2205.10770.pdf | Memorization Without Overfitting Analyzing the Training Dynamics of Large Language Models - Arxiv-2205.10770
https://arxiv.org/pdf/2212.10403.pdf | Towards Reasoning in Large Language Models A Survey - Arxiv-2212.10403

https://arxiv.org/pdf/2210.09338.pdf | Deep Bidirectional Language-Knowledge Graph Pretraining - Arxiv-2210.09338
https://arxiv.org/pdf/2202.05262.pdf | Locating and Editing Factual Associations in GPT - Arxiv-2202.05262
file:///home/chris/Downloads/20215-Article%20Text-24228-1-2-20220628.pdf | An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA
https://arxiv.org/pdf/2101.00376.pdf | RiddleSense Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge - Arxiv-2101.00376
https://proceedings.neurips.cc/paper/2021/file/f45a1078feb35de77d26b3f7a52ef502-Paper.pdf | Gradient-based Editing of Memory Examples for Online Task-free Continual Learning - NeurIPS-2021_f45a1078
https://arxiv.org/pdf/2106.11533.pdf | Do Language Models Perform Generalizable Commonsense Inference? - Arxiv-2106.11533
https://arxiv.org/pdf/2205.12598.pdf | RobustLR Evaluating Robustness to Logical Perturbation in Deductive Reasoning - Arxiv-2205.12598
https://arxiv.org/pdf/2205.12542.pdf | ER-TEST Evaluating Explanation Regularization Methods for NLP Models - Arxiv-2205.12542
https://arxiv.org/pdf/2108.01721.pdf | Improving Counterfactual Generation for Fair Hate Speech Detection - Arxiv-2108.01721
https://github.com/yao8839836/kg-bert | yao8839836/kg-bert: KG-BERT: BERT for Knowledge Graph Completion
https://arxiv.org/pdf/2103.05327.pdf | BERTese Learning to Speak to BERT - Arxiv-2103.05327
https://proceedings.neurips.cc/paper/2021/file/5c04925674920eb58467fb52ce4ef728-Paper.pdf | True Few-Shot Learning with Language Models - NeurIPS-2021_5c049256
https://arxiv.org/pdf/2104.08315.pdf | Surface Form Competition Why the Highest Probability Answer Isn't Always Right - Arxiv-2104.08315
https://arxiv.org/abs/2205.05055 | Data Distributional Properties Drive Emergent In-Context Learning in Transformers - Arxiv-2205.05055
https://arxiv.org/pdf/2210.11560.pdf | Finding Dataset Shortcuts with Grammar Induction - ACL-EMNLP-2022_2022.emnlp-main.293
https://arxiv.org/abs/2202.06539 | Deduplicating Training Data Mitigates Privacy Risks in Language Models - Arxiv-2202.06539
https://arxiv.org/pdf/2106.13353.pdf | Cutting Down on Prompts and Parameters Simple Few-Shot Learning with Language Models - Arxiv-2106.13353
https://arxiv.org/pdf/2301.03044.pdf | https://arxiv.org/pdf/2301.03044.pdf
https://www.surgehq.ai/blog/introduction-to-reinforcement-learning-with-human-feedback-rlhf-series-part-1 | Introduction to Reinforcement Learning with Human Feedback
https://huggingface.co/blog/rlhf | Illustrating Reinforcement Learning from Human Feedback (RLHF)
https://arxiv.org/abs/2204.05186 | Correcting Robot Plans with Natural Language Feedback - Arxiv-2204.05186

https://www.proquest.com/openview/17b48d375b45931f6739a01f9086d6b0/1?pq-origsite=gscholar&cbl=18750&diss=y | Finding and Fixing Undesirable Behaviors in Pretrained Language Models - ProQuest
https://openreview.net/forum?id=r6wu2WDhib9 | Learning from Natural Language Feedback | OpenReview
https://openreview.net/forum?id=89qDzjrWHLs | Can Large Language Models Truly Follow your Instructions? | OpenReview
https://arxiv.org/pdf/2202.03286.pdf | Red Teaming Language Models with Language Models - Arxiv-2202.03286
https://arxiv.org/pdf/2209.01975.pdf | Selective Annotation Makes Language Models Better Few-Shot Learners - Arxiv-2209.01975
https://arxiv.org/pdf/2205.00049.pdf | Prompt Consistency for Zero-Shot Task Generalization - Arxiv-2205.00049
https://arxiv.org/pdf/2212.08410.pdf | Teaching Small Language Models to Reason - Arxiv-2212.08410
https://arxiv.org/pdf/2209.07686.pdf | Text and Patterns For Effective Chain of Thought, It Takes Two to Tango - Arxiv-2209.07686
https://arxiv.org/pdf/2203.09161.pdf | How Many Data Samples is an Additional Instruction Worth? - Arxiv-2203.09161
https://aclanthology.org/2022.emnlp-main.410.pdf | Fine-tuned Language Models are Continual Learners - ACL-EMNLP-2022_2022.emnlp-main.410
https://arxiv.org/pdf/2212.03827.pdf | Discovering Latent Knowledge in Language Models Without Supervision - Arxiv-2212.03827
https://proceedings.mlr.press/v203/jang23a/jang23a.pdf | jang23a.pdf
https://arxiv.org/pdf/2206.13757.pdf | Flexible text generation for counterfactual fairness probing - Arxiv-2206.13757
https://arxiv.org/pdf/2210.11416.pdf | Scaling Instruction-Finetuned Language Models - Arxiv-2210.11416
https://twitter.com/EthanJPerez/status/1604886125482344449 | (1) Ethan Perez on Twitter: "Worrying behavior 2: LMs/RLHF models are people-pleasers, learning to repeat back dialog users‚Äô views as their own (‚Äúsycophancy‚Äù). Sycophancy creates echo-chambers. Below, the same RLHF model gives opposite answers to a political question, in line with the user‚Äôs view: https://t.co/BT6AzMUMic" / Twitter
https://arxiv.org/pdf/2109.01652.pdf | Finetuned Language Models Are Zero-Shot Learners - Arxiv-2109.01652

https://arxiv.org/abs/2109.09193 | Towards Zero-Label Language Learning - Arxiv-2109.09193
https://arxiv.org/pdf/2109.09193.pdf | Towards Zero-Label Language Learning - Arxiv-2109.09193
https://arxiv.org/pdf/1911.03118.pdf | Not Enough Data? Deep Learning to the Rescue! - Arxiv-1911.03118
https://arxiv.org/pdf/2202.04538.pdf | Generating Training Data with Language Models Towards Zero-Shot Language Understanding - Arxiv-2202.04538
https://arxiv.org/pdf/2202.07922.pdf | ZeroGen Efficient Zero-shot Learning via Dataset Generation - Arxiv-2202.07922
https://arxiv.org/pdf/2211.03044.pdf | Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning - Arxiv-2211.03044
https://arxiv.org/pdf/2108.13487.pdf | Want To Reduce Labeling Cost? GPT-3 Can Help - Arxiv-2108.13487
https://arxiv.org/pdf/1811.00741.pdf | Stronger Data Poisoning Attacks Break Data Sanitization Defenses - Arxiv-1811.00741

https://arxiv.org/pdf/1806.00692.pdf | Stress Test Evaluation for Natural Language Inference - Arxiv-1806.00692

https://arxiv.org/pdf/2203.10133.pdf | Probing Factually Grounded Content Transfer with Factual Ablation - Arxiv-2203.10133
https://arxiv.org/pdf/2107.01294.pdf | Is GPT-3 Text Indistinguishable from Human Text? Scarecrow A Framework for Scrutinizing Machine Text - Arxiv-2107.01294

https://arxiv.org/abs/2010.02399 | Guiding Attention for Self-Supervised Learning with Transformers - Arxiv-2010.02399
https://arxiv.org/abs/2105.11115 | Self-Attention Networks Can Process Bounded Hierarchical Languages - Arxiv-2105.11115
https://arxiv.org/abs/2110.14782 | When is BERT Multilingual? Isolating Crucial Ingredients for Cross-lingual Transfer - Arxiv-2110.14782
https://arxiv.org/pdf/2301.11309.pdf | SemSup-XC Semantic Supervision for Zero and Few-shot Extreme Classification - Arxiv-2301.11309

https://docs.google.com/presentation/d/1bZFTW7c-mH5yQOUYmI4vzymJB6Pf7w1Bki-QyviaL7g/edit#slide=id.g11acd991225_0_330 | Adversarial Data Collection - Google Slides
https://arxiv.org/pdf/2211.09788.pdf | DiffusionDet Diffusion Model for Object Detection - Arxiv-2211.09788
https://scholar.google.com/citations?user=gGB0L4kAAAAJ | ‚Ä™Jonas Pfeiffer‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
https://cs-sop.org/ | CS PhD Statements of Purpose
https://www.google.com/search?q=%E5%BF%A7%E9%83%81%E7%9A%84%E7%83%AD%E5%B8%A6&oq=%E5%BF%A7%E9%83%81%E7%9A%84%E7%83%AD%E5%B8%A6&aqs=chrome.0.0i355i512j46i512j0i512l8.2213j0j1&sourceid=chrome&ie=UTF-8 | ÂøßÈÉÅÁöÑÁÉ≠Â∏¶ - Google Search
https://www.google.com/search?q=%E6%9C%AC%E5%A4%9A%E7%B9%81%E9%82%A6&oq=%E6%9C%AC%E5%A4%9Afan&aqs=chrome.1.69i57j0i512j0i4i15i30.4289j0j1&sourceid=chrome&ie=UTF-8 | Êú¨Â§öÁπÅÈÇ¶ - Google Search
https://www.google.com/search?q=%E6%97%B6%E9%92%9F%E4%B8%8D%E4%BC%9A%E6%92%92%E8%B0%8E | Êó∂Èíü‰∏ç‰ºöÊííË∞é - Google Search
https://zhuanlan.zhihu.com/p/391365563 | ËØª‰π¶Á¨îËÆ∞69„Ää2019‰∏≠ÂõΩÊÇ¨ÁñëÂ∞èËØ¥Á≤æÈÄâ„Äã - Áü•‰πé
https://hitomi.la/search.html?artist%3Autu%20language%3Achinese | Search | Hitomi.la
https://hitomi.la/manga/%EF%BC%BB%E3%81%99%E3%81%8E%E3%81%A2--%E3%82%B9%E3%82%B1%E3%83%99%E3%83%89%E3%83%AC%E3%83%84%E3%82%B7%E3%83%B3%E3%82%B0-%E4%B8%AD%E6%96%87-2354959.html#1 | Ôºª„Åô„Åé„Å¢-]„Çπ„Ç±„Éô„Éâ„É¨„ÉÑ„Ç∑„É≥„Ç∞ by sugi g | Hitomi.la
https://hitomi.la/search.html?artist%3Ajamming%20language%3Achinese | Search | Hitomi.la
https://hitomi.la/doujinshi/visiting-%E4%B8%AD%E6%96%87-2456947.html#1 | VISITING by laliberte | Hitomi.la
https://hitomi.la/doujinshi/hamegaki-x-yaritsuma-%E4%B8%AD%E6%96%87-2400588.html#1 | Hamegaki x Yaritsuma by jamming | Hitomi.la

https://arxiv.org/abs/2205.11482 | Tracing Knowledge in Language Models Back to the Training Data - Arxiv-2205.11482
https://www.neuralnet.science/reading-group/ | MIT Reading Group (Fall 2022): The Science of Deep Learning
https://arxiv.org/abs/2106.00737 | Implicit Representations of Meaning in Neural Language Models - Arxiv-2106.00737
https://arxiv.org/abs/2006.14032 | Compositional Explanations of Neurons - Arxiv-2006.14032
https://arxiv.org/pdf/2212.09257.pdf | PromptBoosting Black-Box Text Classification with Ten Forward Passes - Arxiv-2212.09257
https://twitter.com/jacobandreas/status/1600118539263741952 | (1) Jacob Andreas on Twitter: "Speculative (!!!) paper arguing that big LMs can model agency &amp; communicative intent: https://t.co/WYaedqx9TT (somehow in EMNLP findings). Briefly: 1. LMs do not in general have beliefs or goals. An LM trained on the Internet models a distribution over next tokens *marginalized* https://t.co/tZ8eFUhWOq" / Twitter
https://arxiv.org/abs/2211.15661 | What learning algorithm is in-context learning? Investigations with linear models | Abstract

https://arxiv.org/pdf/2210.11610.pdf | Large Language Models Can Self-Improve - Arxiv-2210.11610
https://book.douban.com/subject/36178388/ | È≠îËü≤‰∫∫Èñì 2 (Ë±ÜÁì£)
https://book.douban.com/subject/36081279/ | ÊùÄ‰∫∫Êé®ÁêÜÁ´ûËµõ (Ë±ÜÁì£)
https://book.douban.com/subject/35802295/ | Â§©Â†Ç‰πãÈü≥ÔºåÈ≠îÈ¨º‰πãÂêç (Ë±ÜÁì£)
https://book.douban.com/subject/5375591/ | ÊîæË™≤ÂæåÊé¢ÂÅµÂõ£ (Ë±ÜÁì£)
https://book.douban.com/subject/35977847/ | Âì≤Â≠¶ÂÆ∂ÁöÑÂØÜÂÆ§ ‰∏ä (Ë±ÜÁì£)
https://lockedroom.net/blog/?p=2061 | ÈúûÊµÅ‰∏Ä ÁªºËØÑ | Fang's Mystery Blog

https://docs.google.com/document/d/1sPSV8VY4AUYXiUZJS3uvVesquYA7V5aslN01ynXcQ_Y/edit | Compute - SC - Google ÊñáÊ°£
https://cs.stanford.edu/sc/job-submissions | Job Submissions | Stanford Computer Science
https://cs.stanford.edu/sc/managing-jobs | Managing Jobs | Stanford Computer Science
https://cs.stanford.edu/sc/cluster-storage | Cluster Storage | Stanford Computer Science
https://cs.stanford.edu/sc/useful-cli-tools | Useful CLI tools | Stanford Computer Science
https://baike.baidu.com/item/%E7%A5%9E%E9%9E%AD/66652 | Á•ûÈû≠ÔºàÂÜØÈ™•ÊâçËëó‰∏≠ÁØáÂ∞èËØ¥Ôºâ_ÁôæÂ∫¶ÁôæÁßë

https://piazza.com/class_profile/get_resource/ldjefz5a4yu69n/le4m7z1yijx13b | broderick_lecture3_spring2023
https://arxiv.org/abs/2012.07421 | WILDS A Benchmark of in-the-Wild Distribution Shifts - Arxiv-2012.07421
https://arxiv.org/pdf/2301.11305v1.pdf | DetectGPT Zero-Shot Machine-Generated Text Detection using Probability Curvature - Arxiv-2301.11305
https://detectgpt.ericmitchell.ai/ | DetectGPT: a GPT-2 Detector
https://www.google.com/search?q=conservative+prediction+via+transductive+confidence+minimzaiton&newwindow=1&sxsrf=AJOqlzVFX2zULqzvNNP0trqaifS73NK2Ig%3A1677269686848&ei=thr5Y8meM62q5NoPrMeC6Ak&ved=0ahUKEwjJ0YG__K79AhUtFVkFHayjAJ0Q4dUDCBA&uact=5&oq=conservative+prediction+via+transductive+confidence+minimzaiton&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzoKCAAQRxDWBBCwAzoFCCEQoAE6BQghEKsCOgcIIRCgARAKSgQIQRgAUKwJWNYnYKEpaAFwAXgBgAGGAYgB4BCSAQQxOC41mAEAoAEByAEIwAEB&sclient=gws-wiz-serp | conservative prediction via transductive confidence minimzaiton - Google Search
https://www.google.com/search?q=diversity+and+disambiguate&oq=diversity+and+disambiguate&aqs=chrome..69i57j33i160l2.5407j0j1&sourceid=chrome&ie=UTF-8 | diversity and disambiguate - Google Search
https://arxiv.org/pdf/2202.03418.pdf | Diversify and Disambiguate Learning From Underspecified Data - Arxiv-2202.03418
https://github.com/yoonholee/DivDis | yoonholee/DivDis
https://sites.google.com/view/diversify-and-disambiguate | DivDis

https://arxiv.org/pdf/2302.07459.pdf | The Capacity for Moral Self-Correction in Large Language Models - Arxiv-2302.07459
https://arxiv.org/pdf/2212.09251.pdf | Discovering Language Model Behaviors with Model-Written Evaluations - Arxiv-2212.09251
https://www.google.com/search?q=BBQ%3A+A+Hand-Built+Bias+Benchmark+for+Question+Answering+scholar&newwindow=1&sxsrf=AJOqlzViaecZeW2lvDWx76ScxQyH21RSUg%3A1677251227933&ei=m9L4Y8DGOMSl5NoPgPejkAE&ved=0ahUKEwiAgI_dt679AhXEElkFHYD7CBIQ4dUDCBA&uact=5&oq=BBQ%3A+A+Hand-Built+Bias+Benchmark+for+Question+Answering+scholar&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzoECCMQJzoGCAAQFhAeOgUIABCGAzoFCCEQoAE6BQghEKsCSgQIQRgBUIQBWJEIYMwIaAFwAHgAgAGHAYgBvAaSAQMxLjaYAQCgAQHAAQE&sclient=gws-wiz-serp | BBQ: A Hand-Built Bias Benchmark for Question Answering scholar - Google Search
https://github.com/nyu-mll/BBQ | nyu-mll/BBQ: Repository for the Bias Benchmark for QA dataset.
https://arxiv.org/pdf/2110.08193.pdf | BBQ A Hand-Built Bias Benchmark for Question Answering - Arxiv-2110.08193


https://canvas.mit.edu/courses/20206 | 6.S986 Special Subject in EECS
https://docs.google.com/spreadsheets/d/1dTI2gddd2m5N3YEticolGjK9g29dH7fUkjrIv_cMN0o/edit#gid=1270115572 | LLM Papers - Google Ë°®Ê†º
https://arxiv.org/pdf/2212.07677.pdf | Transformers learn in-context by gradient descent - Arxiv-2212.07677
https://arxiv.org/abs/2211.15661 | What learning algorithm is in-context learning? Investigations with linear models | Abstract
https://openreview.net/forum?id=0g0X4H8yN4I | ‚Äã‚ÄãWhat learning algorithm is in-context learning? Investigations with linear models - OR-ICLR-2023_0g0X4H8yN4I
https://openreview.net/pdf?id=0g0X4H8yN4I | ‚Äã‚ÄãWhat learning algorithm is in-context learning? Investigations with linear models - OR-ICLR-2023_0g0X4H8yN4I
https://twitter.com/ethayarajh/status/1628442002454085632?s=46&t=py8ptijkIjzYInrpRa-RPQ | Kawin Ethayarajh on Twitter: "üì¢ Models like #ChatGPT are trained on tons of human feedback. But collecting this costs $$$! That's why we're releasing the Stanford Human Preferences Dataset (üö¢SHP), a collection of 385K *naturally occurring* *collective* human preferences over text. https://t.co/cRY1F8TjFz" / Twitter
https://arxiv.org/pdf/2012.15723.pdf | Making Pre-trained Language Models Better Few-shot Learners - Arxiv-2012.15723
https://arxiv.org/abs/2212.02475 | Meta-Learning Fast Weight Language Models - Arxiv-2212.02475
https://arxiv.org/abs/2110.15943 | MetaICL Learning to Learn In Context - Arxiv-2110.15943
https://arxiv.org/pdf/2212.05129.pdf | Measuring Data - Arxiv-2212.05129
https://arxiv.org/pdf/1704.01444.pdf | Learning to Generate Reviews and Discovering Sentiment - Arxiv-1704.01444

https://book.douban.com/subject/1861809/ | Êù±‰∫¨Áï∞ËÅû (Ë±ÜÁì£)
https://book.douban.com/subject/34882130/ | ÁµÉ‰πãËÅñÂüü (Ë±ÜÁì£)

https://arxiv.org/pdf/2109.09193.pdf | Towards Zero-Label Language Learning - Arxiv-2109.09193
https://arxiv.org/abs/2108.13487 | Want To Reduce Labeling Cost? GPT-3 Can Help - Arxiv-2108.13487
https://arxiv.org/pdf/2205.12640.pdf | Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating Spurious Correlations in Entity Typing - Arxiv-2205.12640
https://arxiv.org/pdf/1610.05820.pdf | Membership Inference Attacks against Machine Learning Models - Arxiv-1610.05820
https://openaccess.thecvf.com/content/CVPR2021/papers/Rezaei_On_the_Difficulty_of_Membership_Inference_Attacks_CVPR_2021_paper.pdf | On the Difficulty of Membership Inference Attacks - CVPR-2021_17609415
https://arxiv.org/pdf/2203.03929.pdf | Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks - Arxiv-2203.03929

https://mp.weixin.qq.com/s?__biz=MzIzNzU4OTAxMQ==&mid=2247485997&idx=1&sn=004a561bfc87aa868abe3bd58dcdfe33&exportkey=n_ChQIAhIQ8ZyzeDsxsY1%2BQKi%2BlJpQzxKWAgIE97dBBAEAAAAAADlJIIA6PrYAAAAOpnltbLcz9gKNyK89dVj09uu03swE51UzoM%2FpDA%2Fa0RMk1Fu%2ByVGb1tX3k8zvtH2CeZa3F3GT1PmybC4CVP6ZzMvltyuxPr%2Fs%2B%2BV4l3VwldcEJLmNeCOlQYph3Quc669owPVMHfAWQI2LkVMWf8M5x6ijHhKMSNAher1YysXd9ybAafX8%2FGuJXuJ2uk2PvQz9JCidEv0PTRzv0R1HoQNJ%2BUUZmQDLVtxVo3iTQrarUox5fMNAr1lLxkWXLMrH0sGTdjPkeRXzjNYiaMh42jxSLWzuetIJ3YWBThwy2%2FAPSQRseu3GIGQuS5fjHHQCzTcGeffgyy24HuU4WE03zyOV&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q2YiWRGp6wOl8yvbOvb7F7m&wx_header=0 | ECCV2022 | RU&Ë∞∑Ê≠åÊèêÂá∫Áî®CLIPËøõË°åzero-shotÁõÆÊ†áÊ£ÄÊµãÔºÅ‚Äã
https://github.com/microsoft/unilm/tree/master/beit2 | unilm/beit2 at master ¬∑ microsoft/unilm
https://github.com/IDEA-Research/DINO | IDEA-Research/DINO: [ICLR 2023] Official implementation of the paper "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection"
https://github.com/IDEA-Research/detrex | IDEA-Research/detrex: detrex is a research platform for Transformer-based Instance Recognition algorithms including DETR (ECCV 2020), Deformable-DETR (ICLR 2021), Conditional-DETR (ICCV 2021), DAB-DETR (ICLR 2022), DN-DETR (CVPR 2022), DINO (ICLR 2023), H-DETR (arXiv 2022), MaskDINO (arXiv 2022), etc.
https://github.com/microsoft/GLIP | microsoft/GLIP: Grounded Language-Image Pre-training
https://gligen.github.io/ | GLIGEN:Open-Set Grounded Text-to-Image Generation.
https://arxiv.org/abs/2005.00545 | Low-Dimensional Hyperbolic Knowledge Graph Embeddings - Arxiv-2005.00545
https://homepages.inf.ed.ac.uk/rsarkar/papers/HyperbolicDelaunayFull.pdf | HyperbolicDelaunayFull.pdf
https://github.com/shizhediao/ChatGPTPapers | shizhediao/ChatGPTPapers: Must-read papers, related blogs and API tools on the pre-training and tuning methods for ChatGPT.
https://cohere.for.ai/?utm_term=&utm_campaign=na_textclassification_performancemax&utm_source=google&utm_medium=paidsearch&hsa_acc=4946693046&hsa_cam=17376869464&hsa_grp=&hsa_ad=&hsa_src=x&hsa_tgt=&hsa_kw=&hsa_mt=&hsa_net=adwords&hsa_ver=3&gclid=Cj0KCQiAi8KfBhCuARIsADp-A55qccz4zsaRu7oscuoOfaEjImnaVsYlySKTaKjQopnl1cCSI5ipS28aAo7KEALw_wcB | Home | Cohere For AI
https://arxiv.org/pdf/2302.09419.pdf | A Comprehensive Survey on Pretrained Foundation Models A History from BERT to ChatGPT - Arxiv-2302.09419
https://arxiv.org/abs/1706.02216 | Inductive Representation Learning on Large Graphs - Arxiv-1706.02216
https://github.com/NoviScl/GPT3-Reliability | NoviScl/GPT3-Reliability

https://huggingface.co/docs/timm/main/en/training_script | Scripts
https://github.com/microsoft/FocalNet | microsoft/FocalNet: [NeurIPS 2022] Official code for "Focal Modulation Networks"
https://github.com/SwinTransformer/Feature-Distillation | SwinTransformer/Feature-Distillation
https://detrex.readthedocs.io/en/latest/tutorials/Installation.html | Installation ‚Äî detrex documentation

https://captum.ai/api/search.html?q=SHAP | Captum ¬∑ Model Interpretability for PyTorch
https://www.bilibili.com/video/BV1FD4y1A7Ye/?spm_id_from=333.1007.tianma.1-1-1.click&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | Â∞ëÂ•≥Ë¢´Âõ∞ÂØÜÂÆ§ÔºåÁîüÊ≠ªÂèñÂÜ≥‰∫é‰Ω†Ôºå1998Âπ¥‰∏ñÂòâÂúüÊòüÁªèÂÖ∏AVGÊ∏∏Êàè_ÂçïÊú∫Ê∏∏ÊàèÁÉ≠Èó®ËßÜÈ¢ë
https://arxiv.org/abs/2106.09685 | LoRA Low-Rank Adaptation of Large Language Models - Arxiv-2106.09685
https://arxiv.org/pdf/2104.07540.pdf | Generating Datasets with Pretrained Language Models - Arxiv-2104.07540
https://arxiv.org/pdf/2110.05448.pdf | Unsupervised Neural Machine Translation with Generative Language Models Only - Arxiv-2110.05448

https://zhuanlan.zhihu.com/p/91383421 | EMNLP ÊúÄ‰Ω≥ËÆ∫ÊñáËß£ËØªÔºöÊù•Ëá™‰ø°ÊÅØÁì∂È¢àÁöÑÊñ∞ËØ≠Ë®ÄÂ≠¶ÁêÜËÆ∫ - Áü•‰πé
https://zhuanlan.zhihu.com/p/340329943 | RealFormerÔºöReal ÁÆÄÂçïÔºåReal ÊúâÊïà - Áü•‰πé
https://zhuanlan.zhihu.com/p/348402227 | GPT ÁöÑÈáéÊúõ - Áü•‰πé
https://zhuanlan.zhihu.com/p/86900556 | BERT Áò¶Ë∫´‰πãË∑ØÔºöDistillationÔºåQuantizationÔºåPruning - Áü•‰πé
https://zhuanlan.zhihu.com/p/75893972 | SpanBertÔºöÂØπ Bert È¢ÑËÆ≠ÁªÉÁöÑ‰∏ÄÊ¨°Ê∑±Â∫¶Êé¢Á¥¢ - Áü•‰πé
https://www.zhihu.com/people/zhang-jun-lin-76/posts | (41 Â∞ÅÁßÅ‰ø° / 80 Êù°Ê∂àÊÅØ) Âº†‰øäÊûó - Áü•‰πé

https://arxiv.org/pdf/2208.12242.pdf | DreamBooth Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation - Arxiv-2208.12242
https://posts.careerengine.us/author/5f4c50ffd1f9823b82ea2eb8/posts?from=authorDetailSidePanel | Èõ®Áü≥ËÆ∞ËµÑËÆØ

https://www-cs-faculty.stanford.edu/people/jure/pubs/interpretable-kdd16.pdf | interpretable-kdd16.pdf
https://arxiv.org/pdf/1602.04938.pdf | 'Why Should I Trust You?' Explaining the Predictions of Any Classifier - ACM-2016_10114529396722939778
https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf | A Unified Approach to Interpreting Model Predictions - Arxiv-1705.07874
https://arxiv.org/pdf/2302.03494.pdf | A Categorical Archive of ChatGPT Failures - Arxiv-2302.03494

https://lilianweng.github.io/posts/2018-10-13-flow-models/ | Flow-based Deep Generative Models | Lil'Log
https://lilianweng.github.io/posts/2017-09-28-information-bottleneck/ | Anatomize Deep Learning with Information Theory | Lil'Log

https://book.douban.com/subject/35272690/ | Êµ∑Ëëµ (Ë±ÜÁì£)
https://book.douban.com/subject/30405794/ | Â∞∏ËØ≠Â•≥Ê≥ïÂåª (Ë±ÜÁì£)
https://book.douban.com/subject/36127454/ | Èõ™Á•≠ (Ë±ÜÁì£)
https://book.douban.com/subject/35812032/ | È£üË°ÄËçâ (Ë±ÜÁì£)
https://book.douban.com/subject/33418859/ | Êú¨ÊâÄ‰∏ÉÊÄ™Ë∞à (Ë±ÜÁì£)
https://book.douban.com/subject/35755556/ | ÊÅ∂ÊÑèÁöÑÂÖîÂ≠ê (Ë±ÜÁì£)
https://www.google.com/search?q=%E5%9B%9B%E5%8F%A0%E5%8D%8A%E6%97%B6%E5%85%89%E6%9C%BA%E5%B8%83%E9%B2%81%E6%96%AF | ÂõõÂè†ÂçäÊó∂ÂÖâÊú∫Â∏ÉÈ≤ÅÊñØ - Google Search
https://www.99csw.com/book/8812/index.htm | Êê™Áì∑ÁÅµÈ≠ÇÁöÑÊØîÈáç_‰ΩêËó§ÂèãÂìâ_Âú®Á∫øÈòÖËØª_‰πù‰πùËóè‰π¶ÁΩë
https://www.douban.com/search?q=%E6%B8%90%E5%8F%98%E6%B6%88%E5%A4%B1 | ÊêúÁ¥¢: Ê∏êÂèòÊ∂àÂ§±
https://book.douban.com/subject/34784730/ | Èì∂Ê≤≥ÈìÅÈÅì‰πãÂ§ú (Ë±ÜÁì£)
https://www.douban.com/search?q=%E4%B8%80%E6%A1%A9%E4%BA%8B%E5%85%88%E5%BC%A0%E6%89%AC%E7%9A%84%E8%B0%8B%E6%9D%80%E6%A1%88 | ÊêúÁ¥¢: ‰∏ÄÊ°©‰∫ãÂÖàÂº†Êâ¨ÁöÑË∞ãÊùÄÊ°à
https://lilianweng.github.io/page/2/ | Lil'Log
https://lilianweng.github.io/posts/2022-04-15-data-gen/ | Learning with not Enough Data Part 3: Data Generation | Lil'Log
https://lilianweng.github.io/posts/2022-02-20-active-learning/ | Learning with not Enough Data Part 2: Active Learning | Lil'Log
https://lilianweng.github.io/posts/2021-12-05-semi-supervised/ | Learning with not Enough Data Part 1: Semi-Supervised Learning | Lil'Log
https://lilianweng.github.io/posts/2017-06-21-overview/ | An Overview of Deep Learning for Curious People | Lil'Log
https://lilianweng.github.io/posts/2017-08-01-interpretation/ | How to Explain the Prediction of a Machine Learning Model? | Lil'Log
https://lilianweng.github.io/posts/2017-08-20-gan/ | From GAN to WGAN | Lil'Log
https://www.douban.com/search?q=%E9%9B%B6%E7%9A%84%E8%9C%9C%E6%9C%88 | ÊêúÁ¥¢: Èõ∂ÁöÑËúúÊúà
https://book.douban.com/subject/30170658/ | ÁåÆÁªôË∞ãÊùÄÁöÑ‰æõÁâ© (Ë±ÜÁì£)

https://book.douban.com/subject/30488074/ | ÊÄùÊÉ≥ÁöÑÈªÑÊòè (Ë±ÜÁì£)
https://book.douban.com/subject/35900190/ | Âú®ÁªùÊúõ‰πãÂ∑Ö (Ë±ÜÁì£)
https://book.douban.com/subject/25774978/ | ÁúºÊ≥™‰∏éÂú£Âæí (Ë±ÜÁì£)
https://movie.douban.com/subject/24708811/ | 8Âè∑ÊàøÈó¥ (Ë±ÜÁì£)
https://movie.douban.com/subject/3531406/ | ÂÜçÁîüÈó® (Ë±ÜÁì£)
https://www.wikiwand.com/en/Concentration_inequality | Concentration inequality - Wikiwand

https://arxiv.org/pdf/2108.04106.pdf | Noisy Channel Language Model Prompting for Few-Shot Text Classification - Arxiv-2108.04106
https://github.com/shmsw25/Channel-LM-Prompting | shmsw25/Channel-LM-Prompting: An original implementation of "Noisy Channel Language Model Prompting for Few-Shot Text Classification"
https://aclanthology.org/2022.acl-long.365.pdf | 2022.acl-long.365.pdf
https://github.com/Timothyxxx/Chain-of-ThoughtsPapers | Timothyxxx/Chain-of-ThoughtsPapers: A trend starts from "Chain of Thought Prompting Elicits Reasoning in Large Language Models".

https://openreview.net/forum?id=vsShetzoRG9 | InsNet: An Efficient, Flexible, and Performant Insertion-based Text Generation Model | OpenReview
https://openreview.net/pdf?id=vsShetzoRG9 | InsNet An Efficient, Flexible, and Performant Insertion-based Text Generation Model - OR-NeurIPS-2022_vsShetzoRG9
https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation | SolidGoldMagikarp (plus, prompt generation) - LessWrong
https://www.wikiwand.com/en/Conditional_expectation | Conditional expectation - Wikiwand
https://www.wikiwand.com/en/Borel-Kolmogorov_paradox | Borel‚ÄìKolmogorov paradox - Wikiwand

https://arxiv.org/pdf/2210.10749.pdf | Transformers Learn Shortcuts to Automata - Arxiv-2210.10749
https://arxiv.org/pdf/2301.13196.pdf | Looped Transformers as Programmable Computers | PDF
https://github.com/facebookresearch/CutLER | facebookresearch/CutLER: Code release for "Cut and Learn for Unsupervised Object Detection and Instance Segmentation"
http://people.eecs.berkeley.edu/~xdwang/projects/CutLER/ | Cut and Learn for Unsupervised Object Detection and Instance Segmentation

https://twitter.com/lateinteraction/status/1617953413576425472 | Omar Khattab on Twitter: "Introducing Demonstrate‚ÄìSearch‚ÄìPredict (ùóóùó¶ùó£), a framework for composing search and LMs w/ up to 120% gains over GPT-3.5. No more prompt engineering.‚ùå Describe a high-level strategy as imperative code and let ùóóùó¶ùó£ deal with prompts and queries.üßµ https://t.co/2265Ii0vCS https://t.co/WAaHGH7fCQ" / Twitter
https://github.com/hwchase17/langchain | hwchase17/langchain: ‚ö° Building applications with LLMs through composability ‚ö°
https://proceedings.mlr.press/v162/lang22a.html | Co-training Improves Prompt-based Learning for Large Language Models - Arxiv-2202.00828
https://arxiv.org/pdf/2212.14052.pdf | Hungry Hungry Hippos Towards Language Modeling with State Space Models - Arxiv-2212.14052
https://slideslive.com/38967412/sequencetosequence-learning-with-latent-neural-grammars?ref=recommended | Yoon Kim ¬∑ Sequence-to-Sequence Learning with Latent Neural Grammars ¬∑ SlidesLive
https://arxiv.org/pdf/1904.09545.pdf | Good-Enough Compositional Data Augmentation - Arxiv-1904.09545
https://slideslive.com/38984059/cotraining-improves-promptbased-learning-for-large-language-models?ref=speaker-24304 | Hunter Lang, Monica Agrawal, Yoon Kim, David Sontag ¬∑ Co-Training Improves Prompt-Based Learning for Large Language Models ¬∑ SlidesLive
https://arxiv.org/pdf/1904.05521v1.pdf | UniVSE Robust Visual Semantic Embeddings via Structured Semantic Representations - Arxiv-1904.05521
https://www.google.com/search?q=Unified+Visual-Semantic+Embeddings%3A+Bridging+Vision+and+Language+with+Structured+Meaning+Representations&sourceid=chrome&ie=UTF-8 | Unified Visual-Semantic Embeddings: Bridging Vision and Language with Structured Meaning Representations - Google Search
https://jiayuanm.com/#topic-concept-learning | Jiayuan Mao

https://arxiv.org/pdf/2105.08127.pdf | Finding an Unsupervised Image Segmenter in Each of Your Deep Generative Models - Arxiv-2105.08127
https://www.dropbox.com/s/keiaw6tib1afj38/apm120-notes.pdf?dl=0 | apm120-notes.pdf
https://github.com/stanfordnlp/dsp | stanfordnlp/dsp: The Demonstrate-Search-Predict Framework: Composing retrieval and language models for knowledge-intensive NLP
https://github.com/stanfordnlp/dsp/blob/main/intro.ipynb | dsp/intro.ipynb at main ¬∑ stanfordnlp/dsp
https://colab.research.google.com/github/stanfordnlp/dsp/blob/main/intro.ipynb#scrollTo=LJVaof2m3dng | intro.ipynb - Colaboratory

https://arxiv.org/pdf/2301.08721v1.pdf | Batch Prompting Efficient Inference with Large Language Model APIs - Arxiv-2301.08721
https://twitter.com/cwolferesearch/status/1612886048949915650 | Cameron R. Wolfe on Twitter: "Recently, I‚Äôve read and overviewed publications for nearly 20 different large language models (LLMs) from GPT to ChatGPT. Here‚Äôs what I learned‚Ä¶ üßµ [1/10]" / Twitter
https://twitter.com/ChrisXU35407830/likes | Tweets liked by Chris XU (@ChrisXU35407830) / Twitter
https://twitter.com/sharifshameem/status/1618369196387340294 | Sharif Shameem on Twitter: "unpopular opinion: open source LLMs are *really good*. they suck at benchmarks like HELM compared to closed models, but fine-tuned accuracy is amazing, inference is dirt cheap, and both flan-t5-xxl + gpt-neox-20b fit on a single a100. now imagine 1-click RLAIF fine-tuning‚Ä¶ü§î" / Twitter
https://qntm.org/mmacevedo | Lena @ Things Of Interest
https://twitter.com/rajammanabrolu/status/1616590789953589248 | Prithviraj (Raj) Ammanabrolu on Twitter: "Now accepted to #ICLR2023! Look forward to our talk on open source, efficient natural language RLHF algorithms at Kigali, Rwanda!!!" / Twitter
https://twitter.com/dair_ai/status/1612153093101174784 | DAIR.AI on Twitter: "Top ML Papers of the Week (Jan 1-8): - Muse (new text-to-image generation/editing model) - Rethinking with retrieval - Pruning LLMs in one-shot - ConvNeXt V2 - LLMs for corporate lobbying-related activities - StitchNet - VALL-E ‚Ä¶ 1 of 11 https://t.co/XAAQrFN6ma" / Twitter
https://twitter.com/srchvrs/status/1612288813170135047 | Leo Boytsov on Twitter: "üßµAttention the IR community! The era of cheap UNSUPERVISED domain adaptation has begun! Let me introduce the InPars-Light training recipe enabling a small MiniLM model (with 30M parameters) to consistently outperform BM25 on all datasets used in the original InPars study." / Twitter
https://twitter.com/cwolferesearch/status/1612099963013529609 | Cameron R. Wolfe on Twitter: "OPT-IML (just released by @MetaAI) is a version of OPT-175B that's fine-tuned on a benchmark (OPT-IML Bench) of ~2000 instruction-based tasks. Prior work indicated that instruction-tuning is useful for LLMs, but we learn more details about this with OPT-IML... üßµ[1/8]" / Twitter
https://github.com/facebookresearch/metaseq/tree/main/projects/OPT-IML | metaseq/projects/OPT-IML at main ¬∑ facebookresearch/metaseq
https://twitter.com/weijiavxu/status/1616504852447694861 | Weijia Xu on Twitter: "[1/6] What is an NMT model "thinking" when it hallucinates? Are there any internal symptoms that may flag a hallucination? Our new paper "Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection" is out! https://t.co/Wjhfx2XHUP https://t.co/tmhHNlYQbm" / Twitter
https://twitter.com/EzraJNewman/status/1618628985096933377 | Ezra Newman on Twitter: "Almost all human reasoning is next token prediction. This is not a bit. I think this might be true." / Twitter
https://twitter.com/ebugliarello/status/1547741535382294531 | Emanuele Bugliarello | ebugliarello@sigmoid.social on Twitter: "Tired of tokenizers/subwords? Check out PIXEL, a new language model that processes written text as imagesüì∏ ‚ÄúLanguage Modelling with Pixels‚Äù üìÑ https://t.co/pmp7Yvhx9W üßë‚Äçüíªhttps://t.co/RbMemZOpub ü§ñhttps://t.co/J80eju62eB by @rust_phillip @jonasflotz me @esalesk @mdlhx @delliott https://t.co/6nzqlpoPMU" / Twitter
https://twitter.com/realDanFu/status/1617605971395891201 | Dan Fu on Twitter: "Attention is all you need... but how much of it do you need? Announcing H3 - a new generative language models that outperforms GPT-Neo-2.7B with only *2* attention layers! Accepted as a *spotlight* at #ICLR2023! üì£ w/ @tri_dao üìú https://t.co/vKdOTCH8Lk 1/n" / Twitter
https://twitter.com/suchenzang/status/1617093563061522432 | Susan Zhang on Twitter: "Piling on to the pile-on (sorry - it's always easy to criticize üòõ), here's a rant about benchmarks for LLMs that are used to back claims of "stronger" or "better" models. Let's start with a tour through GPT-3's Appendix G... 1/8" / Twitter
https://twitter.com/omarsar0/status/1620090029451403264 | elvis on Twitter: "LLMs still struggle with complex reasoning. Chain-of-thought prompting has shown potential on tasks such as arithmetic and commonsense reasoning. ThoughtSource provides datasets and tools for CoT reasoning in LLMs. https://t.co/OtfynefjLS https://t.co/e2ZilCSaSv" / Twitter
https://twitter.com/peterjliu/status/1620157580114030592 | Peter J. Liu on Twitter: "As generative language models hit production, there‚Äôs increased risk from bad outputs. It‚Äôs useful to know when to *not* show the outputs to the user, or defer to better, larger models (at the cost of compute). A üßµon an ICLR 2023 paper from Google. (1/n)" / Twitter
https://twitter.com/mathemagic1an/status/1620111511321710592 | Jay Hack on Twitter: "What if you could fit an *entire codebase* in an LLM? ü§î "Efficiently Scaling Transformer Inference" (11/2022) https://t.co/OsNPRwkfC8 Jeff Dean + co break out all the hacks to scale PALM-540B's context length to 43,000 tokens! Here's how üëá https://t.co/FkfSqi0R1Q" / Twitter
https://twitter.com/cwolferesearch/status/1613643034717028352 | Cameron R. Wolfe on Twitter: "Large Language Models (LLMs) have the potential to be incredibly useful, but they also make a lot of mistakes (e.g., by generating false or biased information). To eliminate this behavior, recent generations of LLMs utilize a two-part refinement process‚Ä¶ üßµ [1/10]" / Twitter
https://github.com/stanford-futuredata/ColBERT | stanford-futuredata/ColBERT: ColBERT: state-of-the-art neural search (SIGIR'20, TACL'21, NeurIPS'21, NAACL'22, CIKM'22)
https://twitter.com/yoavgo/status/1617970474088288256 | (((ŸÑ()(ŸÑ() 'yoav))))üëæ on Twitter: ""science and engineering, 2023‚Äù https://t.co/OFen6PNjso" / Twitter
https://github.com/Xpitfire/symbolicai | Xpitfire/symbolicai: Compositional Differentiable Programming Library
https://arxiv.org/pdf/2301.04272.pdf | Data Distillation A Survey - Arxiv-2301.04272

https://shamulent.github.io/Lectures/Lecture6_annotated.pdf | Lecture6_annotated.pdf
https://www.bilibili.com/video/BV1zb4y1Q7Js/?from=search&seid=14499920823264205383&spm_id_from=333.337.0.0&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | „ÄêÂÆåÁªì„ÄëÊûÅ‰πêËø™ÊñØÁßë-ÊúÄÁªàÂâ™ËæëÁâà ÂÖ®ÊµÅÁ®ã Êó†Ëß£ËØ¥ „ÄêÂÖ®ÈõÜ„Äë_ÂìîÂì©ÂìîÂì©_bilibili
https://shamulent.github.io/CS_Stat184_Fall22.html | CS/Stat 184 Intro to RL

https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Unified_Visual-Semantic_Embeddings_Bridging_Vision_and_Language_With_Structured_Meaning_CVPR_2019_paper.pdf | Unified Visual-Semantic Embeddings Bridging Vision and Language With Structured Meaning Representations - CVPR-2019_65564619
https://scholar.google.com/citations?user=-xaOIZIAAAAJ&hl=en | ‚Ä™Jiayuan Mao‚Ä¨ - ‚Ä™Google Scholar‚Ä¨
https://jiayuanm.com/ | Jiayuan Mao
https://arxiv.org/pdf/1906.02890.pdf | Visually Grounded Neural Syntax Acquisition - Arxiv-1906.02890
https://people.csail.mit.edu/tommi/ | Tommi Jaakkola
https://arxiv.org/pdf/1606.02447.pdf | Learning Language Games through Interaction - Arxiv-1606.02447
https://arxiv.org/pdf/1904.05521v1.pdf | UniVSE Robust Visual Semantic Embeddings via Structured Semantic Representations - Arxiv-1904.05521

https://www.youtube.com/watch?v=5Zk8eHxmql8&t=2379s | (631) Jiayuan Mao - Neuro-Symbolic Frameworks for Visual Concept Learning and Language Acquisition - YouTube
https://pdsketch.csail.mit.edu/data/papers/2022NeurIPS-PDSketch.pdf | 2022NeurIPS-PDSketch.pdf
https://arxiv.org/pdf/2203.16639.pdf | FALCON Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations - Arxiv-2203.16639
https://arxiv.org/pdf/1906.02890.pdf | Visually Grounded Neural Syntax Acquisition - Arxiv-1906.02890
https://arxiv.org/pdf/2202.08806.pdf | Grammar-Based Grounded Lexicon Learning - Arxiv-2202.08806

https://rltheorybook.github.io/rltheorybook_AJKS.pdf | rltheorybook_AJKS.pdf
http://incompleteideas.net/book/bookdraft2017nov5.pdf | bookdraft2017nov5.pdf
https://www.di.ens.fr/~fbach/ | Francis Bach - INRIA - ENS - PSL
https://mjt.cs.illinois.edu/dlt/#rademacher-complexity | Deep learning theory lecture notes
https://transformer-circuits.pub/ | Transformer Circuits Thread
http://www.cs.toronto.edu/~avner/teaching/S6-2414/ | Metrc Embeddings (CSC 2414H), Spring 2006: Home Page
https://zhuanlan.zhihu.com/p/36699314 | Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision - Áü•‰πé
https://zhuanlan.zhihu.com/p/22513016 | ËØª„ÄäNeural Turing Machines„Äã - Áü•‰πé
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=n_ts4eYAAAAJ&sortby=pubdate&citation_for_view=n_ts4eYAAAAJ:TQgYirikUcIC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=n_ts4eYAAAAJ&sortby=pubdate&citation_for_view=n_ts4eYAAAAJ:R3hNpaxXUhUC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=n_ts4eYAAAAJ&sortby=pubdate&citation_for_view=n_ts4eYAAAAJ:HDshCWvjkbEC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=n_ts4eYAAAAJ&sortby=pubdate&citation_for_view=n_ts4eYAAAAJ:mB3voiENLucC | View article
https://arxiv.org/abs/2203.01146 | Controlling the Focus of Pretrained Language Generation Models - Arxiv-2203.01146
https://proceedings.neurips.cc/paper/2021/file/dd17e652cd2a08fdb8bf7f68e2ad3814-Paper.pdf | Sequence-to-Sequence Learning with Latent Neural Grammars - NeurIPS-2021_dd17e652
https://neurips2022-enlsp.github.io/papers/paper_27.pdf | paper_27.pdf
https://arxiv.org/pdf/2212.09140.pdf | Unsupervised Discontinuous Constituency Parsing with Mildly Context-Sensitive Grammars - Arxiv-2212.09140

https://openreview.net/forum?id=rg-zrfteOZc | Reasoning with Transformer-based Models Deep Learning, but Shallow Reasoning - OR-AKBC-2021_Ozp1WrgtF5_
https://worksheets.codalab.org/worksheets/0x2b314dc3536b482dbba02783a24719fd/ | CodaLab Worksheets
https://arxiv.org/pdf/2105.08127.pdf | Finding an Unsupervised Image Segmenter in Each of Your Deep Generative Models - Arxiv-2105.08127
https://openreview.net/pdf?id=nUmCcZ5RKF | IS SYNTHETIC DATA FROM GENERATIVE MODELS READY FOR IMAGE RECOGNITION? - OR-ICLR-2023_nUmCcZ5RKF
https://openreview.net/forum?id=nUmCcZ5RKF | IS SYNTHETIC DATA FROM GENERATIVE MODELS READY FOR IMAGE RECOGNITION? - OR-ICLR-2023_nUmCcZ5RKF
https://arxiv.org/pdf/2104.14294.pdf | Emerging Properties in Self-Supervised Vision Transformers - Arxiv-2104.14294
https://www.google.com/search?q=%E9%87%91%E6%9E%9D | ÈáëÊûù - Google Search

https://www.danfu.org/ | Dan Fu - Stanford University
https://cs.stanford.edu/people/chrismre/ | Homepage of Christopher Re (Chris Re)
https://arxiv.org/pdf/2212.14052
https://arxiv.org/abs/2301.07014 | Dataset Distillation A Comprehensive Review - Arxiv-2301.07014
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LIjnUGgAAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=LIjnUGgAAAAJ:z_wVstp3MssC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LIjnUGgAAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=LIjnUGgAAAAJ:Fu2w8maKXqMC | View article
https://arxiv.org/pdf/2012.07463.pdf | https://arxiv.org/pdf/2012.07463.pdf
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LIjnUGgAAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=LIjnUGgAAAAJ:ZuybSZzF8UAC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LIjnUGgAAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=LIjnUGgAAAAJ:hkOj_22Ku90C | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LIjnUGgAAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=LIjnUGgAAAAJ:LjlpjdlvIbIC | View article
https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LIjnUGgAAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=LIjnUGgAAAAJ:J-pR_7NvFogC | View article

https://arxiv.org/abs/2204.09664 | Deep Learning meets Nonparametric Regression Are Weight-Decayed DNNs Locally Adaptive? - Arxiv-2204.09664

http://people.umass.edu/klement/tlp/tlp.html | Tractatus Logico-Philosophicus | Side-by-side-by-side edition
https://gligen.github.io/ | GLIGEN:Open-Set Grounded Text-to-Image Generation.
https://huggingface.co/spaces/gligen/demo | Demo - a Hugging Face Space by gligen
https://arxiv.org/pdf/2301.07093.pdf | GLIGEN Open-Set Grounded Text-to-Image Generation - Arxiv-2301.07093
https://phillipi.github.io/6.s898/ | 6.S898 Deep Learning, Fall 2022
https://phillipi.github.io/6.s898/materials/slides/13_rep_learning_theory.pdf | 13_rep_learning_theory
https://www.wikiwand.com/en/The_Idiot | The Idiot - Wikiwand

https://arxiv.org/pdf/2209.12711.pdf | Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts - Arxiv-2209.12711
https://github.com/facebookresearch/NPM | facebookresearch/NPM: The original implementation of Min et al. "Nonparametric Masked Language Modeling" (paper https//arxiv.org/abs/2212.01349)
https://www.wikiwand.com/en/William_James | William James - Wikiwand
https://www.reddit.com/r/cioran/comments/sx0kc1/i_want_to_get_into_cioran_i_enjoyed_kafkas/ | (3) I want to get into Cioran, I enjoyed Kafka's aphorisms the most of any philosophical texts I've read, where shoudl I start? : cioran
https://www.google.com/search?q=kafka+all+aphorisms&newwindow=1&sxsrf=AJOqlzVvvnrragkQYp1Zpln91qvUyUAOsw%3A1674234680412&ei=OMvKY8bYGLSg5NoP_NCQ2AU&ved=0ahUKEwjG1JWb0tb8AhU0EFkFHXwoBFsQ4dUDCBA&uact=5&oq=kafka+all+aphorisms&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIFCCEQoAE6CggAEEcQ1gQQsAM6BAgjECc6EQguEIMBEMcBELEDENEDEIAEOgUILhCABDoNCC4QsQMQxwEQ0QMQCjoECC4QQzoLCC4QsQMQxwEQ0QM6CAguEIAEELEDOhEILhCABBCxAxCDARDHARDRAzoFCAAQgAQ6CwgAELEDEIMBEJECOgUIABCRAjoNCC4QgAQQFBCHAhCxAzoLCAAQgAQQsQMQgwE6CwguEIMBELEDEIAEOggIABCABBCxAzoOCC4QsQMQgwEQxwEQrwE6CAguEIAEENQCOgsILhCABBCxAxCDAToICC4QgwEQsQM6DQgAEIAEEBQQhwIQsQM6BQguEJECOggIABCABBDLAToFCAAQhgM6BggAEBYQHkoECEEYAEoECEYYAFCuB1i4L2DKMGgEcAF4AIABoAGIAYESkgEENi4xNJgBAKABAcgBCMABAQ&sclient=gws-wiz-serp | kafka all aphorisms - Google Search
https://press.princeton.edu/books/hardcover/9780691205922/the-aphorisms-of-franz-kafka | The Aphorisms of Franz Kafka | Princeton University Press
http://zurauaphorisms.blogspot.com/ | Zurau Aphorisms
https://app.yinxiang.com/Home.action#n=7bb066d2-da7f-4294-86be-ee24e46cb7bf&s=s70&b=b5787273-91e4-4403-ad4f-eaa924cbe09e&ses=4&sh=1&sds=5& | Âà§ÂÜ≥ | Âç∞Ë±°Á¨îËÆ∞ÁΩëÈ°µÁâà
https://tieba.baidu.com/p/1246919155?pn=1 | Âç°Â§´Âç°Á¨îËÆ∞ÂèäÁÆ¥Ë®ÄË°•ÂÖÖ_Âç°Â§´Âç°Âêß_ÁôæÂ∫¶Ë¥¥Âêß
https://www.douban.com/group/topic/3489955/?_i=4235368KLQjbnS | ÈôÑÂç°Â§´Âç°Êó•ËÆ∞
http://miniyuan.com/simple/?t1013.html | Âç°Â§´Âç°ÔºöÁÆ¥Ë®ÄÂΩï‚Äî‚ÄîÂØπÁΩ™ÊÑÜ„ÄÅËã¶Èöæ„ÄÅÂ∏åÊúõÂíåÁúüÊ≠£ÁöÑÈÅìË∑ØÁöÑËßÇÂØü | ËØó‰∏éËØóÂ≠¶ - ÂÖÉÁü• - Powered by PHPWind
https://www.kafka-online.info/a-report-for-an-academy.html | A Report for An Academy by Franz Kafka

https://arxiv.org/pdf/2210.06726.pdf | Explanations from Large Language Models Make Small Reasoners Better - Arxiv-2210.06726
https://plato.stanford.edu/entries/ryle/ | Gilbert Ryle (Stanford Encyclopedia of Philosophy)
https://arxiv.org/pdf/2212.10001.pdf | Towards Understanding Chain-of-Thought Prompting An Empirical Study of What Matters - Arxiv-2212.10001
https://arxiv.org/pdf/2212.09865.pdf | Z-ICL Zero-Shot In-Context Learning with Pseudo-Demonstrations - Arxiv-2212.09865
https://arxiv.org/pdf/2210.12517.pdf | Exploring The Landscape of Distributional Robustness for Question Answering Models - Arxiv-2210.12517
https://arxiv.org/pdf/2205.12507.pdf | Re-Examining Calibration The Case of Question Answering - Arxiv-2205.12507
https://arxiv.org/abs/2110.08387 | Generated Knowledge Prompting for Commonsense Reasoning - Arxiv-2110.08387
https://arxiv.org/abs/2108.04106 | Noisy Channel Language Model Prompting for Few-Shot Text Classification - Arxiv-2108.04106
https://arxiv.org/abs/2111.02080 | An Explanation of In-context Learning as Implicit Bayesian Inference - Arxiv-2111.02080
https://arxiv.org/pdf/2210.03350.pdf | Measuring and Narrowing the Compositionality Gap in Language Models - Arxiv-2210.03350

https://arxiv.org/pdf/2301.00234.pdf | A Survey for In-context Learning - Arxiv-2301.00234

https://openaccess.thecvf.com/content/CVPR2022/papers/Wortsman_Robust_Fine-Tuning_of_Zero-Shot_Models_CVPR_2022_paper.pdf | Robust Fine-Tuning of Zero-Shot Models - CVPR-2022_23206643
https://arxiv.org/pdf/2104.07885.pdf | Probing Across Time What Does RoBERTa Know and When? - Arxiv-2104.07885
https://arxiv.org/pdf/2110.08387.pdf | Generated Knowledge Prompting for Commonsense Reasoning - Arxiv-2110.08387
https://arxiv.org/pdf/2211.09260.pdf | Task-aware Retrieval with Instructions - Arxiv-2211.09260
https://aclanthology.org/2022.findings-acl.50.pdf | 2022.findings-acl.50.pdf
https://arxiv.org/pdf/2002.06305.pdf | Fine-Tuning Pretrained Language Models Weight Initializations, Data Orders, and Early Stopping - Arxiv-2002.06305
https://danielkhashabi.com/files/2022_super_natural_instructions/nvidia-superni-talk.pdf | nvidia-superni-talk
https://aclanthology.org/2021.naacl-main.422.pdf | Probing Contextual Language Models for Common Ground with Visual Representations - ACL-NAACL-2021_2021.naacl-main.422
https://arxiv.org/pdf/2011.07127.pdf | IIRC A Dataset of Incomplete Information Reading Comprehension Questions - Arxiv-2011.07127
https://arxiv.org/pdf/2212.09865.pdf | Z-ICL Zero-Shot In-Context Learning with Pseudo-Demonstrations - Arxiv-2212.09865

https://arxiv.org/pdf/2205.05638.pdf | Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning - Arxiv-2205.05638
https://github.com/Shivanshu-Gupta | Shivanshu-Gupta (Shivanshu Gupta)
https://proceedings.neurips.cc/paper/2021/file/009c434cab57de48a31f6b669e7ba266-Paper.pdf | Counterfactual Explanations Can Be Manipulated - NeurIPS-2021_009c434c
https://arxiv.org/pdf/2109.05052.pdf | Entity-Based Knowledge Conflicts in Question Answering - Arxiv-2109.05052
https://arxiv.org/pdf/2202.07206.pdf | Impact of Pretraining Term Frequencies on Few-Shot Reasoning - Arxiv-2202.07206
https://arxiv.org/abs/2207.00747 | Rationale-Augmented Ensembles in Language Models - Arxiv-2207.00747
https://movie.douban.com/subject/35876302/?source=2022_annual_movie | ÊôíÂêéÂÅáÊó• (Ë±ÜÁì£)
https://movie.douban.com/subject/35769174/?source=2022_annual_movie | ‰∏áÊπñ‰ºöËÆÆ (Ë±ÜÁì£)
https://movie.douban.com/subject/35354759/?source=2022_annual_movie | Â∑¥ÈªéÂ§úÊóÖ‰∫∫ (Ë±ÜÁì£)
https://movie.douban.com/subject/3042261/?source=2022_annual_movie | Ë•øÁ∫øÊó†Êàò‰∫ã (Ë±ÜÁì£)
https://movie.douban.com/subject/35242938/?source=2022_annual_movie | ÁÄëÂ∏É (Ë±ÜÁì£)
https://movie.douban.com/subject/35371261/ | ÈìÉËäΩ‰πãÊóÖ (Ë±ÜÁì£)
https://movie.douban.com/subject/2237378/ | Âú®‰∏ñÁïåÂ∞ΩÂ§¥Áõ∏ÈÅá (Ë±ÜÁì£)
https://movie.douban.com/subject/1299661/ | ËãèÂ∑ûÊ≤≥ (Ë±ÜÁì£)
https://movie.douban.com/subject/1301912/ | ÁßãÂ§©ÁöÑÁ´•ËØù (Ë±ÜÁì£)
https://movie.douban.com/subject/1305164/ | ÁîúËúúËúú (Ë±ÜÁì£)
https://movie.douban.com/subject/25814705/ | Â∞èÊ£ÆÊûó Â§èÁßãÁØá (Ë±ÜÁì£)
https://movie.douban.com/subject/1299054/ | È¶ôËçâÁöÑÂ§©Á©∫ (Ë±ÜÁì£)
https://movie.douban.com/subject/1789283/ | Êó∂Á©∫Á∫øÁ¥¢ (Ë±ÜÁì£)
https://movie.douban.com/subject/33400376/?from=subject-page | Âπ≥Âéü‰∏äÁöÑÂ§èÊ¥õÂÖã (Ë±ÜÁì£)
https://movie.douban.com/subject/26576995/ | ÂêåÊ≠• (Ë±ÜÁì£)
https://movie.douban.com/subject/26269726/ | Êó†Â∞Ω (Ë±ÜÁì£)
https://movie.douban.com/subject/1295815/ | ÈÇ£Âπ¥Â§èÂ§©ÔºåÂÆÅÈùôÁöÑÊµ∑ (Ë±ÜÁì£)
https://movie.douban.com/subject/35652715/?source=2021_annual_movie | Êù∞‰ºä¬∑ÊØîÂßÜ (Ë±ÜÁì£)
https://movie.douban.com/subject/34805873/?source=2021_annual_movie | Â≠§Âë≥ (Ë±ÜÁì£)
https://movie.douban.com/subject/34850598/?source=2021_annual_movie | Êó†Â£∞ (Ë±ÜÁì£)
https://movie.douban.com/subject/1292275/ | ÁΩóÊãâÂø´Ë∑ë (Ë±ÜÁì£)
https://movie.douban.com/subject/26790580/ | Ëè≤Âà©ÊôÆ¬∑Ëø™ÂÖãÁöÑÁîµÂ≠êÊ¢¶ (Ë±ÜÁì£)
https://movie.douban.com/subject/26353671/ | ÊòéÊó•ÊàòËÆ∞ (Ë±ÜÁì£)
https://movie.douban.com/subject/34963486/ | ÁîµÂΩ±‰πãÁ•û (Ë±ÜÁì£)
https://movie.douban.com/subject/1793909/ | È¢ÑËßÅÊú™Êù• (Ë±ÜÁì£)
https://book.douban.com/subject/34934119/ | ÊùÉÂäõÁöÑÁúºÁùõ (Ë±ÜÁì£)
https://movie.douban.com/subject/1307200/ | Êú∫ÈÅá‰πãÊ≠å (Ë±ÜÁì£)
https://movie.douban.com/subject/35715638/ | Êó∂Èó¥‰ºöËÆÆ (Ë±ÜÁì£)
https://movie.douban.com/subject/30387441/ | Â§©ÊñπÂºÇË∞à (Ë±ÜÁì£)
https://book.douban.com/subject/11599245/ | ÈáèÂ≠êÊ±üÊπñ¬∑ÁáïÂ≠êÂùûÔºà‰∏äÔºâ (Ë±ÜÁì£)
https://movie.douban.com/subject/1308993/ | ÁÅµÂπªÂ§πÂÖã (Ë±ÜÁì£)
https://movie.douban.com/subject/3813779/ | ÂâëÈõ® (Ë±ÜÁì£)
https://book.douban.com/subject/24935042/ | Êó∂Èó¥‰πãÂ¢ü (Ë±ÜÁì£)
https://book.douban.com/subject/35218578/ | ‰∏ÉÂõΩÈì∂Ê≤≥ (Ë±ÜÁì£)

https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247541344&idx=2&sn=8a3e3f02b82fe89cce8c940a5acf4dba&exportkey=n_ChQIAhIQTqvEYqSlT9A7LDjZytRiQxKWAgIE97dBBAEAAAAAAG1hJPwqrnkAAAAOpnltbLcz9gKNyK89dVj0fnW0%2Fwakqf0gKRNuG1hNvg4yIuFdmuiI95dNPOZ%2FJ9joCOvy5WAzG4mngwFrl8S%2BxKNixlVq95SB2aAibtsEYKtqbt11XM0QAXnIXUvna9gCtHVAFgGWkZ7Jpsd7LcJiGvui1Frxh%2FG0B%2Bg8I6Qx3gkkI2e27ixWJnw2RwCNvVh1I5TMl47ZKl1tePVDrzAltTzaiHc7eA7zmZWJSEdrwV%2Fcm2reIYqBjaYTs2kzTLRYm7guMwKIEJeBaITjkhNAeHCejVb5U4ivFLFvmsIPEaKnEEOz4St5ZB0AgdaheybZXgBYIVU1Hl0TDS1lmSVT&acctmode=0&pass_ticket=3HIlKyjzTAFL%2FxFljj2tEtyLfkTobYOBU23FblbSLUp8Vu1SdLzUzUslMYnl2lP3NKfoplIeqIBS54Iibm%2FChw%3D%3D&wx_header=0 | ‰∏ÄÂè•ËØù‰øÆÂõæÊó∂‰ª£Êù•‰∫ÜÔºÅË∞∑Ê≠åÊñ∞Ê®°ÂûãÁ¢æÂéãDALLE¬∑2ÂíåImagen
https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652281804&idx=5&sn=d8314b372447232920af9406f22fec6d&exportkey=n_ChQIAhIQTSZ7LNbEkDg7CPjDj89fWhKWAgIE97dBBAEAAAAAAKHkFBKiBT8AAAAOpnltbLcz9gKNyK89dVj0rwXOVow%2BTokElchimHSFig7afNUP0rOBOhWYkBEYbxRh%2BhQozb2jxSZOycl30vyr4ILjxP22vdwg1Pq7VQuHELPj1Xvh5Rn7KbfEJa%2F%2FzdD2cNA4NRpCPQDeCPhTqtX%2B9xeVgoumuyc9L80OOqua2Bl2zkpqC06GSw4%2BL1%2BiiOD%2BwglaPvvVdTslsJUKiSGejFVOhxt%2FDMzEK8dcXBJjc333yzZqKsAsGsfVBLpsErBgGaBA13RgSfmqRl8QHgwO0GrypbUYW8YjIQDkniDVrt0Ci24qW8rNpkWA2%2BsgTTaEUcXUZhsfLbzZTYqrghoX&acctmode=0&pass_ticket=3HIlKyjzTAFL%2FxFljj2tEtyLfkTobYOBU23FblbSLUoOBbNUDxmZkfPNQYuQsky1hOlaOIRRZKNtxEF1pnL3Hw%3D%3D&wx_header=0 | ‰∏çË¶Åthink step by stepÔºÅË∞∑Ê≠åÊúÄÊñ∞Ëá™ÁÑ∂ËØ≠Ë®ÄÊé®ÁêÜÁÆóÊ≥ïLAMBADAÔºö„ÄåÂèçÂêëÈìæÊé®ÁêÜ„ÄçÊâçÊòØÁ≠îÊ°à
https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650864860&idx=5&sn=0a35281f4335fd751099cca8548a8048&chksm=84e538a2b392b1b48a60ac2fb6c5023da30112c51c7c34848ebae905672afe259a3061721bc5&mpshare=1&scene=1&srcid=0116TfX12tQz0Ta30fXd5FYL&sharer_sharetime=1673842963015&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQ%2FJBMsjFi4m9MEi4uMB191xKWAgIE97dBBAEAAAAAAMq1JxAf7fYAAAAOpnltbLcz9gKNyK89dVj0POOsT8RU%2BUXt8FnqyeMPDoH4JTwn45mFbNNsxZP61jM8t1VebMCfu8psKk6QhsO%2BvhOBsRJG5uRXE8GXUj9HEIJOiemuXRt8HzubcmvyH12hXAYouTpNGomoTZP1ASdrznt9Ve194nMCdPE%2FxI4YFmfhO2WPFyqrlfxnlD7mWflZwgG%2BCgRSjso3fL4DNZ3YdSY5Kbzou7y4imDzNS3j3ryx4Smj4BXcZ7vTxd9j%2BijUwS%2B4DJVpTm9Dldv4M7H5quHFpHtud9L1T2gZug07dBH33Fv9yW3PLMuSZgZnLJeGi3q5a%2FtCI9pmPVz%2BdFk2&acctmode=0&pass_ticket=3HIlKyjzTAFL%2FxFljj2tEtyLfkTobYOBU23FblbSLUoESymFRz29uU1%2FRX7h6w2gnrjMHpQi8dKB1psxTVfoFA%3D%3D&wx_header=0#rd | 7 Papers & Radios | Êó†ÈúÄÊ≥®ÊÑèÂäõÁöÑÈ¢ÑËÆ≠ÁªÉÔºõË¢´GPTÂ∏¶È£ûÁöÑIn-Context Learning
https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652270885&idx=4&sn=0232d96036808b701d6c28c49ee50d8d&exportkey=n_ChQIAhIQEuq9NZ7HGXSEHZW4Ool20RKWAgIE97dBBAEAAAAAACu%2FCkw9QjAAAAAOpnltbLcz9gKNyK89dVj0uPYtdDJEuklrmT3ERHHWEPmtgMRjdlDgROzIuuSXCFcQCsPQK13bDmvu27PjE7byY7FkQq%2F6zWNJjXPbPBBcVpW0jxc14jVrfMDZ3XQBEOPlfkb9AH5%2FrtG1ZGJLgbiyByATNTAuPsriqflHDKdoEuj%2FfJdKpbi6tBWFJL0NU9JBATfl3DQAl5JnJxgVGX4llotlDtqDSfSV45fR5snf7u0TUDbJxyjIa2obk79iA%2BQvxECAKXxuDS2y%2By4j%2Fxjk4ERDQAFVYoJ9mqWcJo9GZrHzXjupgKSTbl8Ofj50NENCbXKXbRRTJdlnow62M4ap&acctmode=0&pass_ticket=3HIlKyjzTAFL%2FxFljj2tEtyLfkTobYOBU23FblbSLUovf3ue%2BRPJG7MUeTbcw7WZzRDq3lq4p6HKylx4LkngeQ%3D%3D&wx_header=0 | MetaÂèëÂ∏ÉÈ¶ñ‰∏™„ÄåÈùûÂèÇÊï∞Âåñ„ÄçÊé©Á†ÅËØ≠Ë®ÄÊ®°ÂûãNPMÔºöÂêäÊâì500ÂÄçÂèÇÊï∞ÈáèÁöÑGPT-3
https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247492344&idx=1&sn=aeebe09529a9bd531249d6cdcd5a771f&exportkey=n_ChQIAhIQvU2bxWX5fMwQeblGWDIKoBKWAgIE97dBBAEAAAAAAAsCBOdq8GcAAAAOpnltbLcz9gKNyK89dVj0i6%2FseyLQ4pEXKuqV3uTfzjDy4Ne4380HnoSJ7LZDdMTtSbglamTzzNZPO9Yz%2BzKshTV0j0spcA4To4bqoCzFbuRs5oZnXz8%2FuzVbNuyK%2B5pkFmsZdIjeZNnaAwDVSgmR4uJs6coS2LR4PBjZWx9Afsb6Vv63hFRU5Hcd%2BLkM4Nnp7OEx%2F0o0ok%2FlfIU9opIL2jXUYuRx%2FFATissF1rb%2BUaaFq2KpI%2FJ4KkPrFBiaDC4xromXAGFmJhc5KZW2xotknSxiQagkGNt41ilB%2BE6Kuqhjc5usDxwfjDDToUBDhSnX8q9rs1HorWjtrcYQoVW6&acctmode=0&pass_ticket=3HIlKyjzTAFL%2FxFljj2tEtyLfkTobYOBU23FblbSLUpSDf1wOvjeezXbG9ET1fGmpu4cuIfoXjSTPxsQpWO56g%3D%3D&wx_header=0 | In-Context LearningÁé©Ê≥ïÂ§ßÂÖ®
https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650864545&idx=2&sn=cba397f55eb950da0fd794ae67c5c050&chksm=84e53fdfb392b6c9e2a7e360ac6d61c174691bdfdf5641f25d6caeb6adca80c5e99b50651b67&mpshare=1&scene=1&srcid=011696retelBJro4j4DxJL0z&sharer_sharetime=1673842926153&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQyKpMkoydhR1O%2BR%2F5UIzinxKWAgIE97dBBAEAAAAAAKwuJ5WrLbsAAAAOpnltbLcz9gKNyK89dVj0tmoDHRRqo5kCj1Qv6bIEUtpG4xo7EVvv3iX%2Fb%2F8ozFmXeTvuvF9AaSh9kW6WsLa%2FWB%2B76MBcF60gaqPc4l0N3blv5CteML7PYB3LTqPUyuDkoY%2Fvefn87kOdnxmJ0LiWlaaiInoQ02OR4Nexh5xgpBwqkk7iQ4zBSz97fqoxYOKDWc6G8Wz0oouXZsBZtmjIAQQAJGP1GVpBVQMG17nU4cBfribHme7ypaBgS62buxbF0aBdf7rl%2BWvaCgmOcbJ1GOLTfAoqR6jCXE3wSNqND2ULHi70SLiZfmqkBiTNld8UpSlna9WMJtSwAGeaCUr%2B&acctmode=0&pass_ticket=3HIlKyjzTAFL%2FxFljj2tEtyLfkTobYOBU23FblbSLUrq2L6n%2FO86L57931qjUdyQl6P4jx9HpLS6%2FHWr%2BWDmUQ%3D%3D&wx_header=0#rd | Ë¢´GPTÂ∏¶È£ûÁöÑIn-Context Learning‰∏∫‰ªÄ‰πàËµ∑‰ΩúÁî®ÔºüÊ®°ÂûãÂú®ÁßòÂØÜÊâßË°åÊ¢ØÂ∫¶‰∏ãÈôç
https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650864503&idx=1&sn=ce2ee0ef67b5dbb3a5d317876fe6c4ee&chksm=84e53f09b392b61fd147a882c62182fc763c5a3b277088034bee6f7fe7f688f326cee51d05d5&mpshare=1&scene=1&srcid=0116IbftODk0hCiCpDtSZbCW&sharer_sharetime=1673842915752&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQNxXIXhvBYUSYqYAIcEMMGRKWAgIE97dBBAEAAAAAAHfeIrYeodAAAAAOpnltbLcz9gKNyK89dVj0FRXdhxZDWMBJ7xf76hktqTo5Mpc2kfAJ%2B4BXC3zFWKTYtba9X8ERBv4D89j7ztJFvs14NSAntweIt3IDMeXb00l%2BqXFVh6tCIGXFpuPKTL2hFB29bPs4HY8peLBZMxf%2BkZ7Jl%2BsUgcRJbwRTA7W7FhNPdEvwoQXXxzuZ4yF21bhyBjGCl9hkA96JzgMIO5zPyDC8b0LlxLePuuVeRQ%2FODNnJ2z%2FuN%2FM3lRjGsUWFSAQaOp1Gd31DH7Mn%2BEB8JBpZb%2BwQmQ7jSQeVYTw6R2YAjgTDtCNhikrHGW0cy%2Fe7DovtC391XqM9Prj46rfuYkSA&acctmode=0&pass_ticket=3HIlKyjzTAFL%2FxFljj2tEtyLfkTobYOBU23FblbSLUryiXQKeukyC1UTESVF%2FQzmPV58UkgRAhOEPkiHbp%2ByKg%3D%3D&wx_header=0#rd | 2022Âá∫ÂúàÁöÑMLÁ†îÁ©∂ÔºöÁàÜÁÅ´ÁöÑStable Diffusion„ÄÅÈÄöÊâçÊô∫ËÉΩ‰ΩìGatoÔºåLeCunËΩ¨Êé®
https://docs.google.com/document/u/0/ | Google ÊñáÊ°£
https://arxiv.org/pdf/1909.01492.pdf | Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation - Arxiv-1909.01492

https://probml.github.io/pml-book/book2.html | https://probml.github.io/pml-book/book2.html
https://twitter.com/astonzhangAZ/status/1611400421255557122 | https://twitter.com/astonzhangAZ/status/1611400421255557122
https://twitter.com/cwolferesearch/status/1612099963013529609 | (2) Cameron R. Wolfe on Twitter: "OPT-IML (just released by @MetaAI) is a version of OPT-175B that's fine-tuned on a benchmark (OPT-IML Bench) of ~2000 instruction-based tasks. Prior work indicated that instruction-tuning is useful for LLMs, but we learn more details about this with OPT-IML... üßµ[1/8]" / Twitter
https://plato.stanford.edu/entries/wittgenstein/ | Ludwig Wittgenstein (Stanford Encyclopedia of Philosophy)
https://arxiv.org/pdf/2003.05997.pdf | Efficient Content-Based Sparse Attention with Routing Transformers - Arxiv-2003.05997
https://arxiv.org/pdf/2108.12284.pdf | The Devil is in the Detail Simple Tricks Improve Systematic Generalization of Transformers - ACL-EMNLP-2021_2021.emnlp-main.49
https://arxiv.org/abs/2212.13894 | LAMBADA Backward Chaining for Automated Reasoning in Natural Language - Arxiv-2212.13894
https://arxiv.org/pdf/2101.00010.pdf | UnNatural Language Inference - Arxiv-2101.00010
https://arxiv.org/pdf/2205.11482.pdf | Tracing Knowledge in Language Models Back to the Training Data - Arxiv-2205.11482
https://arxiv.org/pdf/2301.02828.pdf | Why do Nearest Neighbor Language Models Work? - Arxiv-2301.02828
https://openreview.net/pdf?id=-h6WAS6eE4 | pdf

https://book.douban.com/subject/35778212/ | „É´„Éº„É†„É°„Ç§„Éà„Å®Ë¨éËß£„Åç„Çí (Ë±ÜÁì£)
https://book.douban.com/subject/36180916/ | The Red Death Murders (Ë±ÜÁì£)
https://book.douban.com/subject/35380220/ | „Ç¢„É≥„Éá„ÉÉ„Éâ„Ç¨„Éº„É´„Éª„Éû„Éº„ÉÄ„Éº„Éï„Ç°„É´„Çπ 3 (Ë±ÜÁì£)
https://book.douban.com/subject/35223121/ | ÂÇçËÅ¥ËÄÖ (Ë±ÜÁì£)
https://book.douban.com/subject/35581093/ | ÊåáÂàá„Çä„Éë„Ç∫„É´ (Ë±ÜÁì£)
https://book.douban.com/subject/35806504/ | ‰ø∫„Åß„ÅØ„Å™„ÅÑÁÇé‰∏ä (Ë±ÜÁì£)
https://book.douban.com/subject/35152470/ | „Ç∏„Éß„É≥„Éª„Éá„Ç£„ÇØ„Çπ„É≥„Éª„Ç´„Éº„ÅÆÊúÄÁµÇÂÆöÁêÜ (Ë±ÜÁì£)
https://book.douban.com/subject/35982719/ | ÁÅ∞„Åã„Å∂„Çä„ÅÆÂ§ïÊµ∑ (Ë±ÜÁì£)
https://book.douban.com/subject/35216978/ | Êè∫Á±†„ÅÆ„Ç¢„Éá„Ç£„Éù„ÇØ„É´ (Ë±ÜÁì£)
https://book.douban.com/subject/5309519/ | Z Is for Zombie (Ë±ÜÁì£)
https://book.douban.com/subject/36159115/ | ÂØÜÂÆ§ÁãÇ‰π±ÊôÇ‰ª£„ÅÆÊÆ∫‰∫∫ (Ë±ÜÁì£)
https://www.douban.com/search?q=11%E6%96%87%E5%AD%97%E3%81%AE%E6%AA%BB | ÊêúÁ¥¢: 11ÊñáÂ≠ó„ÅÆÊ™ª
https://www.douban.com/search?q=%E3%82%B5%E3%83%BC%E3%82%AB%E3%82%B9%E3%81%8B%E3%82%89%E6%9D%A5%E3%81%9F%E5%9F%B7%E9%81%94%E5%90%8F | ÊêúÁ¥¢: „Çµ„Éº„Ç´„Çπ„Åã„ÇâÊù•„ÅüÂü∑ÈÅîÂêè
https://www.douban.com/search?q=Butterfly%20World%20%E6%9C%80%E5%BE%8C%E3%81%AE%E5%85%AD%E6%97%A5 | ÊêúÁ¥¢: Butterfly World ÊúÄÂæå„ÅÆÂÖ≠Êó•
https://www.douban.com/search?q=The%20Mask%20of%20the%20Vampire | ÊêúÁ¥¢: The Mask of the Vampire
https://www.douban.com/search?q=%E6%8E%A8%E7%90%86%E5%A4%A7%E6%88%A6 | ÊêúÁ¥¢: Êé®ÁêÜÂ§ßÊà¶

https://movie.douban.com/subject/3073124/ | ÊúàÁêÉ (Ë±ÜÁì£)
https://movie.douban.com/subject/1361276/ | ÂÖ´ÈÉ®Âçä (Ë±ÜÁì£)
https://movie.douban.com/subject/26799731/ | ËØ∑‰ª•‰Ω†ÁöÑÂêçÂ≠óÂëºÂî§Êàë (Ë±ÜÁì£)
https://movie.douban.com/subject/2222996/ | Ê≠•Â±•‰∏çÂÅú (Ë±ÜÁì£)
https://ai.papers.bar/paper/8b992d3e4b6c11ed92e7bfd3ad86b7ef | Diffusion models as plug-and-play priors
https://arxiv.org/abs/2202.02435 | On Neural Differential Equations - Arxiv-2202.02435
https://arxiv.org/abs/2301.01821 | Parameter-Efficient Fine-Tuning Design Spaces - Arxiv-2301.01821
https://lilianweng.github.io/posts/2023-01-10-inference-optimization/ | Large Transformer Model Inference Optimization | Lil'Log
https://arxiv.org/abs/2212.04488 | Multi-Concept Customization of Text-to-Image Diffusion - Arxiv-2212.04488
https://twitter.com/giffmana/status/1608568387583737856 | https://twitter.com/giffmana/status/1608568387583737856
https://twitter.com/Hidenori8Tanaka/status/1613561620999077889 | https://twitter.com/Hidenori8Tanaka/status/1613561620999077889
https://twitter.com/hardmaru/status/1611237067589095425 | https://twitter.com/hardmaru/status/1611237067589095425
https://wandb.ai/carperai/summarize_RLHF/reports/Implementing-RLHF-Learning-to-Summarize-with-trlX--VmlldzozMzAwODM2 | Implementing RLHF: Learning to Summarize with trlX ‚Äì Weights & Biases
https://arxiv.org/abs/2212.05015 | Robustness Implies Privacy in Statistical Estimation - Arxiv-2212.05015
https://arxiv.org/abs/2301.02241 | CiT Curation in Training for Effective Vision-Language Data - Arxiv-2301.02241
https://arxiv.org/abs/2212.14024 | Demonstrate-Search-Predict Composing retrieval and language models for knowledge-intensive NLP - Arxiv-2212.14024
https://arxiv.org/abs/2301.01947 | StitchNet Composing Neural Networks from Pre-Trained Fragments - Arxiv-2301.01947
https://arxiv.org/abs/2210.09276 | Imagic Text-Based Real Image Editing with Diffusion Models - Arxiv-2210.09276
https://www.youtube.com/watch?v=1aXOXHA7Jcw | (600) Greg Yang | Large N Limits: Random Matrices & Neural Networks | The Cartesian Cafe w/ Timothy Nguyen - YouTube
https://aclanthology.org/P18-1008/ | The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation - ACL Anthology
https://twitter.com/omarsar0/status/1607080018546417665?s=20&t=CdsM6CNvFJRvaABUZ5Bj8w | (3) elvis on Twitter: "2022: A Year in Review (ML Papers Edition) In this thread, let's take a look at some of the top trending ML papers of 2022 ‚Üì https://t.co/E4VIuF23HX" / Twitter
https://huggingface.co/blog/clipseg-zero-shot | Zero-shot image segmentation with CLIPSeg

https://www.99csw.com/book/10538/index.htm | Êé®ÁêÜË¶ÅÂú®Êú¨Ê†ºÂâç_Ë∞∑Â¥éÊ∂¶‰∏ÄÈÉé Ëä•Â∑ùÈæô‰πã‰ªã Ê¢¶Èáé‰πÖ‰Ωú_Âú®Á∫øÈòÖËØª_‰πù‰πùËóè‰π¶ÁΩë
https://plato.stanford.edu/entries/montague-semantics/ | Montague Semantics (Stanford Encyclopedia of Philosophy)
https://arxiv.org/pdf/1812.06834.pdf | A Tutorial on Deep Latent Variable Models of Natural Language - Arxiv-1812.06834

https://arxiv.org/pdf/2102.07350.pdf | Prompt Programming for Large Language Models Beyond the Few-Shot Paradigm - ACM-2021_10114534117633451760
https://openreview.net/pdf?id=yf1icZHC-l9 | pdf
https://github.com/Timothyxxx/Chain-of-ThoughtsPapers | Timothyxxx/Chain-of-ThoughtsPapers: A trend starts from "Chain of Thought Prompting Elicits Reasoning in Large Language Models".
https://arxiv.org/pdf/2206.02336.pdf | On the Advance of Making Language Models Better Reasoners - Arxiv-2206.02336
https://github.com/WENGSYX/Self-Verification | WENGSYX/Self-Verification
https://arxiv.org/pdf/2212.09561.pdf | Large Language Models are reasoners with Self-Verification - Arxiv-2212.09561
https://github.com/amazon-science/auto-cot | amazon-science/auto-cot: Official implementation for "Automatic Chain of Thought Prompting in Large Language Models" (stay tuned & more will be updated)
https://arxiv.org/pdf/2212.10403.pdf | Towards Reasoning in Large Language Models A Survey - Arxiv-2212.10403
https://arxiv.org/abs/2205.12393 | Fine-tuned Language Models are Continual Learners - Arxiv-2205.12393
https://arxiv.org/pdf/2212.10923.pdf | Language Models as Inductive Reasoners - Arxiv-2212.10923
https://github.com/jeffhj/LM-reasoning | jeffhj/LM-reasoning: This repository contains a collection of papers and resources on Reasoning in Large Language Models.
https://arxiv.org/pdf/2210.11610.pdf | Large Language Models Can Self-Improve - Arxiv-2210.11610
https://openreview.net/pdf?id=5NTt8GFjUHkr | pdf
https://arxiv.org/pdf/2211.10435.pdf | PAL Program-aided Language Models - Arxiv-2211.10435
https://arxiv.org/pdf/2210.07128.pdf | Language Models of Code are Few-Shot Commonsense Learners - Arxiv-2210.07128
https://arxiv.org/pdf/2210.03493.pdf | Automatic Chain of Thought Prompting in Large Language Models - Arxiv-2210.03493
https://arxiv.org/pdf/2208.10760.pdf | How good are deep models in understanding the generated images? - Arxiv-2208.10760
https://arxiv.org/pdf/2203.14465.pdf | STaR Bootstrapping Reasoning With Reasoning - Arxiv-2203.14465
https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html | In-context Learning and Induction Heads
https://arxiv.org/abs/2010.03648 | A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks - Arxiv-2010.03648
https://arxiv.org/pdf/2111.02080.pdf | An Explanation of In-context Learning as Implicit Bayesian Inference - Arxiv-2111.02080
https://proceedings.neurips.cc/paper/2021/hash/86b3e165b8154656a71ffe8a327ded7d-Abstract.html | Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning - NeurIPS-2021_86b3e165
https://arxiv.org/pdf/2205.05055.pdf | Data Distributional Properties Drive Emergent In-Context Learning in Transformers - Arxiv-2205.05055
https://www.google.com/search?q=predictability+and+surprise+in+large+generative+models&oq=Predictability+and+Surprise+in+Large+Generative+Models&aqs=chrome.0.35i39j0i390l3j69i61j69i60.2084j0j1&sourceid=chrome&ie=UTF-8 | predictability and surprise in large generative models - Google Search
https://arxiv.org/pdf/2104.08691.pdf | The Power of Scale for Parameter-Efficient Prompt Tuning - Arxiv-2104.08691
https://arxiv.org/pdf/1906.02361.pdf | Explain Yourself! Leveraging Language Models for Commonsense Reasoning - Arxiv-1906.02361
https://github.com/HazyResearch/ama_prompting | HazyResearch/ama_prompting: Ask Me Anything language model prompting
https://reasonwithpal.com/ | PAL: Program-aided Language Models
https://github.com/reasoning-machines/pal/blob/main/pal/prompt/math_prompts.py | pal/math_prompts.py at main ¬∑ reasoning-machines/pal

https://www.douban.com/note/833554818/?_i=3221336KLQjbnS | „ÄêËá™Áøª„Äë„ÄêÊ≠¶Áî∞Áª´‰πÉ„ÄëÊº†ÁÑ∂‰∏é‰∫î‰ΩìÔºàÈáçÂèëËØïËØïÔºâ
https://www.google.com/search?q=%E5%B0%8F%E5%8C%85%E5%AD%90%E5%92%8CJumbo | Â∞èÂåÖÂ≠êÂíåJumbo - Google Search
https://www.google.com/search?q=%E9%80%81%E8%91%AC%E5%88%97%E8%BD%A6 | ÈÄÅËë¨ÂàóËΩ¶ - Google Search
https://www.google.com/search?q=%E6%B2%89%E7%9D%A1%E7%9A%84%E5%90%8D%E4%BE%A6%E6%8E%A2%E4%B8%8E%E9%9B%B7%E7%94%B5%E5%AF%86%E5%AE%A4 | Ê≤âÁù°ÁöÑÂêç‰æ¶Êé¢‰∏éÈõ∑ÁîµÂØÜÂÆ§ - Google Search
https://www.google.com/search?q=%E6%96%A9%E9%A6%96T%E5%AD%97%E4%B9%8B%E8%B0%9C | Êñ©È¶ñTÂ≠ó‰πãË∞ú - Google Search
https://www.google.com/search?q=%E5%B0%8F%E6%9E%97%E6%B3%B0%E4%B8%89%E3%80%8Ajunk%E3%80%8B | Â∞èÊûóÊ≥∞‰∏â„Ääjunk„Äã - Google Search
https://www.google.com/search?q=%E4%BA%91%E4%B8%AD%E4%BA%BA | ‰∫ë‰∏≠‰∫∫ - Google Search
https://www.google.com/search?q=%E5%86%BB%E7%BB%93%E7%9A%84%E4%BF%84%E7%BD%97%E6%96%AF+%E6%A2%93%E5%B4%8E%E4%BC%98&newwindow=1&sxsrf=ALiCzsaW-G7Z4bmx_DHvIg80i_2Du1S0ew%3A1673220134952&ei=JlC7Y_DbOb6u5NoP2fS0wAo&ved=0ahUKEwjwhpnejrn8AhU-F1kFHVk6DagQ4dUDCBA&uact=5&oq=%E5%86%BB%E7%BB%93%E7%9A%84%E4%BF%84%E7%BD%97%E6%96%AF+%E6%A2%93%E5%B4%8E%E4%BC%98&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzoICAAQogQQsAM6BQgAEKIESgQIQRgBSgQIRhgAUF9YswJgnQdoAXAAeACAAWmIAbkBkgEDMS4xmAEAoAEBoAECyAEEwAEB&sclient=gws-wiz-serp | ÂÜªÁªìÁöÑ‰øÑÁΩóÊñØ Ê¢ìÂ¥é‰ºò - Google Search
https://www.google.com/search?q=%E8%BF%BD%E9%9A%8F%E5%A5%B9%E7%9A%84%E6%97%85%E7%A8%8B | ËøΩÈöèÂ•πÁöÑÊóÖÁ®ã - Google Search
https://www.google.com/search?q=%E5%B0%91%E5%B9%B4%E5%B7%B4%E6%AF%94%E4%BC%A6 | Â∞ëÂπ¥Â∑¥ÊØî‰º¶ - Google Search
https://www.google.com/search?q=%E9%A3%9F%E4%B9%8B%E4%BF%A1%E6%9D%A1 | È£ü‰πã‰ø°Êù° - Google Search
https://www.google.com/search?q=%E6%AD%A4%E3%81%AE%E4%B8%96%E3%81%AE%E6%9E%9C%E3%81%A6%E3%81%AE%E6%AE%BA%E4%BA%BA | Ê≠§„ÅÆ‰∏ñ„ÅÆÊûú„Å¶„ÅÆÊÆ∫‰∫∫ - Google Search

https://piazza.com/class/l75f9r92zbz4ha/post/349 | 6.7810 (142 unread)
https://math.stackexchange.com/questions/4611091/conditional-independence-property?noredirect=1#comment9718294_4611091 | probability - Conditional Independence property - Mathematics Stack Exchange
https://arxiv.org/pdf/2301.00774.pdf | Massive Language Models Can Be Accurately Pruned in One-Shot - Arxiv-2301.00774
https://arxiv.org/pdf/2105.03075.pdf | A Survey of Data Augmentation Approaches for NLP - Arxiv-2105.03075
https://arxiv.org/pdf/2108.08485.pdf | Language Model Augmented Relevance Score - Arxiv-2108.08485
https://github.com/apoorvumang/kgt5 | apoorvumang/kgt5: Sequence-to-Sequence Knowledge Graph Completion and Question Answering (KGT5)
https://arxiv.org/pdf/2210.14975.pdf | MABEL Attenuating Gender Bias using Textual Entailment Data - Arxiv-2210.14975
https://arxiv.org/pdf/2204.00408.pdf | Structured Pruning Learns Compact and Accurate Models - Arxiv-2204.00408
https://arxiv.org/pdf/2010.04762.pdf | Counterfactually-Augmented SNLI Training Data Does Not Yield Better Generalization Than Unaugmented Data - ACL-EMNLP| insights-2020_2020.insights-1.13
https://arxiv.org/abs/2210.01848 | Explaining Patterns in Data with Language Models via Interpretable Autoprompting - Arxiv-2210.01848
https://www.google.com/search?q=Robustness+of+Demonstration-based+Learning+Under+Limited+Data+Scenario&oq=Robustness+of+Demonstration-based+Learning+Under+Limited+Data+Scenario&aqs=chrome.0.69i59j69i61l2.188j0j1&sourceid=chrome&ie=UTF-8 | Robustness of Demonstration-based Learning Under Limited Data Scenario - Google Search
https://book.douban.com/subject/30448939/ | Â§úÂÖâ‰∏éÁã¨Ë°åËÄÖ (Ë±ÜÁì£)
https://ja.annas-archive.org/search?q=%E5%A4%9C%E5%85%89%E4%B8%8E%E7%8B%AC%E8%A1%8C%E8%80%85 | Â§úÂÖâ‰∏éÁã¨Ë°åËÄÖ - Search - „Ç¢„É≥„Éä„ÅÆ„Ç¢„Éº„Ç´„Ç§„Éñ
https://www.douban.com/group/topic/48243678/?_i=3216156KLQjbnS | [ÂéüÂàõÁøªËØë]„ÄäÊ≠ªÂàëÁäØ‰πãË∞ú „ÄãÊ≥ïÊúàÁ∫∂Â§™ÈÉéÔºàÂÆåÁªìÔºâ
https://arxiv.org/abs/2211.15029 | DiffusionBERT Improving Generative Masked Language Models with Diffusion Models - Arxiv-2211.15029
https://github.com/krasserm/bayesian-machine-learning | krasserm/bayesian-machine-learning: Notebooks about Bayesian methods for machine learning
https://arxiv.org/abs/2112.14757 | A Simple Baseline for Open-Vocabulary Semantic Segmentation with Pre-trained Vision-language Model - Arxiv-2112.14757
https://arxiv.org/abs/2107.06278#:~:text=Per%2DPixel%20Classification%20is%20Not%20All%20You%20Need%20for%20Semantic%20Segmentation,-Bowen%20Cheng%2C%20Alexander&text=Modern%20approaches%20typically%20formulate%20semantic,with%20an%20alternative%20mask%20classification. | Per-Pixel Classification is Not All You Need for Semantic Segmentation - Arxiv-2107.06278
https://arxiv.org/abs/1707.09835 | Meta-SGD Learning to Learn Quickly for Few-Shot Learning - Arxiv-1707.09835
https://arxiv.org/abs/1907.10529 | SpanBERT Improving Pre-training by Representing and Predicting Spans - Arxiv-1907.10529

https://arxiv.org/abs/2301.00303 | Rethinking with Retrieval Faithful Large Language Model Inference - Arxiv-2301.00303
https://arxiv.org/abs/2208.01066 | What Can Transformers Learn In-Context? A Case Study of Simple Function Classes - Arxiv-2208.01066
https://arxiv.org/abs/1905.02175 | Adversarial Examples Are Not Bugs, They Are Features - Arxiv-1905.02175
https://arxiv.org/abs/2112.01008 | Editing a classifier by rewriting its prediction rules - Arxiv-2112.01008
https://aclanthology.org/2022.acl-long.269/ | Is Attention Explanation? An Introduction to the Debate - ACL Anthology
https://arxiv.org/abs/2301.01379 | A Succinct Summary of Reinforcement Learning - Arxiv-2301.01379
http://proceedings.mlr.press/v139/liu21f/liu21f.pdf | Just Train Twice Improving Group Robustness without Training Group Information - PMLR-2021-liu21f
http://proceedings.mlr.press/v139/miller21b/miller21b.pdf | Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization
https://arxiv.org/pdf/1911.08731.pdf | Distributionally Robust Neural Networks for Group Shifts On the Importance of Regularization for Worst-Case Generalization - Arxiv-1911.08731
https://www.google.com/search?q=scalable+differential+privacy+with+certified+robustness+in+adversarial+learning&oq=scalable+differen&aqs=chrome.0.35i39j69i57j0i512j0i15i22i30j0i22i30l2j69i61l2.4136j0j1&sourceid=chrome&ie=UTF-8 | scalable differential privacy with certified robustness in adversarial learning - Google Search
https://www.google.com/search?q=a+unified+approach+to+interpreting+model+predictions&oq=a+unified+approach&aqs=chrome.0.0i20i263i512j69i57j0i512l8.4177j0j1&sourceid=chrome&ie=UTF-8 | a unified approach to interpreting model predictions - Google Search
https://arxiv.org/abs/1705.07874 | A Unified Approach to Interpreting Model Predictions - Arxiv-1705.07874
https://arxiv.org/abs/1711.11279 | Interpretability Beyond Feature Attribution Quantitative Testing with Concept Activation Vectors (TCAV) - Arxiv-1711.11279
https://www.google.com/search?q=snorkel+paper&oq=snorkel+paper&aqs=chrome..69i57j0i15i22i30j0i22i30j0i390l5.2204j0j1&sourceid=chrome&ie=UTF-8 | snorkel paper - Google Search
https://www.google.com/search?q=deep+learning+with+label+differential+privacy&oq=deep+learning+with+label+d&aqs=chrome.0.0i512j69i57j0i22i30j69i61.2665j0j1&sourceid=chrome&ie=UTF-8 | deep learning with label differential privacy - Google Search
https://www.google.com/search?q=commonsense+knowledge+mining+from+pretrained+models&oq=commonsense+knowledge+mining&aqs=chrome.0.0i512j69i57.4632j0j1&sourceid=chrome&ie=UTF-8 | commonsense knowledge mining from pretrained models - Google Search

https://www.bilibili.com/video/BV1Ke4y1j737/?spm_id_from=333.1007.tianma.5-4-20.click&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | „ÄêÂÆåÁªì„ÄëÈõ®Ë°Ä | ÂÜ∑Èó®Ê≠¶‰æ†Á•û‰Ωú | Âè§ÈæôÂë≥ÈÅì | Â£∞Â£∞ÊµÅÁ®ãËß£ËØ¥„ÄêÁ≥ªÂàó‰∏âÈÉ®Êõ≤„Äë_ÂçïÊú∫Ê∏∏ÊàèÁÉ≠Èó®ËßÜÈ¢ë
https://arxiv.org/abs/2208.01618 | An Image is Worth One Word Personalizing Text-to-Image Generation using Textual Inversion - Arxiv-2208.01618
https://distill.pub/2021/understanding-gnns/ | Understanding Convolutions on Graphs
https://distill.pub/2021/gnn-intro/ | A Gentle Introduction to Graph Neural Networks
https://arxiv.org/pdf/2204.07697.pdf | Theory of Graph Neural Networks Representation and Learning - Arxiv-2204.07697
https://arxiv.org/abs/1710.10571v5 | Certifying Some Distributional Robustness with Principled Adversarial Training - Arxiv-1710.10571
https://arxiv.org/pdf/1811.08489.pdf | Balanced Datasets Are Not Enough Estimating and Mitigating Gender Bias in Deep Image Representations - Arxiv-1811.08489
https://huggingface.co/course/chapter7/6?fw=pt | Training a causal language model from scratch - Hugging Face Course
https://arxiv.org/pdf/1806.08010.pdf | Fairness Without Demographics in Repeated Loss Minimization - PMLR-2018-hashimoto18a
https://arxiv.org/pdf/1801.09344.pdf | Certified Defenses against Adversarial Examples - Arxiv-1801.09344
http://proceedings.mlr.press/v80/wong18a/wong18a.pdf | Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope - PMLR-2018-wong18a
https://github.com/stefanoteso/awesome-explanatory-supervision | stefanoteso/awesome-explanatory-supervision: List of relevant resources for machine learning from explanatory supervision
https://arxiv.org/pdf/2103.10773.pdf | UniMoCo Unsupervised, Semi-Supervised and Full-Supervised Visual Representation Learning - Arxiv-2103.10773
https://arxiv.org/pdf/2012.00893.pdf | Evaluating Explanations How much do explanations from the teacher aid students? - Arxiv-2012.00893
https://pgmpy.org/examples/Monty%20Hall%20Problem.html | 2. Monty Hall Problem ‚Äî pgmpy 0.1.19 documentation
https://arxiv.org/pdf/2009.06732.pdf | Efficient Transformers A Survey - Arxiv-2009.06732
https://arxiv.org/abs/2109.14119 | Stochastic Training is Not Necessary for Generalization - Arxiv-2109.14119
https://github.com/JonasGeiping/breaching | JonasGeiping/breaching: Breaching privacy in federated learning scenarios for vision and text
https://github.com/tuero/perturbations-differential-pytorch | tuero/perturbations-differential-pytorch: Differentiable Optimizers with Perturbations in Pytorch
https://github.com/google-research/google-research/tree/master/perturbations | google-research/perturbations at master ¬∑ google-research/google-research
http://cs.emory.edu/site/aims/pub/wang21naacl.pdf | wang21naacl.pdf
https://www.google.com/search?q=Learning%20with%20Differentiable%20Perturbed%20Optimizers | Learning with Differentiable Perturbed Optimizers - Google Search
https://www.zhihu.com/question/66794537 | Âç°Â∞îÁª¥ËØ∫ÁöÑ„ÄäÂàÜÊàê‰∏§ÂçäÁöÑÂ≠êÁàµ„ÄãÂ•ΩÂú®Âì™ÈáåÔºü - Áü•‰πé
https://arxiv.org/abs/2102.06701 | Explaining Neural Scaling Laws - Arxiv-2102.06701
https://arxiv.org/abs/2102.04074 | Learning Curve Theory - Arxiv-2102.04074

https://arxiv.org/abs/2204.05832 | What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization? - Arxiv-2204.05832
https://arxiv.org/pdf/2212.09282.pdf | APOLLO A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning - Arxiv-2212.09282
https://twitter.com/G_S_Bhogal/status/1608135379055480833 | (2) Gurwinder on Twitter: "In 2022 I learned hundreds of useful concepts that improved my understanding of the world. Here are the 10 best:" / Twitter
https://twitter.com/DrJimFan/status/1607746957753057280 | (2) Jim Fan on Twitter: "The AI explosion is warping our sense of time. Can you believe Stable Diffusion is only 4 months old, and ChatGPT &lt;4 weeks old ü§Ø? If you blink, you miss a whole new industry. Here are my TOP 10 AI spotlights, from a breathtaking 2022 in rewind ‚èÆ: a long thread üßµ https://t.co/5k8Q6VQ0tD" / Twitter
https://twitter.com/larsiusprime/status/1605765732968677378 | (2) Lars "Land is a Big Deal" Doucet on Twitter: "@ojoshe @Noahpinion @emollick @delong Per your example about a child growing up; I would slightly caveat that human brains have been genetically "pretrained" with built in grain hardware capabilities primed for language acquisition, ie pretrained on the conversations of one's countless ancestors" / Twitter
https://arxiv.org/abs/2203.03466v2 | Tensor Programs V Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer - Arxiv-2203.03466

https://arxiv.org/abs/2212.03714 | Reconstructing Training Data from Model Gradient, Provably - Arxiv-2212.03714
https://arxiv.org/abs/2201.12675 | Decepticons Corrupted Transformers Breach Privacy in Federated Learning for Language Models - Arxiv-2201.12675
https://yaofu.notion.site/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f | A Closer Look at Large Language Models Emergent Abilities
https://arxiv.org/abs/2212.14024v1 | Demonstrate-Search-Predict Composing retrieval and language models for knowledge-intensive NLP - Arxiv-2212.14024
https://arxiv.org/abs/2205.05055 | Data Distributional Properties Drive Emergent In-Context Learning in Transformers - Arxiv-2205.05055
https://arxiv.org/abs/2207.10551 | Scaling Laws vs Model Architectures How does Inductive Bias Influence Scaling? - Arxiv-2207.10551
https://aps.arxiv.org/abs/2212.12131 | Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times? - Arxiv-2212.12131
http://www.cs.tau.ac.il/~oriram/projections.pdf | projections.pdf
https://arxiv.org/pdf/2212.10947.pdf | Parallel Context Windows Improve In-Context Learning of Large Language Models - Arxiv-2212.10947
https://arxiv.org/abs/2211.09260 | Task-aware Retrieval with Instructions - Arxiv-2211.09260
https://instructor-embedding.github.io/ | Instructor Text Embedding
https://arxiv.org/abs/2212.10560 | Self-Instruct Aligning Language Model with Self Generated Instructions - Arxiv-2212.10560
https://arxiv.org/abs/2212.09803 | Training Trajectories of Language Models Across Scales - Arxiv-2212.09803
https://arxiv.org/pdf/2212.09736.pdf | Don't Generate, Discriminate A Proposal for Grounding Language Models to Real-World Environments - Arxiv-2212.09736
https://arxiv.org/abs/2212.10466 | Controllable Text Generation with Language Constraints - Arxiv-2212.10466
https://arxiv.org/abs/2212.09849 | Dataless Knowledge Fusion by Merging Weights of Language Models - Arxiv-2212.09849
https://arxiv.org/pdf/2212.02027.pdf | Retrieval as Attention End-to-end Learning of Retrieval and Reading within a Single Transformer - Arxiv-2212.02027
https://www.google.com/search?q=Robustness+of+Demonstration-based+Learning+Under+Limited+Data+Scenario&oq=Robustness+of+Demonstration-based+Learning+Under+Limited+Data+Scenario&aqs=chrome..69i57j69i61l2.265j0j1&sourceid=chrome&ie=UTF-8 | Robustness of Demonstration-based Learning Under Limited Data Scenario - Google Search

https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf | Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss - NeurIPS-2019_621461af
https://openreview.net/pdf?id=AmUhwTOHgm | pdf
https://openreview.net/forum?id=fe2S7736sNS | $k$NN Prompting: Learning Beyond the Context with Nearest Neighbor Inference | OpenReview
https://proceedings.mlr.press/v162/rame22a/rame22a.pdf | Fishr Invariant Gradient Variances for Out-of-Distribution Generalization - PMLR-2022-rame22a
https://arxiv.org/pdf/2204.02937.pdf | Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations - Arxiv-2204.02937
https://svivek.com/research/publications/karidi2021putting.pdf | Putting Words in BERT's Mouth: Navigating Contextualized Vector Spaces with Pseudowords
https://arxiv.org/abs/2210.11560 | Finding Dataset Shortcuts with Grammar Induction - Arxiv-2210.11560
https://arxiv.org/abs/2210.14011 | Are All Spurious Features in Natural Language Alike? An Analysis through a Causal Lens - Arxiv-2210.14011
https://arxiv.org/pdf/2205.11432.pdf | Logical Reasoning with Span-Level Predictions for Interpretable and Robust NLI Models - Arxiv-2205.11432
https://arxiv.org/pdf/2210.17541.pdf | Zero-Shot Text Classification with Self-Training - Arxiv-2210.17541
https://www.eurecom.fr/en/publication/7095 | You are my type! Type embeddings for pre-trained language models | EURECOM
https://arxiv.org/pdf/2209.00840.pdf | FOLIO Natural Language Reasoning with First-Order Logic - Arxiv-2209.00840
https://arxiv.org/abs/2210.16637 | Beyond Prompting Making Pre-trained Language Models Better Zero-shot Learners by Clustering Representations - Arxiv-2210.16637
https://arxiv.org/abs/2205.12548 | RLPrompt Optimizing Discrete Text Prompts with Reinforcement Learning - Arxiv-2205.12548
https://arxiv.org/abs/2211.07830 | Prompting Language Models for Linguistic Structure - Arxiv-2211.07830
https://arxiv.org/pdf/2210.03575.pdf | Are Representations Built from the Ground Up? An Empirical Examination of Local Composition in Language Models - Arxiv-2210.03575
http://proceedings.mlr.press/v119/han20c/han20c.pdf | SIGUA Forgetting May Make Learning with Noisy Labels More Robust - PMLR-2020-han20c
https://arxiv.org/pdf/2208.05592.pdf | Patching open-vocabulary models by interpolating weights - Arxiv-2208.05592
https://www.google.com/search?q=empowering+language+modle&oq=empowering+language+modle&aqs=chrome..69i57j69i59j69i60l6.3325j0j1&sourceid=chrome&ie=UTF-8 | empowering language modle - Google Search
https://arxiv.org/abs/2211.08380 | Empowering Language Models with Knowledge Graph Reasoning for Question Answering - Arxiv-2211.08380
https://www.google.com/search?q=robust+curriculum+learning&oq=robust+curriculum+learning&aqs=chrome..69i57j0i390l4.3696j1j1&sourceid=chrome&ie=UTF-8 | robust curriculum learning - Google Search
https://openreview.net/forum?id=lmTWnm3coJJ | Robust Curriculum Learning: from clean label detection to noisy label self-correction | OpenReview
https://arxiv.org/abs/2210.10362 | CPL Counterfactual Prompt Learning for Vision and Language Models - Arxiv-2210.10362
https://arxiv.org/abs/2211.05110 | Large Language Models with Controllable Working Memory - Arxiv-2211.05110
https://arxiv.org/abs/2106.09129 | A Winning Hand Compressing Deep Networks Can Improve Out-Of-Distribution Robustness - Arxiv-2106.09129
https://arxiv.org/abs/2104.06644 | Masked Language Modeling and the Distributional Hypothesis Order Word Matters Pre-training for Little - Arxiv-2104.06644
https://arxiv.org/abs/2202.07206 | Impact of Pretraining Term Frequencies on Few-Shot Reasoning - Arxiv-2202.07206
https://arxiv.org/pdf/2110.11328.pdf | A Fine-Grained Analysis on Distribution Shift - Arxiv-2110.11328
https://arxiv.org/abs/2202.10054 | Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution - Arxiv-2202.10054
https://arxiv.org/abs/2002.08307 | Compressing BERT Studying the Effects of Weight Pruning on Transfer Learning - Arxiv-2002.08307
https://arxiv.org/abs/2204.01691 | Do As I Can, Not As I Say Grounding Language in Robotic Affordances - Arxiv-2204.01691
https://arxiv.org/abs/2204.04163 | Contextual Representation Learning beyond Masked Language Modeling - Arxiv-2204.04163
https://akariasai.github.io/files/evidentiality_arxiv_2021.pdf | evidentiality_arxiv_2021.pdf
https://arxiv.org/abs/2204.00511 | Learning Disentangled Representations of Negation and Uncertainty - Arxiv-2204.00511
https://arxiv.org/abs/2109.05602 | Good-Enough Example Extrapolation | Abstract
https://aclanthology.org/2021.acl-long.422/ | Counterfactual Inference for Text Classification Debiasing - ACL-ACL| IJCNLP-2021_2021.acl-long.422
https://openreview.net/forum?id=StHCELh9PVE | Analyzing Commonsense Emergence in Few-shot Knowledge Models | OpenReview
https://aclanthology.org/2021.emnlp-main.7/ | Raise a Child in Large Language Model Towards Effective and Generalizable Fine-tuning - ACL-EMNLP-2021_2021.emnlp-main.749
https://aclanthology.org/2021.emnlp-main.113.pdf | Shortcutted Commonsense: Data Spuriousness in Deep Learning of Commonsense Reasoning
https://arxiv.org/abs/2108.10604 | Prompt-Learning for Fine-Grained Entity Typing - Arxiv-2108.10604
https://www.google.com/search?q=%22Spurious+Correlations+in+Reference-Free+Evaluation+of+Text+Generation&oq=%22Spurious+Correlations+in+Reference-Free+Evaluation+of+Text+Generation&aqs=chrome..69i57j0i512.293j0j1&sourceid=chrome&ie=UTF-8 | "Spurious Correlations in Reference-Free Evaluation of Text Generation - Google Search
https://www.google.com/search?q=generating+data+to+mitigate+spurious+correlations+in+natural+language+inference+datasets&oq=Generating+Data+to+Mitigate+Spurious+Correlations+in+Natural+Language+Inference+Datasets&aqs=chrome.0.0i512.277j0j1&sourceid=chrome&ie=UTF-8 | generating data to mitigate spurious correlations in natural language inference datasets - Google Search
https://arxiv.org/pdf/2110.07736.pdf | Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models - Arxiv-2110.07736

https://arxiv.org/abs/2103.08933 | Reweighting Augmented Samples by Minimizing the Maximal Expected Loss - Arxiv-2103.08933
https://arxiv.org/abs/1908.06112 | Symmetric Cross Entropy for Robust Learning with Noisy Labels - Arxiv-1908.06112
https://arxiv.org/abs/2110.08387 | Generated Knowledge Prompting for Commonsense Reasoning - Arxiv-2110.08387
https://arxiv.org/abs/2205.12604 | Leveraging QA Datasets to Improve Generative Data Augmentation - Arxiv-2205.12604
https://arxiv.org/pdf/2205.12404.pdf | FLUTE Figurative Language Understanding through Textual Explanations - Arxiv-2205.12404
https://www.google.com/search?q=scalable+penalized+regression+for+noise+detection+in+learning+with+noisy+labels&oq=Scalable+Penalized+Regression+for+Noise+Detection+in+Learning+with+Noisy+Labels&aqs=chrome.0.0i512.289j0j1&sourceid=chrome&ie=UTF-8 | scalable penalized regression for noise detection in learning with noisy labels - Google Search
https://arxiv.org/abs/2203.07788 | Scalable Penalized Regression for Noise Detection in Learning with Noisy Labels - Arxiv-2203.07788
https://arxiv.org/abs/2210.15859 | You can't pick your neighbors, or can you? When and how to rely on retrieval in the $k$NN-LM - Arxiv-2210.15859
https://www.google.com/search?q=Uncertainty+Calibration+for+Ensemble-Based+Debiasing+Method&oq=Uncertainty+Calibration+for+Ensemble-Based+Debiasing+Method&aqs=chrome..69i57.262j0j1&sourceid=chrome&ie=UTF-8 | Uncertainty Calibration for Ensemble-Based Debiasing Method - Google Search
https://www.google.com/search?q=Can+Prompt+Probe+Pretrained+Language+Models%3F+Understanding+the+Invisible+Risks+from+a+Causal+View&oq=Can+Prompt+Probe+Pretrained+Language+Models%3F+Understanding+the+Invisible+Risks+from+a+Causal+View&aqs=chrome..69i57.346j0j1&sourceid=chrome&ie=UTF-8 | Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View - Google Search
https://www.google.com/search?q=owards+Interpreting+and+Mitigating+Shortcut+Learning+Behavior+of+NLU+Models&oq=owards+Interpreting+and+Mitigating+Shortcut+Learning+Behavior+of+NLU+Models&aqs=chrome..69i57j0i22i30l2.352j0j1&sourceid=chrome&ie=UTF-8 | owards Interpreting and Mitigating Shortcut Learning Behavior of NLU Models - Google Search
https://arxiv.org/abs/2103.06922 | Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU Models - Arxiv-2103.06922
https://www.google.com/search?q=PASTA%3A+Parameter-efficient+Tuning+with+Special+Token+Adaptation&oq=PASTA%3A+Parameter-efficient+Tuning+with+Special+Token+Adaptation&aqs=chrome..69i57j69i58.260j0j1&sourceid=chrome&ie=UTF-8 | PASTA: Parameter-efficient Tuning with Special Token Adaptation - Google Search
https://www.google.com/search?q=Can+NLI+Models+Verify+QA+Systems%E2%80%99+Predictions%3F&oq=Can+NLI+Models+Verify+QA+Systems%E2%80%99+Predictions%3F&aqs=chrome..69i57j0i22i30i625.266j0j1&sourceid=chrome&ie=UTF-8 | Can NLI Models Verify QA Systems‚Äô Predictions? - Google Search
https://arxiv.org/abs/2110.07904 | SPoT Better Frozen Model Adaptation through Soft Prompt Transfer - Arxiv-2110.07904
https://www.google.com/search?q=few+shot+learning+with+noisy+labels&oq=few+shot+learning+with+noisy+labels&aqs=chrome.0.0i512j0i22i30i625.3666j0j1&sourceid=chrome&ie=UTF-8 | few shot learning with noisy labels - Google Search
https://www.google.com/search?q=dversarial+Examples+for+Evaluating+Math+Word+Problem+Solvers&oq=dversarial+Examples+for+Evaluating+Math+Word+Problem+Solvers&aqs=chrome..69i57j33i299l2.390j0j1&sourceid=chrome&ie=UTF-8 | dversarial Examples for Evaluating Math Word Problem Solvers - Google Search
https://aclanthology.org/2021.findings-emnlp.230.pdf | Adversarial Examples for Evaluating Math Word Problem Solvers

https://neurips2022-enlsp.github.io/papers/paper_27.pdf | paper_27.pdf
https://arxiv.org/abs/2211.09748 | Probing for Incremental Parse States in Autoregressive Language Models - Arxiv-2211.09748
https://arxiv.org/abs/2211.07906 | Hierarchical Phrase-based Sequence-to-Sequence Learning - Arxiv-2211.07906
https://proceedings.mlr.press/v162/lang22a/lang22a.pdf | Co-training Improves Prompt-based Learning for Large Language Models
https://proceedings.neurips.cc/paper/2021/file/dd17e652cd2a08fdb8bf7f68e2ad3814-Paper.pdf | Sequence-to-Sequence Learning with Latent Neural Grammars - NeurIPS-2021_dd17e652
https://arxiv.org/pdf/2205.01464.pdf | Inducing and Using Alignments for Transition-based AMR Parsing - Arxiv-2205.01464
https://arxiv.org/pdf/2012.07463.pdf | Parameter-Efficient Transfer Learning with Diff Pruning - Arxiv-2012.07463
https://arxiv.org/pdf/2011.09039.pdf | Sequence-Level Mixed Sample Data Augmentation - Arxiv-2011.09039
https://arxiv.org/pdf/1904.03746.pdf | Unsupervised Recurrent Neural Network Grammars - Arxiv-1904.03746
http://proceedings.mlr.press/v89/dieng19a/dieng19a.pdf | Avoiding Latent Variable Collapse with Generative Skip Models - PMLR-2019-dieng19a
https://proceedings.neurips.cc/paper/2018/file/b691334ccf10d4ab144d672f7783c8a3-Paper.pdf | Latent Alignment and Variational Attention - NeurIPS-2018_b691334c
http://proceedings.mlr.press/v80/kim18e/kim18e.pdf | Semi-Amortized Variational Autoencoders - PMLR-2018-kim18e
http://proceedings.mlr.press/v80/zhao18b/zhao18b.pdf | zhao18b.pdf
https://www.google.com/search?q=AMR+parsing&newwindow=1&sxsrf=ALiCzsbn8WyPccVNDRkgNr8PP4HYGnUsaA%3A1672644505820&ei=mYeyY9TTMb6g5NoPgL2L-Ak&ved=0ahUKEwiUhOusrqj8AhU-EFkFHYDeAp8Q4dUDCBA&uact=5&oq=AMR+parsing&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIFCAAQgAQyBQgAEIAEMgUIABCABDIFCAAQgAQyBQgAEIAEMggIABCABBDLATIGCAAQFhAeMgYIABAWEB4yCQgAEBYQHhDxBDIGCAAQFhAeOgcIIxDqAhAnOg0IABCPARDqAhC0AhgBOg0ILhCPARDqAhC0AhgBOgQIIxAnOgQIABBDOgcILhDUAhBDOgUIABCRAjoQCC4QsQMQgwEQxwEQ0QMQQzoLCAAQgAQQsQMQgwE6EQguEIAEELEDEIMBEMcBENEDOgoILhDHARDRAxBDOgsILhCABBCxAxCDAToLCC4QgAQQxwEQrwE6CwguEIAEELEDENQCOg4ILhDHARCxAxDRAxCABDoOCC4QgAQQxwEQ0QMQ1AI6CAgAEIAEELEDOg4ILhCABBCxAxDHARDRAzoNCC4QgAQQxwEQ0QMQCjoFCAAQhgNKBAhBGAFKBAhGGAFQ4xpYjydgiyhoAnAAeACAAfMBiAGHC5IBBTQuNy4xmAEAoAEBsAEUwAEB2gEGCAEQARgK&sclient=gws-wiz-serp | AMR parsing - Google Search
https://openreview.net/pdf?id=NiEtU7blzN | pdf
https://openreview.net/forum?id=NiEtU7blzN | Large Language Models Can Self-improve | OpenReview
https://arxiv.org/pdf/2009.07118.pdf | It's Not Just Size That Matters Small Language Models Are Also Few-Shot Learners - Arxiv-2009.07118
https://arxiv.org/abs/2211.01910 | Large Language Models Are Human-Level Prompt Engineers - Arxiv-2211.01910
https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650860970&idx=1&sn=2a5548eaf927cdd10fe14c3b0e3a4420&chksm=84e529d4b392a0c2cc418e4f2a1702716bd9dd876d4adeb2cdee9e48c31940b1ff4bcfd278c5&mpshare=1&scene=1&srcid=1116KsrixVC7xbDzLWOzGAoc&sharer_sharetime=1668532797367&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQWfEPwrafmpqFT5ZY2b%2FZXBKWAgIE97dBBAEAAAAAALUuBZ7j1psAAAAOpnltbLcz9gKNyK89dVj0h4rTIVRFwA7QH7JNzsEPoLbqfbZdKQACuYCIdMM0ANMI9xhaQzCplVSeSfUCgWEqTAsG5s2SB8okU6pgBbvTR%2BI1F%2Fy26sp1aqwAahFsxQQLRvHWk24Sx8%2B%2F3J156Sz11V3yzXDFdyVn%2FOMTiqQfxgpRxY%2F7jM%2FzsH%2FVt9QsKNi%2BtlqvRQF6PGZPLJc1GoLz0EsgCgWJYDQspp20KtN8UrnMd9gZz5mN37PJ6WYLzwtbkVERyeoa5knwCzlC1CPBMfzws9Q%2BWAesd6Ee47ZGxaz8bFCGCtR2cZ7HLNV48qSYyD%2Fc2CVU6fZR8j99VSZn&acctmode=0&pass_ticket=zUvJ6Bt8HsTNAPBA6U4qajGZ5sn0Pt6WEGVtKUG1A6PRrzVvi9Ssje8SgNLIF2OW&wx_header=0#rd | AIËá™Âä®ÁîüÊàêpromptÂ™≤Áæé‰∫∫Á±ªÔºåÁΩëÂèãÔºöÂ∑•Á®ãÂ∏àÂàöË¢´ËÅòÁî®ÔºåÂèàË¶ÅÊ∑òÊ±∞‰∫Ü
https://github.com/thunlp/PromptPapers | thunlp/PromptPapers: Must-read papers on prompt-based tuning for pre-trained language models.
https://arxiv.org/pdf/2103.10385.pdf | GPT Understands, Too - Arxiv-2103.10385
https://github.com/THUDM/P-tuning | THUDM/P-tuning: A novel method to tune language models. Codes and datasets for paper ``GPT understands, too''.
https://zhuanlan.zhihu.com/p/366771566 | Prompt-based Language ModelsÔºöÊ®°ÁâàÂ¢ûÂº∫ËØ≠Ë®ÄÊ®°ÂûãÂ∞èÁªì - Áü•‰πé
https://zhuanlan.zhihu.com/p/488279606 | NLP PromptÁ≥ªÂàó‚Äî‚ÄîPrompt EngineeringÊñπÊ≥ïËØ¶ÁªÜÊ¢≥ÁêÜ - Áü•‰πé
https://www.zhihu.com/question/439114659/answer/2409287359 | NLP Èáå prompt engineering ÔºàËÆæËÆ°‰∏Ä‰∏™ÈóÆÈ¢òÁöÑËÉåÊôØÊèêÁ§∫ÔºâÊúâÂ§öÈáçË¶Å? - Áü•‰πé
https://www.zhihu.com/question/508658141 | NLP ‰∏≠ prompt learning ÊúâÂì™‰∫õÂèØËÉΩÁöÑÂ§©ÁîüÁº∫Èô∑ÔºüÁõÆÂâçÊúâ‰ªÄ‰πàÊ†∑ÁöÑÊñπÊ≥ïÊù•Ëß£ÂÜ≥ËøôÊ†∑ÁöÑÁº∫Èô∑Ôºü - Áü•‰πé

https://arxiv.org/pdf/1909.13375.pdf | A Simple and Effective Model for Answering Multi-span Questions - Arxiv-1909.13375
https://arxiv.org/pdf/2104.07478.pdf | Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations - Arxiv-2104.07478
https://www.zhihu.com/question/68730628 | Batch normalizationÂíåInstance normalizationÁöÑÂØπÊØîÔºü - Áü•‰πé
https://arxiv.org/pdf/2102.06062.pdf | Deep Learning with Label Differential Privacy - Arxiv-2102.06062
http://proceedings.mlr.press/v119/phan20a/phan20a.pdf | Scalable Differential Privacy with Certified Robustness in Adversarial Learning - PMLR-2020-phan20a
https://proceedings.mlr.press/v119/phan20a.html | Scalable Differential Privacy with Certified Robustness in Adversarial Learning - PMLR-2020-phan20a
https://arxiv.org/pdf/1902.02918.pdf) | Certified Adversarial Robustness via Randomized Smoothing - PMLR-2019-cohen19c
https://arxiv.org/abs/2003.03284 | TaskNorm Rethinking Batch Normalization for Meta-Learning - Arxiv-2003.03284
https://stats.stackexchange.com/questions/377287/need-help-understanding-the-benefit-of-score-function-estimator | reinforcement learning - need help understanding the benefit of score function estimator - Cross Validated

https://github.com/Yutong-Zhou-cv/Awesome-Text-to-Image | Yutong-Zhou-cv/Awesome-Text-to-Image: (‡∑Ü`Íí≥¬¥‡∑Ü) A Survey on Text-to-Image Generation/Synthesis.
https://hackmd.io/@prajwalsingh/imagesynthesis# | Survey Text Based Image Synthesis - HackMD
https://www.zhihu.com/people/FesianXu | ÂæêÂúüË±Ü - Áü•‰πé
https://mercurixito.github.io/ | Hako
https://arxiv.org/pdf/2102.12092.pdf | Zero-Shot Text-to-Image Generation | PDF
https://github.com/sheqi/GAN_Review | sheqi/GAN_Review: A Survey and Taxonomy of the Recent GANs DevelopmentÔºåcomputer vision & time series
https://github.com/CompVis/taming-transformers | CompVis/taming-transformers: Taming Transformers for High-Resolution Image Synthesis
https://www.zhihu.com/search?q=SeqGAN&type=content | SeqGAN - ÊêúÁ¥¢ÁªìÊûú - Áü•‰πé
https://arxiv.org/pdf/1806.02817.pdf | Probabilistic Model-Agnostic Meta-Learning | PDF

https://arxiv.org/pdf/1611.01734.pdf | Deep Biaffine Attention for Neural Dependency Parsing - Arxiv-1611.01734
https://github.com/ShannonAI/mrc-for-dependency-parsing | ShannonAI/mrc-for-dependency-parsing
https://arxiv.org/pdf/2105.07654.pdf | Dependency Parsing as MRC-based Span-Span Prediction - Arxiv-2105.07654

https://francisbach.com/the-gumbel-trick/ | The Gumbel trick ‚Äì Machine Learning Research Blog
https://bair.berkeley.edu/blog/2022/08/29/reverse-engineering/ | Reverse engineering the NTK: towards first-principles architecture design ‚Äì The Berkeley Artificial Intelligence Research Blog
https://iclr-blog-track.github.io/2022/03/25/unnormalized-resnets/ | Normalization is dead, long live normalization! ¬∑ The ICLR Blog Track
https://iclr-blog-track.github.io/blog/ | The ICLR Blog Track ¬∑
https://movie.douban.com/subject/30171424/?source=2021_annual_movie | ÊãÜÂºπ‰∏ìÂÆ∂2 (Ë±ÜÁì£)
https://www.douban.com/note/839749776/?_i=2431422KLQjbnS,2433879KLQjbnS | „ÄäËøáÂéª‰πã‰π¶„Äã„ÄÅ„ÄäÊú™Êù•‰πã‰π¶„ÄãÁÆÄËØÑ
https://proceedings.neurips.cc/paper/2020/file/322f62469c5e3c7dc3e58f5a4d1ea399-Paper.pdf | Bayesian Deep Learning and a Probabilistic Perspective of Generalization - NeurIPS-2020_322f6246
https://people.csail.mit.edu/ronitt/COURSE/F22/ | 6.5240 Sublinear Time Algorithms, Fall 2022
https://arxiv.org/pdf/2211.11719.pdf | First Steps Toward Understanding the Extrapolation of Nonlinear Models to Unseen Domains - Arxiv-2211.11719

https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247538592&idx=2&sn=3af58b8a69d3ac334a514f62ffbc4cc0&chksm=970f7f76a078f660f1a168d5302084fd5bd47a934f5e568b1746cc954bbeb027bf238009d767&mpshare=1&scene=1&srcid=1218pGLvdDyQbNtXim7P7tPh&sharer_sharetime=1671346345841&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQIv5oRiVsxKsBe%2FM%2F8a5pehKWAgIE97dBBAEAAAAAAP2tKTnkwfIAAAAOpnltbLcz9gKNyK89dVj0LFpb1pRiE0GDoI7IWxLeIF%2BB6LaijW4sBACAihH1pRA9AsdJ5j9aLP5vA0mQH1MsBRKceN7fH1TDNPXClyCYMG6tqlBOhAOD0q0waWf%2BwAjnuYUH8oYJDN9HRQikKDxBsPV4zoaPWqpMuPmEiixQwl6r2AButDrX047mH%2BcalhUH6pZHTA0gvwJ8Is1D75Q4e6u62r%2B6dSRMBWxEkqsS5OUNvpW%2BOgRw0bE%2BraxKhU2giNbudcL%2BXWcCmpRw8LXZxotEQfSYQoPux2UibPxX%2BZZeJE%2Fi7Mkdmmt%2BxJJkbtm4hxhD3TYRLFhA9I1oMH0l&acctmode=0&pass_ticket=B%2Fr2wa0S%2FM5lL%2FB24fD%2FIrSWC3GeT01IvggTYRYcwup5y6H%2Fp1m1UNUUaUC%2Fd0sL6rzQIBDg3kKPGV9TdODIAg%3D%3D&wx_header=0#rd | Â§ßÂπÖË∂ÖË∂äDALL¬∑E 2ÂíåImagenÔºåÊñØÂù¶Á¶èÂèëÂ∏ÉRA-CM3Ê®°ÂûãÔºåËûçÂêàÊ£ÄÁ¥¢‰∏éÁîüÊàê
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247537380&idx=2&sn=e39083ddd3ef4b7af88386761ef32380&chksm=970f7a32a078f324e79e3595feb25bd874c51add9bee3a84000b667e2a9475e116251869b6bf&mpshare=1&scene=1&srcid=1218axF8smChbSnHPgN06AXT&sharer_sharetime=1671346261151&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQ21kFn5QO3zq%2FIfeuaDoO3xKWAgIE97dBBAEAAAAAAGyxMN%2BZoP4AAAAOpnltbLcz9gKNyK89dVj0Yd06ao31f3X4bGtEc%2BzEB6ns80LartdjTdxQF5wTwDy20hJi4KWcjPBw6cHQ3GVlJ%2BflLdF%2FWryCUvJclD9BLFOH8j3jvCMJt4Kf7btqdIzhGPEog1GRQVBKE5043Qm8bZUjVyl2k20RBCmz4s1WGUsr5b0w6Ek5YMwAS%2F6%2B89EAjVTcH4qfEoUFCo128D%2FnvWxXAeGpwvVsbDduEBfp%2Ft6AEwNXIRIWQdwhFxt2SDW3if7kRRAVthtE4tU543ae0g1YJN532lS7uxi%2FuJnLNmwNI49kEVKiMKed5HKYbi%2BNlPCH5q2iNTkGSb8%2FLJTr&acctmode=0&pass_ticket=B%2Fr2wa0S%2FM5lL%2FB24fD%2FIrSWC3GeT01IvggTYRYcwupWc8AJ%2FN4acx237B3KDw7x1nmkCqb%2BsP5jDrU5l%2FHsoA%3D%3D&wx_header=0#rd | MITÂèëÁé∞ËØ≠Ë®ÄÊ®°ÂûãÂÜÖÁöÑ‰∫ãÂÆûÁü•ËØÜÂèØË¢´‰øÆÊîπÔºüÔºü
https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652259237&idx=1&sn=cb5cfcd242f6bd0db2e16039fb52643b&exportkey=n_ChQIAhIQgf%2BbYLwl6oMGw%2FoFoGNfLBKWAgIE97dBBAEAAAAAAO39JSP6k4EAAAAOpnltbLcz9gKNyK89dVj0ej6oJ%2BQLKMAE5bTxnV%2FSkAB6mtO1FPO3HNXyLGqrNfItyOWqw3limou79xA1BA7dJoNgYm0ptLc%2FsvnUWmDmyjU7d1gAerLAqzb0RjbdTMtTvWLiJ3XgbFLMk%2FI71eLEtWmudLslhaFJacH1FxwMqxUHzWfs3srXe9YrCLtyVDc3e3tjrkGUfKuLIDm9t9pxP3poXNuQhY1QIfqCYwMi%2F4YRiR5xUN1hKey42aHVjhJrl5hsVA6naAvRhNVwwwnXc%2F9SJpo%2BJkDN4rKnSH%2BwJIMtUtXqQHiP6IGDyaJkRtWL3ayuMKjf6INhokEMOebZ&acctmode=0&pass_ticket=B%2Fr2wa0S%2FM5lL%2FB24fD%2FIrSWC3GeT01IvggTYRYcwuqZeCg78PNJVjfkT8mPfGMrvEDF%2FnRd53u4VktMnA499A%3D%3D&wx_header=0 | AIGCÂ§ß‰∏ÄÁªüÊ®°ÂûãÊù•‰∫ÜÔºÅCVÁïåÊ≥∞ÊñóÈªÑÁÖ¶Ê∂õÂàõÁ´ãÂõ¢ÈòüÊèêÂá∫„ÄåÂÖ®ËÉΩDiffusion„Äç
https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247565529&idx=2&sn=2ebbcdee73318a925664a5db7ae9513e&chksm=f9a0b256ced73b4067f294041819e8b49f76d00ced442e9e8cb6479cb78aa53484492667308c&mpshare=1&scene=1&srcid=1218Xs2cfHnFPJf0nmsbiv4L&sharer_sharetime=1671346237329&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQWyKo5EJ9rIcEL0g2bh3bBhKWAgIE97dBBAEAAAAAAP%2FkKsKhABwAAAAOpnltbLcz9gKNyK89dVj0hLfpghxDTMMjptFK2oOMuYW6eLieNlQmpNPfvzEvRszlEWrrB%2BCykSWHVCKRCSsCXUlZZYShKvwrAw7UJeKyHiEz0V%2BNZcL6wL3vQZJ8HfSCepUxILyvEJdx9g6Klzezu%2BL8L8jEvrsnMHnoEz%2FQcTq2TMCXdFIczVpYeO1qAb6ieLhj9YIOGXkqmg0zckEbqLlK5uwqILgqe%2F1%2BVY1b9VMg%2BwvQq2Y8n8WA043qDzY3B%2FUFjTGh7ONJAGjbnFSypWE5OKy%2BaM9SSQRp1igGdv33I6hpiD2SPT%2BfyYGK44oamVyG7AoGxV8y0kww%2BlWL&acctmode=0&pass_ticket=B%2Fr2wa0S%2FM5lL%2FB24fD%2FIrSWC3GeT01IvggTYRYcwurD%2BsY4u%2Bo39nB6zH3MmZWD6TS0GPZhM5cL2jeJ87IpEA%3D%3D&wx_header=0#rd | NeurIPS 2022È´òÂàÜËÆ∫ÊñáÔºÅDeRyÔºöËÆ©Áü•ËØÜËøÅÁßªÂÉèÊãºÁßØÊú®‰∏ÄÊ†∑ÁÆÄÂçïÈ´òÊïàÔºÅ
https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247655026&idx=1&sn=6ab838f15974ba349caea39bf03450d0&chksm=e8de2900dfa9a016e01e465a4e02e685aaa7807993660defcd0874d3e4962abacb7676b30fe9&mpshare=1&scene=1&srcid=1218p4T6wCLNCEHIpjdjDJrl&sharer_sharetime=1671346229351&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQoxurM3%2Fu729dD0sjYOtIZhKWAgIE97dBBAEAAAAAAGSyACyoEg0AAAAOpnltbLcz9gKNyK89dVj0%2BGSargFudjg%2Bu2lywggNHvbiRFArlho12H1E5Il%2BlDyiuP61IvERvCoyvDhQxP5OjJiIcTWldmNArX32qmLfsTbhs4DpXpq%2Bo959H1mzfz1v3L2lyxR%2B2Oy8eWARxICWwcEld5JubufkmT6ri%2FfseRhRkzEVlKi9uU%2FpZ9EWMdaQczrdlChedIR2PIAwguk1MFIs3LAYMQ%2FhoP6qmfpOyikTOcW8s%2Fhy0Ck0bc4EbOhZKvhh507U%2BLjzrSCllZEEZOJf3E1ZsBxnnNCKUGrhykXg9uEPQ7FDx4j2g%2BpH%2Faan9d9%2F%2B35dovu%2FGwgX5DOr&acctmode=0&pass_ticket=B%2Fr2wa0S%2FM5lL%2FB24fD%2FIrSWC3GeT01IvggTYRYcwurTeFLLgFUN%2FX6lpucFhYNM3qSP4D2rlw4ndVtnva2kJA%3D%3D&wx_header=0#rd | ‰ΩïÊÅ∫ÊòéÂõ¢Èòü12È°µËÆ∫ÊñáÊñ∞‰ΩúÂâëÊåáAIGCÔºÅ‚ÄúÊñ∞CLIP‚ÄùÂè™ÈúÄ‰∏Ä‰∏™trickÔºåËÆ≠ÁªÉÈÄüÂ∫¶Âø´3.7ÂÄçÔºÅÊÄßËÉΩ‰∏çÈôçÂèçÂçá
https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247492157&idx=1&sn=e429832288f50701056dfbaddc84cb52&chksm=9bba6b59accde24f2333e4c5004d45803a5aca256c13ee9f36d40c9b2ea592b91522ca142bbd&mpshare=1&scene=1&srcid=1218DyBY4tX32tEMyCu5J2q2&sharer_sharetime=1671346219166&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQnoWWuaFpvsZSZXPZG7L3rBKWAgIE97dBBAEAAAAAAG95MhSFeYkAAAAOpnltbLcz9gKNyK89dVj0JaRzsFvQODKXQx1RT0ZnNnU6ZRDfUClVynHx3qx0a28%2B7VnxRFmDRZ%2FJPJAe83Hla0gcNb2acQBdqp4f439O4h5d5tyP4jet2KzZPeWWAXFvXUa%2B10g5suDMyZQGLNoBonMuYkayLr9n1VuEw0q36H6r9Evww2tZu1idCBv%2FVbYnUqsVVcDjO3ITMvjrog1NC%2BJ3xOAmXELWCuc9rzbQg2nBX1HgUmFvXRlirKy3gq2KEw9NseH0aoqGzVIOje6ovlWUpa6iRHtfpTlMU1ol7wZ%2F4FK0kusrrM2OPMmhrCM%2Bt7eCol2E2gISKsKUMBd3&acctmode=0&pass_ticket=B%2Fr2wa0S%2FM5lL%2FB24fD%2FIrSWC3GeT01IvggTYRYcwuoqrEEB65byq958NpPvNdBJa12vc2jRboP3Uoa8Dgn0NQ%3D%3D&wx_header=0#rd | ËØª‰∫Ü14ÁØáËÆ∫ÊñáÔºåÁªà‰∫é‰ºöÊãøÊçèDiffusion‰∫Ü
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247539037&idx=1&sn=e7214950088bb67210cfc356ad770c17&chksm=970f718ba078f89db9a19155b1a112cb6e425e7191a3a4999aa560b0f47f1a59c18da964f751&mpshare=1&scene=1&srcid=1218rcOan93DpU8Vvc3Ojum3&sharer_sharetime=1671346137769&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQs9SxE%2BQdpPGOSMrFFd0xPxKWAgIE97dBBAEAAAAAAEgyLmzBLUcAAAAOpnltbLcz9gKNyK89dVj0M2CtOBjpjUPagWPQECC1RBblws6Ten6W0n99v2zMc3lt%2BK8l2mG%2F45WTJnz37S%2B5nv%2F26WzMHhjWXVxy3LlnN4y4WK7jnFkn5GVo3UneBfDcXO3Xxj9BFBN4Ja4xo7Q0CMHvV5gntvjHHTXVSqXL49TLcM8P6c7T94iZ2zgdl2cK6kxHhMe1QycXh3PzFIR%2BFWMat3iBtZnsWPx1D7DeLdKQNtnzwM49tSwtv0%2BDQMsLQMICgxpYndeur6KKTUM3QJttD4BiLs0mAzCFbdYVUILLslPNN%2F%2BvstkpLi3PA32%2B1GZQalbKdUxEq6ZN8Mjt&acctmode=0&pass_ticket=B%2Fr2wa0S%2FM5lL%2FB24fD%2FIrSWC3GeT01IvggTYRYcwurIfNm7WOoqiOpddrAS3DLlwj1c8hYtXV9J7sTP2lwaxg%3D%3D&wx_header=0#rd | EMNLP 2022 ÊúÄ‰Ω≥ËÆ∫ÊñáÊè≠ÊôìÔºÅËøôËÑëÊ¥ûÁªù‰∫Ü‚Ä¶.
https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650864226&idx=3&sn=44a9b991a0eba9b16ca8fc575d9e3685&chksm=84e53e1cb392b70a7382611dee0cec478c1180f573013aadf2c086f35bdda6ea4c728b9edead&mpshare=1&scene=1&srcid=1224kJ13eUE0cNf2GNH9AT5u&sharer_sharetime=1671836770873&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQ0HpEpWPPojp71BqhZscKARKUAgIE97dBBAEAAAAAAGAFKyyeEGQAAAAOpnltbLcz9gKNyK89dVj0NXuugfCzE2O8UZH3z06gk9Ug2obhOOxzF%2FNn6NA%2BHa%2B%2BGxhG5Zye4PG8yss5WXq99b8ElQB8eEfxDojjXexz33q8iQA7yeYAEntpQy3xT9iwfIrtq4Lux1%2BH61Xw06xitYu%2Filu4Geww0h604oyF0cfxz1kIQcC4X40ajXQ28lmQn5%2BGxqo7cOQWzhFVhEyUUpHF1h9NopX5PYwHtj%2Fm8Z7bvfUfzOnLNpu61m5SBm3qmtvENC8jmvwDr3o1QzmNY0%2FJOi7snzG0ie40p5dMEIzpgg01%2BfvthmCafw5k6mFWXH9fExhrokXmsgQqSA%3D%3D&acctmode=0&pass_ticket=RuGGTB4SplvCwKsI2KQuIGeMic0YEMrpZC0xdlI2bqbLFRwR3pxiQhTHo50quxu1e65Mg%2Ffoz1ivddwfWjHoWA%3D%3D&wx_header=0#rd | ÁªüÊ≤ªÊâ©Êï£Ê®°ÂûãÁöÑU-NetË¶ÅË¢´Âèñ‰ª£‰∫ÜÔºåË∞¢ËµõÂÆÅÁ≠âÂºïÂÖ•TransformerÊèêÂá∫DiT
https://dreamfusion3d.github.io/ | DreamFusion: Text-to-3D using 2D Diffusion
https://download.arxiv.org/pdf/2211.08332v2 | Versatile Diffusion Text, Images and Variations All in One Diffusion Model - Arxiv-2211.08332
https://github.com/ashawkey/stable-dreamfusion | ashawkey/stable-dreamfusion: A pytorch implementation of text-to-3D dreamfusion, powered by stable diffusion.
https://arxiv.org/pdf/2212.00794.pdf | Scaling Language-Image Pre-training via Masking - Arxiv-2212.00794
https://book.douban.com/subject/20563216/ | ÂñÇÈ£üËÄÖÂçè‰ºö (Ë±ÜÁì£)
https://book.douban.com/subject/34980177/ | ÈòøÂ∏ïÂøíÈÅäÊà≤ (Ë±ÜÁì£)
https://book.douban.com/subject/35245405/ | ‰∏çÁü•Â±±‰∏ä (Ë±ÜÁì£)
https://book.douban.com/subject/34809604/ | Âº∑Âº± (Ë±ÜÁì£)

https://scholar.google.com/scholar?start=10&q=gpt-3+world+knowledge&hl=en&as_sdt=0,22&as_vis=1 | gpt-3 world knowledge - Google Scholar
https://arxiv.org/pdf/2206.01718.pdf | A-OKVQA A Benchmark for Visual Question Answering using World Knowledge - Arxiv-2206.01718
https://arxiv.org/abs/2104.13478 | Geometric Deep Learning Grids, Groups, Graphs, Geodesics, and Gauges - Arxiv-2104.13478
https://towardsdatascience.com/geometric-foundations-of-deep-learning-94cdd45b451d | Geometric foundations of Deep Learning | by Michael Bronstein | Towards Data Science
https://www.di.ens.fr/~fbach/learning_theory_class/lecture9.pdf | lecture9.dvi
https://arxiv.org/pdf/1906.11300.pdf | Benign Overfitting in Linear Regression - Arxiv-1906.11300

https://www.youtube.com/watch?v=gmoX2SSk9q4 | (516) OPTML++ 10/12/2022 Tengyu Ma - YouTube

https://book.douban.com/subject/36104107/ | ÈïøÂÆâÁöÑËçîÊûù (Ë±ÜÁì£)

https://ai.stanford.edu/blog/understanding-incontext/ | How does in-context learning work? A framework for understanding the differences from traditional supervised learning | SAIL Blog
https://www.google.com/search?q=garg+et+al+2022+in+contex+learning&newwindow=1&sxsrf=ALiCzsZhehlGH0y-T-FZtEB9KIiu6jH14g%3A1670951396834&ei=5LGYY7i5Moue5NoPyK-KiAo&ved=0ahUKEwi4u8eDi_f7AhULD1kFHciXAqEQ4dUDCBA&uact=5&oq=garg+et+al+2022+in+contex+learning&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzoKCAAQRxDWBBCwAzoFCCEQqwI6BQghEKABOgUIIRCSAzoHCCEQoAEQCkoECEEYAEoECEYYAFDWA1jQK2D6LGgBcAF4AIABigGIAfcOkgEEMTMuNpgBAKABAcgBB8ABAQ&sclient=gws-wiz-serp | garg et al 2022 in contex learning - Google Search
https://arxiv.org/abs/2106.13884 | Multimodal Few-Shot Learning with Frozen Language Models - Arxiv-2106.13884
https://www.google.com/search?q=What+can+transformers+learn+in-context%3F+a+case+study+of+simple+function+classes&sourceid=chrome&ie=UTF-8 | What can transformers learn in-context? a case study of simple function classes - Google Search
https://www.google.com/search?q=text+based+real+image+editing+with+diffusion+models&oq=text+based+real+image+editing+with+d&aqs=chrome.0.0i512j69i57j0i22i30i625j0i390l4.6058j0j1&sourceid=chrome&ie=UTF-8 | text based real image editing with diffusion models - Google Search
https://arxiv.org/abs/2210.09276 | Imagic Text-Based Real Image Editing with Diffusion Models - Arxiv-2210.09276
https://arxiv.org/pdf/2112.10741.pdf | GLIDE Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models - Arxiv-2112.10741
https://arxiv.org/abs/2203.08414 | Unsupervised Semantic Segmentation by Distilling Feature Correspondences - Arxiv-2203.08414
https://arxiv.org/abs/2104.14294 | Emerging Properties in Self-Supervised Vision Transformers - Arxiv-2104.14294
https://arxiv.org/abs/2203.03605 | DINO DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection - Arxiv-2203.03605
https://arxiv.org/abs/2202.04200 | MaskGIT Masked Generative Image Transformer - Arxiv-2202.04200
https://www.google.com/search?q=NeurIPS+2022+best+papers&newwindow=1&sxsrf=ALiCzsb8ZFY7XK_IsIonniRyVV31KUdnrw%3A1670970420677&ei=NPyYY_2AKY3QkPIPwMiUqAQ&ved=0ahUKEwj96ury0ff7AhUNKEQIHUAkBUUQ4dUDCBA&uact=5&oq=NeurIPS+2022+best+papers&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIFCAAQgAQyBQgAEIYDOgoIABBHENYEELADOgUIABCiBDoICCEQwwQQoAFKBAhBGABKBAhGGABQhQVYqw1gyQ9oAnABeACAAc0BiAHSCZIBBTAuNi4xmAEAoAEByAEIwAEB&sclient=gws-wiz-serp | NeurIPS 2022 best papers - Google Search
https://blog.neurips.cc/2022/11/21/announcing-the-neurips-2022-awards/ | Announcing the NeurIPS 2022 Awards ‚Äì NeurIPS Blog
https://www.2022.aclweb.org/best-paper-awards | ACL 2022 | 60th Annual Meeting of the Association for Computational Linguistics | Ireland
https://stableboost.ai/home?tab=2&createModel=1 | Stableboost
https://github.com/f/awesome-chatgpt-prompts | f/awesome-chatgpt-prompts: This repo includes ChatGPT promt curation to use ChatGPT better.
https://arxiv.org/pdf/2212.06138v1.pdf | CLIP Itself is a Strong Fine-tuner Achieving 85.7% and 88.0% Top-1 Accuracy with ViT-B and ViT-L on ImageNet - Arxiv-2212.06138
https://openreview.net/pdf?id=dF6aEW3_62O | You Can Have Better Graph Neural Networks by Not Training Weights at All Finding Untrained GNNs Tickets - OR-logconference-2022_dF6aEW3_62O
https://www.google.com/search?q=revisiting+parameter+efficient+tuning&oq=revisiting+parameter+efficient+tuning&aqs=chrome..69i57j0i546l3.5798j0j1&sourceid=chrome&ie=UTF-8 | revisiting parameter efficient tuning - Google Search
https://arxiv.org/abs/2202.07962#:~:text=Parameter%2DEfficient%20Tuning%20(PETuning),or%20even%20better%20than%20finetuning. | Revisiting Parameter-Efficient Tuning Are We Really There Yet? - Arxiv-2202.07962
https://arxiv.org/abs/2210.10693 | Robustness of Demonstration-based Learning Under Limited Data Scenario - Arxiv-2210.10693
https://arxiv.org/pdf/2209.08206.pdf | Selective Token Generation for Few-shot Natural Language Generation - Arxiv-2209.08206
https://github.com/jmschrei/torchegranate | jmschrei/torchegranate: A temporary repository hosting a pomegranate re-write using PyTorch as the backend.
https://pyro.ai/ | Pyro
https://www.google.com/search?q=atlas+neurips+map&oq=atlas+neurips+map&aqs=chrome..69i57j33i160l3j33i299l2.5276j0j1&sourceid=chrome&ie=UTF-8 | atlas neurips map - Google Search
https://atlas.nomic.ai/map/neurips | Atlas: NeurIPS 1987-2022
https://arxiv.org/abs/2211.11719 | First Steps Toward Understanding the Extrapolation of Nonlinear Models to Unseen Domains - Arxiv-2211.11719
https://www.google.com/search?q=arxiv+2207+09640&oq=arxiv+2207+09640&aqs=chrome..69i57j0i546l3j69i64.4104j0j1&sourceid=chrome&ie=UTF-8 | arxiv 2207 09640 - Google Search
https://www.google.com/search?q=text+guided+3D+diffusion+mdoel&oq=text+guided+3D+diffusion+mdoel&aqs=chrome..69i57j33i10i160.4530j0j1&sourceid=chrome&ie=UTF-8 | text guided 3D diffusion mdoel - Google Search
https://deepai.org/publication/3ddesigner-towards-photorealistic-3d-object-generation-and-editing-with-text-guided-diffusion-models | 3DDesigner: Towards Photorealistic 3D Object Generation and Editing with Text-guided Diffusion Models | DeepAI

https://huggingface.co/docs/transformers/main/en/task_summary#sequence-classification | Summary of the tasks
https://huggingface.co/docs/transformers/main/en/bertology | BERTology
https://huggingface.co/course/chapter7/6 | Training a causal language model from scratch - Hugging Face Course
https://huggingface.co/docs/transformers/tasks/language_modeling | Language modeling
https://huggingface.co/blog/big-bird | Understanding BigBird's Block Sparse Attention
https://huggingface.co/blog/pretraining-bert | Pre-Train BERT with Hugging Face Transformers and Habana Gaudi
https://www.google.com/search?q=huggingface+NSP+pretraining&newwindow=1&sxsrf=ALiCzsYiRHTC84K7ePgh8qADHAYKke1-Uw%3A1670871966029&ei=nnuXY4aiAb6h5NoP6pum6A0&ved=0ahUKEwiG6f-P4_T7AhW-EFkFHeqNCd0Q4dUDCBE&uact=5&oq=huggingface+NSP+pretraining&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIFCAAQogQyBQgAEKIEMgUIABCiBDIFCAAQogQ6CggAEEcQ1gQQsAM6BggAEBYQHjoFCCEQoAE6BQghEKsCOgcIIRCgARAKSgQIQRgASgQIRhgAUM8DWN0MYLwNaAFwAXgAgAFuiAGoCZIBAzYuNpgBAKABAcgBCMABAQ&sclient=gws-wiz-serp | huggingface NSP pretraining - Google Search
https://github.com/gyhandy/Caption_Generation_Detection/blob/main/imagenet_diffusion.yaml | Caption_Generation_Detection/imagenet_diffusion.yaml at main ¬∑ gyhandy/Caption_Generation_Detection
https://spaces.ac.cn/archives/6853 | ‰∏∫ËäÇÁ∫¶ËÄåÁîüÔºö‰ªéÊ†áÂáÜAttentionÂà∞Á®ÄÁñèAttention - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/7575 | BERT-of-TheseusÔºöÂü∫‰∫éÊ®°ÂùóÊõøÊç¢ÁöÑÊ®°ÂûãÂéãÁº©ÊñπÊ≥ï - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/7430 | GoogleÊñ∞‰ΩúSynthesizerÔºöÊàë‰ª¨Ëøò‰∏çÂ§ü‰∫ÜËß£Ëá™Ê≥®ÊÑèÂäõ - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/7325 | Á™ÅÁ†¥Áì∂È¢àÔºåÊâìÈÄ†Êõ¥Âº∫Â§ßÁöÑTransformer - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/7124 | Âü∫‰∫éConditional Layer NormalizationÁöÑÊù°‰ª∂ÊñáÊú¨ÁîüÊàê - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/7921 | PerformerÔºöÁî®ÈöèÊú∫ÊäïÂΩ±Â∞ÜAttentionÁöÑÂ§çÊùÇÂ∫¶Á∫øÊÄßÂåñ - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/7661 | ‰øÆÊîπTransformerÁªìÊûÑÔºåËÆæËÆ°‰∏Ä‰∏™Êõ¥Âø´Êõ¥Â•ΩÁöÑMLMÊ®°Âûã - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/8180 | Nystr√∂mformerÔºöÂü∫‰∫éÁü©ÈòµÂàÜËß£ÁöÑÁ∫øÊÄßÂåñAttentionÊñπÊ°à - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/8027 | RealFormerÔºöÊääÊÆãÂ∑ÆËΩ¨ÁßªÂà∞AttentionÁü©Èòµ‰∏äÈù¢Âéª - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/8823 | ‰ªéÁÜµ‰∏çÂèòÊÄßÁúãAttentionÁöÑScaleÊìç‰Ωú - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/8431 | ‰πüÊù•ÁõòÁÇπ‰∏Ä‰∫õÊúÄËøëÁöÑÈùûTransformerÂ∑•‰Ωú - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/9034 | ÁÜµ‰∏çÂèòÊÄßSoftmaxÁöÑ‰∏Ä‰∏™Âø´ÈÄüÊé®ÂØº - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces

https://github.com/thu-ml/tianshou/blob/master/test/discrete/test_dqn.py | tianshou/test_dqn.py at master ¬∑ thu-ml/tianshou
https://tianshou.readthedocs.io/en/master/tutorials/dqn.html | Deep Q Network ‚Äî Tianshou 0.4.10 documentation
https://nn.labml.ai/rl/dqn/replay_buffer.html | Prioritized Experience Replay Buffer
https://nn.labml.ai/rl/ppo/gae.html | Generalized Advantage Estimation (GAE)
https://nn.labml.ai/rl/dqn/ | Deep Q Networks (DQN)
https://nn.labml.ai/rl/dqn/model.html | Deep Q Network (DQN) Model
https://arxiv.org/pdf/1511.06581.pdf | Dueling Network Architectures for Deep Reinforcement Learning - Arxiv-1511.06581
https://nn.labml.ai/rl/index.html | Reinforcement Learning Algorithms
https://nn.labml.ai/rl/ppo/experiment.html | PPO Experiment with Atari Breakout
https://nn.labml.ai/rl/ppo/ | Proximal Policy Optimization - PPO
https://papers.labml.ai/paper/e2bbad8cc8e211eb80dc0bd1877e23b6 | Proximal Policy Optimization Algorithms
https://www.google.com/search?q=dqn+paper&oq=DQN+paper&aqs=chrome.0.0i512l2j0i22i30l5j69i65.1085j0j1&sourceid=chrome&ie=UTF-8 | dqn paper - Google Search
https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf | dqn.pdf

https://arxiv.org/abs/2102.12092 | Zero-Shot Text-to-Image Generation - Arxiv-2102.12092
https://arxiv.org/abs/2103.00020 | Learning Transferable Visual Models From Natural Language Supervision - Arxiv-2103.00020

https://arxiv.org/pdf/2202.04824.pdf | AdaPrompt Adaptive Model Training for Prompt-based NLP - Arxiv-2202.04824
https://arxiv.org/pdf/2209.08721.pdf | Joint Language Semantic and Structure Embedding for Knowledge Graph Completion - Arxiv-2209.08721

https://www.google.com/search?q=nlp+%E7%AC%AC%E5%9B%9B%E8%8C%83%E5%BC%8F+%E8%BF%9E%E7%BB%AD&newwindow=1&sxsrf=ALiCzsbzhaNNgu9RVTfeY15Nbj5Y8nwF9Q%3A1670459268193&ei=hC-RY5ijC_ah5NoPvIqUoAU&ved=0ahUKEwiY9ara4ej7AhX2EFkFHTwFBVQQ4dUDCBA&uact=5&oq=nlp+%E7%AC%AC%E5%9B%9B%E8%8C%83%E5%BC%8F+%E8%BF%9E%E7%BB%AD&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIFCCEQoAEyBQghEKABOgcIABAeELADOgQIABAeOgcIABAeEKIEOgUIABCiBEoECEEYAUoECEYYAFCTA1j8IWD5JGgCcAB4AIABogGIAcYIkgEDNy40mAEAoAEByAEBwAEB&sclient=gws-wiz-serp | nlp Á¨¨ÂõõËåÉÂºè ËøûÁª≠ - Google Search
https://jishuin.proginn.com/p/763bfbd6b477 | NLPÁöÑ‚ÄúÁ¨¨ÂõõËåÉÂºè‚Äù‰πãPrompt LearningÊÄªÁªìÔºö44ÁØáËÆ∫ÊñáÈÄê‰∏ÄÊ¢≥ÁêÜ-ÊäÄÊúØÂúà
https://zhuanlan.zhihu.com/p/397004230 | CMU ÂàòÈπèÈ£ûÔºöNLPÁöÑÁ¨¨ÂõõËåÉÂºè - Áü•‰πé
https://zhuanlan.zhihu.com/p/550705038 | NLPÁ¨¨ÂõõËåÉÂºèÔºöËøûÁª≠ÊÄßPrompt - Áü•‰πé
https://www.google.com/search?q=Few-Shot%20Text%20Generation%20with%20Natural%20Language%20Instructions | Few-Shot Text Generation with Natural Language Instructions - Google Search
https://spaces.ac.cn/archives/8295/comment-page-1 | P-tuningÔºöËá™Âä®ÊûÑÂª∫Ê®°ÁâàÔºåÈáäÊîæËØ≠Ë®ÄÊ®°ÂûãÊΩúËÉΩ - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/8671 | ÊõæË¢´Â´åÂºÉÁöÑÈ¢ÑËÆ≠ÁªÉ‰ªªÂä°NSPÔºåÂÅöÂá∫‰∫Ü‰ºòÁßÄÁöÑZero ShotÊïàÊûú - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/8978 | ËÆ≠ÁªÉ1000Â±ÇÁöÑTransformerÁ©∂Á´üÊúâ‰ªÄ‰πàÂõ∞ÈöæÔºü - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/8764 | ChildTuningÔºöËØïËØïÊääDropoutÂä†Âà∞Ê¢ØÂ∫¶‰∏äÂéªÔºü - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/7805 | Â¶Ç‰ΩïÂàíÂàÜ‰∏Ä‰∏™Ë∑üÊµãËØïÈõÜÊõ¥Êé•ËøëÁöÑÈ™åËØÅÈõÜÔºü - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/7476 | Êó†ÁõëÁù£ÂàÜËØçÂíåÂè•Ê≥ïÂàÜÊûêÔºÅÂéüÊù•BERTËøòÂèØ‰ª•ËøôÊ†∑Áî® - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/7764 | ÂøÖÈ°ªË¶ÅGPT3ÂêóÔºü‰∏çÔºåBERTÁöÑMLMÊ®°Âûã‰πüËÉΩÂ∞èÊ†∑Êú¨Â≠¶‰π† - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/category/Big-Data/19/ | ÂàÜÁ±ª ‰ø°ÊÅØÊó∂‰ª£ ‰∏ãÁöÑÊñáÁ´† - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://arxiv.org/abs/1907.07355 | Probing Neural Network Comprehension of Natural Language Arguments - Arxiv-1907.07355
https://github.com/chong-z/nlp-second-order-attack | chong-z/nlp-second-order-attack: [NAACL 2021] Code for "Double Perturbation: On the Robustness of Robustness and Counterfactual Bias Evaluation"
https://www.google.com/search?q=Learning+to+rank+using+gradient+descent+%E7%9F%A5%E4%B9%8E&newwindow=1&sxsrf=ALiCzsb7rXPRYJd0HBK0d_3R7rnzJz1ZCg%3A1656810633838&ei=iezAYoPYMsjPkPIPkOegyA8&oq=Learning+to+rank+using+gradient+descent+%E7%9F%A5%E4%B9%8E&gs_lcp=ChNtb2JpbGUtZ3dzLXdpei1zZXJwEAM6BwgAEEcQsAM6BwgAELADEEM6BggAEB4QFjoFCAAQhgM6BQghEKABSgQIQRgAUMMNWMMXYIcZaAFwAXgAgAGcAYgBswKSAQMwLjKYAQCgAQHIAQvAAQE&sclient=mobile-gws-wiz-serp | Learning to rank using gradient descent Áü•‰πé - Google Search
https://github.com/kzkadc/ranknet | kzkadc/ranknet: PyTorch and Chainer implementation of RankNet
https://zhuanlan.zhihu.com/p/68682607 | ÊµÖË∞àLearning to Rank‰∏≠ÁöÑRankNetÂíåLambdaRankÁÆóÊ≥ï - Áü•‰πé
https://zhuanlan.zhihu.com/p/26539920 | Learning to rankÂü∫Êú¨ÁÆóÊ≥ïÂ∞èÁªì - Áü•‰πé
https://www.google.com/search?q=PT-Ranking%3A+A+Benchmarking+Platform+for+Neural+Learning-to-Rank&sourceid=chrome-mobile&ie=UTF-8 | PT-Ranking: A Benchmarking Platform for Neural Learning-to-Rank - Google Search
https://wildltr.github.io/ptranking/ | PT-Ranking
https://wildltr.github.io/ptranking/ltr_diversification/Direct_Metric_Optimization/ | Direct Metric Optimization - PT-Ranking
https://github.com/wildltr/ptranking | wildltr/ptranking: Learning to Rank in PyTorch
https://www.google.com/search?q=learning+2+rank+pytorch&oq=&aqs=chrome.1.69i176j35i39i362l14.-1j0j7&sourceid=chrome-mobile&ie=UTF-8 | learning 2 rank pytorch - Google Search
https://www.google.com/search?q=Learning+to+rank+using+gradient+descent&sourceid=chrome-mobile&ie=UTF-8 | Learning to rank using gradient descent - Google Search
https://zhuanlan.zhihu.com/p/20711017 | „ÄäLearning to Rank using Gradient Descent„Äã - Áü•‰πé
https://www.google.com/search?q=learning+2+rank+%E7%9F%A5%E4%B9%8E&newwindow=1&sxsrf=APq-WBthK18uxk-5suj97oxpybhDKukdHw%3A1645752044777&ei=7C4YYu_vLq7PkPIPvqWx4A4&oq=learning+2+rank+%E7%9F%A5%E4%B9%8E&gs_lcp=ChNtb2JpbGUtZ3dzLXdpei1zZXJwEAMyBQgAEKIEOgcIABBHELADOgQIIxAnOgUIIRCgAToICCEQFhAdEB46BggAEBYQHjoFCCEQqwJKBAhBGABQ7gRYyRhg4BtoAXABeACAAbUBiAGOCZIBAzAuOZgBAKABAcgBCMABAQ&sclient=mobile-gws-wiz-serp | learning 2 rank Áü•‰πé - Google Search
https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/examples/README.md | adversarial-robustness-toolbox/README.md at main ¬∑ Trusted-AI/adversarial-robustness-toolbox
https://www.google.com/search?q=randomized+smoothing+transformer&newwindow=1&sxsrf=APq-WBu7Pw-v7fWSW4M4vrry6XcqjfJdkw%3A1643881562115&ei=WqT7YYC3Bq_DkPIP1bOz2A8&oq=randomized+smoothing+transformer&gs_lcp=ChNtb2JpbGUtZ3dzLXdpei1zZXJwEAMyBQghEKABMgUIIRCgAToHCCMQsAMQJzoCCAA6BAgjECc6BQgAEIAEOgYIABAWEB46BwghEAoQoAFKBAhBGAFQqgRYoS9gijFoAHAAeACAAasCiAG9CpIBBTAuNi4xmAEAoAEByAEBwAEB&sclient=mobile-gws-wiz-serp | randomized smoothing transformer - Google Search

https://www.bilibili.com/video/BV1W3411H79o/?spm_id_from=333.851.b_7265636f6d6d656e64.4&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | „ÄêQÂêõ„ÄëÊúÄÂ•ΩÁöÑÈáëÁî∞‰∏ÄÊ∏∏ÊàèÔºÅÊâÆÊºîÁäØ‰∫∫ÂÖ®Á®ãÊäóÂéãÔºÅ‰∏≠ÊñáÂ≠óÂπï 01

https://www.zhihu.com/search?q=Muv-Luv%20Alternative&type=content | Muv-Luv Alternative - ÊêúÁ¥¢ÁªìÊûú - Áü•‰πé

https://waxworksmath.com/Authors/N_Z/Sutton/RLAI_1st_Edition/WWW/chapter_2.html | Chapter 2 in Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto.
https://ai.stanford.edu/~cbfinn/ | Chelsea Finn, Stanford University
https://ai.stanford.edu/~cbfinn/_files/neurips19_memorization.pdf | neurips19_memorization
https://slideslive.com/38922670/invited-talk-the-big-problem-with-metalearning-and-how-bayesians-can-fix-it | Chelsea Finn ¬∑ Invited Talk: The Big Problem with Meta-Learning and How Bayesians Can Fix It ¬∑ SlidesLive
https://www.zhihu.com/people/superbrother-58/posts | superbrother - Áü•‰πé

https://book.douban.com/author/1988420/books?sortby=collect&format=pic | Ê∂©Ê≥ΩÈæôÂΩ¶ Tatsuhiko ShibusawaÁöÑ‰ΩúÂìÅÔºà81Ôºâ
https://www.google.com/search?q=%E7%94%9F%E6%AD%BB%E7%96%B2%E5%8A%B3&oq=%E7%94%9F%E6%AD%BB%E7%96%B2%E5%8A%B3&aqs=chrome.0.0i355i512j46i512j0i512l8.2258j0j1&sourceid=chrome&ie=UTF-8 | ÁîüÊ≠ªÁñ≤Âä≥ - Google Search
https://www.google.com/search?q=%E4%BD%A9%E9%9B%B7%E5%85%8B&oq=%E4%BD%A9%E9%9B%B7%E5%85%8B&aqs=chrome..69i57j0i15i30.14359j0j1&sourceid=chrome&ie=UTF-8 | ‰Ω©Èõ∑ÂÖã - Google Search
https://www.google.com/search?q=%E6%AD%BB%E5%9C%A8%E5%8D%97%E6%96%B9&oq=%E6%AD%BB%E5%9C%A8%E5%8D%97%E6%96%B9&aqs=chrome..69i57j0i30j0i5i30.6308j0j1&sourceid=chrome&ie=UTF-8 | Ê≠ªÂú®ÂçóÊñπ - Google Search
https://www.douban.com/search?cat=1001&q=%E5%88%9D%E6%9C%9F%E5%A5%8E%E5%9B%A0%E8%AE%BA | ÊêúÁ¥¢: ÂàùÊúüÂ•éÂõ†ËÆ∫
https://book.douban.com/subject/35407396/ | ÂüÉÂãíÈáå¬∑Â•éÂõ†ËÆ∫ (Ë±ÜÁì£)
https://www.wikiwand.com/ja/%E9%A3%AF%E5%9F%8E%E5%8B%87%E4%B8%89 | È£ØÂüéÂãá‰∏â - Wikiwand
https://www.google.com/search?q=%E3%82%A8%E3%83%A9%E3%83%AA%E3%83%BC%E3%83%BB%E3%82%AF%E3%82%A4%E3%83%BC%E3%83%B3%E8%AB%96 | „Ç®„É©„É™„Éº„Éª„ÇØ„Ç§„Éº„É≥Ë´ñ - Google Search
https://book.douban.com/subject/35301339/ | „ÉÜ„Çπ„Ç´„Éà„É™„Éù„Ç´ (Ë±ÜÁì£)
https://book.douban.com/subject/33438831/ | Ê¢¶ÁöÑÂÆáÂÆôÂøó (Ë±ÜÁì£)
https://www.google.com/search?q=%E7%99%BD%E9%B9%BF%E5%8E%9F&oq=%E7%99%BD%E9%B9%BF%E5%8E%9F&aqs=chrome.0.0i355i512j46i512j0i512j46i512j0i512l6.2640j0j1&sourceid=chrome&ie=UTF-8 | ÁôΩÈπøÂéü - Google Search
https://www.google.com/search?q=%E8%8F%8A%E8%B1%86&oq=%E8%8F%8A%E8%B1%86&aqs=chrome..69i57j0i512l3j46i512j0i512l5.3634j0j1&sourceid=chrome&ie=UTF-8 | ËèäË±Ü - Google Search
https://www.google.com/search?q=%E5%BA%9F%E9%83%BD&oq=%E5%BA%9F%E9%83%BD&aqs=chrome..69i57j46i512l2j0i512l6.5676j0j1&sourceid=chrome&ie=UTF-8 | Â∫üÈÉΩ - Google Search

https://github.com/AdamCobb/hamiltorch | AdamCobb/hamiltorch: PyTorch-based library for Riemannian Manifold Hamiltonian Monte Carlo (RMHMC) and inference in Bayesian neural networks
https://adamcobb.github.io/journal/hamiltorch.html | hamiltorch: a PyTorch Python package for sampling | Adam Cobb

https://www.bilibili.com/video/BV1Ne4y1W7E3/?spm_id_from=333.1007.tianma.3-2-8.click&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | „ÄêÂÆåÁªì„ÄëÊ¥ª‰æ†‰º† | Ê≠¶‰æ†ÂÖªÊàê | Â§öÁªìÂ±Ä | ÊúÄ‰∏ëÁî∑‰∏ª | ÊΩúÂäõÊñ∞Ê∏∏ | ÊµÅÁ®ãÂÆûÂÜµÂêàÈõÜ„ÄêDemo„Äë_ÂìîÂì©ÂìîÂì©_bilibili

https://book.douban.com/subject/26720073/ | ÂÅúÂú®‰∏âÊ®ì (Ë±ÜÁì£)
https://www.douban.com/note/839881322/?_i=9590426KLQjbnS | „ÄäÊé®ÁêÜÂ≠¶ÂØºËÆ∫9„ÄãËØÑ

https://book.douban.com/subject_search?search_text=%E6%8B%9F%E5%8D%97%E8%8A%A5 | ÊãüÂçóËä• - ËØª‰π¶ - Ë±ÜÁì£ÊêúÁ¥¢
https://book.douban.com/subject/36029749/ | Â§ßÊº†Â•áÈóªÂΩï (Ë±ÜÁì£)

https://book.douban.com/subject/35653884/ | ÊΩÆÊ±êÂõæ (Ë±ÜÁì£)

https://www.douban.com/note/838649960/?_i=9006047KLQjbnS | ‰∏ä‰∫§Á§æÂàä„ÄäËøáÂéª‰πã‰π¶„Äã„ÄäÊú™Êù•‰πã‰π¶„Äã‰∏™‰∫∫ËØÑ‰ª∑
https://book.douban.com/subject/3783263/ | Êó•Êú¨Êé®ÁêÜÂêç‰ΩúÈÄâÔºöÊµúÂ∞æÂõõÈÉéÔºàÂç∑‰∏ÄÔºâ (Ë±ÜÁì£)
https://book.douban.com/subject/35280426/ | ÊÆòÂÉè17ÔºöÊñ∞Áñ´ÊôÇÊúüÁöÑÊÆ∫ÊÑè (Ë±ÜÁì£)
https://www.douban.com/search?q=%E4%B8%A4%E4%BA%AC%E5%8D%81%E4%BA%94%E6%97%A5 | ÊêúÁ¥¢: ‰∏§‰∫¨ÂçÅ‰∫îÊó•

https://github.com/Dotkat-dotcome/OpenPrompt | Dotkat-dotcome/OpenPrompt: An Open-Source Toolkit for Prompt-Learning.
https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247561202&idx=1&sn=4e03162edf39679158611fc0824eaa73&chksm=f9a09d7dced7146ba3b8c3927809946728cb90f792b7814bc0a3952d6ed6a461d8c2a446f73e&mpshare=1&scene=1&srcid=1116KzfqoNjX560k9VVZriuD&sharer_sharetime=1668577228478&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQZh8nbjCyHRzzTn8lE%2Fa65hKWAgIE97dBBAEAAAAAAH8%2BOTdnAy8AAAAOpnltbLcz9gKNyK89dVj05nwTZvI6wCiktfH%2FfbMb1V%2F2OCH9pTBH71xJCohirMuwDjlTuK8%2BZkyhzAPFPZ6cRT6e%2FZx20%2BuWyNpU1OVcCO1jYEkVVr3wwndt8PpK7j5uGPMhnwpwlNM1WSMbBn3M%2Byyl5vAZA%2BfYN80ib%2B5ciiEYs2reZ4iKGBKmqcYLh8dVQIhZVBZx84snBC%2BndTOeg6G%2FdypC2Zj14DxkBvR0Phwv5lt%2F3TmAAA0yUP0cDjkk3fbpY%2FNS94SnDgzXMmu7UwM25iEMT017RuBMdRg86T%2BoLrBwbG7ArwkH%2BH44%2Bf6sa90XaoiJ1rzfDyo9ZRcq&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q21GkvEvW%2F4r7VlMLJ0Qu53&wx_header=0#rd | È°∂ÂàäTIP 2022ÔºÅÈòøÈáåÊèêÂá∫Ôºö‰ªéÂàÜÂ∏ÉËßÜËßíÂá∫ÂèëÁêÜËß£ÂíåÊèêÂçáÂØπÊäóÊ†∑Êú¨ÁöÑËøÅÁßªÊÄß
https://mp.weixin.qq.com/s?__biz=MzIzNzU4OTAxMQ==&mid=2247492658&idx=1&sn=0e8ce066af3250de639594d63d2f270b&exportkey=n_ChQIAhIQ6T90Vu79lCHEZhhwCXZbRhKWAgIE97dBBAEAAAAAAO1cIVUxaDUAAAAOpnltbLcz9gKNyK89dVj08l018TKgFHmFqfFDEi91pHRt6gI4hYtHe%2BDdXHcCXlSbpeaoduId6u1NRNg8F%2Bb%2FyhYC%2BryxEAG%2FwZQ802qvuIGXHwtdYhil%2B%2BlM%2B%2BfRxBQLMTVOxMfrZKfvqciXo1WjJZHYuGniT%2FuEJxS9e2mrDpI5EhVoXL9ftPfOjTBUWVYukWxQGc9cGL2vVoysNb2AeIlbLAJs1ReAO2txEs2B%2FmqKHtPMtEIWLeLBvvH9qhOwjpMcSWyEqW1Iqstkq3TF3IF0X%2FvfaerVH65SWiMBqzRf2waSPDcp9n%2FmeNzeFbd6Z7HTfJD4qpK904GMsv31&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q1gvKC2xbFlioiyfsJ0UODN&wx_header=0 | CVPR2022 | AttentionÊú∫Âà∂ÊòØ‰∏∫‰∫ÜÊâæÊúÄÁõ∏ÂÖ≥ÁöÑitemÔºü‰∏≠ÁßëÂ§ßÂõ¢ÈòüÂèçÂÖ∂ÈÅìËÄåË°å‰πãÔºÅ
https://mp.weixin.qq.com/s?__biz=Mzk0NDE5Nzg1Ng==&mid=2247504550&idx=1&sn=596768cc4fa51609e24fae65f358fd0f&exportkey=n_ChQIAhIQGLP9agIdyiNmJWfUH89bEBKWAgIE97dBBAEAAAAAAEGAME%2BqmqoAAAAOpnltbLcz9gKNyK89dVj0XvEc%2BYw6BaG3SknrLbZXdJnVGSMwFInAWBOMjn7TO6hubqf3U4hZeQB77L17y3JUpKaYv5eZClEwlSlsC2zgNH6hwr4ByGxVGWNUgplpY76XLOAGSJb%2F%2BFNgXCvqPSZYJHcdqB6J%2FKbL5QJVacp0yoyk59SRgmMIpBkePMX2ZdoJpJwhaiM8EsvmeWqy%2FXG9%2Fd%2BDNmsV6r32b7ugTroXbq98Bm29fqN1XbiOfK5HrWlpNJQK8wdPph9av1xewTmPUVNjJfCJl41jijqwj1jqEnJ6wT2Xxs325%2B2lAkrl1DRSoJ%2FQpgYawii55H4kXjhV&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q2JREvUaxd36WH3Kv19J6NO&wx_header=0 | ÈíàÂØπ‰∏çÂπ≥Ë°°ÈóÆÈ¢òÂª∫Ê®°ÁöÑÊúâË∂£Loss„ÄÇ
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247519625&idx=1&sn=2ad9d58f8626c18ec7ef5bcd0e75d11e&exportkey=n_ChQIAhIQkHIV%2Fyg6LrO5CI%2BXFvTYAhKWAgIE97dBBAEAAAAAAKRAM8n0uJYAAAAOpnltbLcz9gKNyK89dVj0XCv066TM0ehUIpIgLZlOpj4G59QZckJT5wQaXur8Zz2s0vn6JdCfQlPt0JLudXA%2BZcLJLR4ufgsxqzqScI380nfPYbhWyk9Np6j0VxKrd0VcX%2BS%2FTVw8DupJEI5fRN7m1rPZPNo1S6epLNNaJr1U0mLafkcIxNdbKq7ZEHASwtQSG3uNJJF4IPzuGeWRQ%2BGAfmp1bDwPBj1rMTKhTdOwjdbmN8pUdiOtqC5RQAU%2FUvTmZPQZlouGdwYBV%2BAWv%2BCpXG9QfTm6i5HLKUwoJiKu%2BK4c3XIfPJgyLHXhtJ%2Bb6HdcX3BscdwcbKbJ4N0Ojzu3&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q3bdWILo60AnKxtn2MfZTE3&wx_header=0 | ÂõæÁÅµÂ•ñÂ§ß‰Ω¨+Ë∞∑Ê≠åÂõ¢ÈòüÔºå‰∏∫ÈÄöÁî®‰∫∫Â∑•Êô∫ËÉΩËÉå‰π¶ÔºÅCV ‰ªªÂä°‰πüËÉΩÁî® LM Âª∫Ê®°ÔºÅ
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247530816&idx=1&sn=78ff024c6dac20bf7e19474b7b9c1b61&exportkey=n_ChQIAhIQtfrl6IpwFofy5rnZ8aC5ixKWAgIE97dBBAEAAAAAAI4XLqn07p4AAAAOpnltbLcz9gKNyK89dVj0aNDPffWThQiQZGDU0vDr6BYX4eBBJGKVO0Wsla8LA2bW2ZeYqEUj4JSLyau7wRjxxh6rXv8qJsBPH6nlrSdjOV16kVaQ8WT8Yur3Xp5yXwB2SbZt5p1q%2BR0WczWIOxQ2dynJCuwHtsZEVaMNWZ4rNIu1JglyoZLx8bgn40K%2B53aRvZlf1Etrjk2nWJaGA%2BjDpMh4tCGthwZxxLDPWTUPtS93MzlLfOV1g5dAHwrBWRE%2BQM8DSqJ3EcpzVuY9SwkgXnL00K1EqE9gp4H1J1u0CIjtDoPZj6JkAwqRy8W7KzvC5MBu5f4Pp8yGXv4XEe5Q&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q15x66qZzt8QojaGoMhMPtJ&wx_header=0 | Pix2SeqÔºöË∞∑Ê≠åÂ§ßËÑëÊèêÂá∫ CV ‰ªªÂä°Áªü‰∏ÄÊé•Âè£ÔºÅ
https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247550934&idx=1&sn=895ec412a849e15f13316acde9f53d53&exportkey=n_ChQIAhIQQM3vO3ml90wA9SEtD5zK5xKWAgIE97dBBAEAAAAAAOqXL3nnOQYAAAAOpnltbLcz9gKNyK89dVj07WVg2uWWlLNZWv%2BZND3dfPc14h%2Bve4hDPuwqIVgWgYxjc3rw5MJEaXanqN6C9QTerCCmmlfWY5QN2eH35lm4pL94GBgwNhDpF0XIll70qmnhzThQjXNk%2F81KmkG49dul3nN8fOyBxr3lh0D6%2BkbpITqIGt%2FO2RKkqfSOeeKDXgbaAePZdfqV9DMfAsMNFspLPposTfYtN3CJQALlY1EGZfLx7m83nwzNc%2FaC88vlyS8kpvYiMFxVxn%2F1G%2BgwUD7ntqer%2Bv6Kq5%2FcbXKhEU6yJVKAGv%2FXgY0puiptyx45ouWlklOVewmlY%2BLaVPk56T8T&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q0IsoLB3sN0%2BRzkapHeWeQJ&wx_header=0 | ‰ªéACL 2022 OnsiteÁªèÂéÜÁúãNLPÁÉ≠ÁÇπ
https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652238017&idx=5&sn=1e292aae780c708ea5b8279f4bc38630&chksm=f1268430c6510d26b92c939f0db2e3addb6e3bea7836db183b68e1aa90577d8690f304709f87&mpshare=1&scene=1&srcid=1116iosEQCYbDKHNQQw7iNgh&sharer_sharetime=1668545671724&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQq7shMG4e04AeookIN3sXFhKIAgIE97dBBAEAAAAAAENFNmVT%2FnAAAAAOpnltbLcz9gKNyK89dVj0hu6wpWjs0m5v3SFiO%2Bwp%2BEScTglxJ4ulRKo4Lpfds6rR0YNjIOEf8MYXaLEfHW3eExFAR9nlkWTHjEa99Z02R5QBJG%2FX9LWGZB7Pv5xVzmzjX5Oq1WYg8%2Fog4o30IMu9RsTQ1G71lQvPcKVYm4bjwkaM5eRopwChCFM5RzNH26Me1rv1yFrUq9jI0VBZ6PtKz4QpSa27IY6%2BI23XhOG3WKEOM30rSJ8kfSc0GXAcqkT4euz5D%2BBN%2FCtd1S7dj4B8iMWTMXwUnuODwdHhNhaUzKdNu9VezxwsLaV057bun552uA%3D%3D&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q2jNoQEbw1Bk8otYTL2sUaJ&wx_header=0#rd | Êâ©Êï£Ê®°ÂûãÂú®ÊñáÊú¨ÁîüÊàêÈ¢ÜÂüüÁöÑÂ∫îÁî®
https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247631975&idx=3&sn=d10c4f76e95b8fb79d19d46f8a87323b&chksm=e8de7315dfa9fa03d2ced352e5dea6d7033496c21ed8425a0c0406b1ee9b4bf3d8b6fa085b5e&mpshare=1&scene=1&srcid=1116noySzvGcWs86ROpSAyl7&sharer_sharetime=1668544894840&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQ%2FZKRUzriagkJcXivCXKsMhKWAgIE97dBBAEAAAAAAJ8iE5s%2F%2BFMAAAAOpnltbLcz9gKNyK89dVj0F6ULe2fNQ9U5o6c5RcEe07w3k8yGOVskl5Sw%2BvWVyXM8TsPSEk0oDuzJ4iLIFO9J5tbKEG7k4HsCFntU3eL5Yj%2B0ggYWkfvKdYwqPPbjD7q0dVxDwbX%2BIQdh6Zr38ItO1MI%2F%2F%2FjOI6meeF4D8pUOI4Bis4vJ8tA8eNYIaAtqKcxkHo2sf9RFKi%2Bmu6mFO%2FQQqhmX5RHGY2Vft%2BI55ywTv%2BwOsG4AvUYEgpwM0bKiBDFmO%2B1%2BHY7O5CJIqM%2Bb5YyxN2fsWVjw9c2ZNq6ILdsA4ck7H2ecZXOKoXoLeozzj6wN%2BcsMuYz1F5K%2Bi9NPfAYU&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q0jkSGmaCg2ddIBG6lCexyf&wx_header=0#rd | È©¨ÊØÖÊ≤àÂêëÊ¥ãÊõπÈ¢ñÊúÄÊñ∞AIÁªºËø∞ÁÅ´‰∫ÜÔºÅËÄóÊó∂3ÊúàÊâìÈÄ†ÔºåÁΩëÂèãÔºöÂøÖËØªËÆ∫Êñá
https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650847145&idx=2&sn=ebf8ff8f8fb04a8ef4134272eb49ea88&chksm=84e573d7b392fac1536f10ec18963410c68ea5a2c0499cd7219cb781c8d6e167f680da7feb13&mpshare=1&scene=1&srcid=1116iLyaKa63ZA1yFC8YP2an&sharer_sharetime=1668544714998&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQ7YQQxfR7eyaVvBTi8n8sAhKWAgIE97dBBAEAAAAAAIj%2FK0e3yDIAAAAOpnltbLcz9gKNyK89dVj0FBTmqTNpwTXvWk5zawyd66I%2FWIYe6fymfYXUQbuF3atBRV43vVs11OAWJTAFsZmHBg9T8EDstRGOMwvIT5sl5gCGDias4gGyT8wMuYXs0m7bv00VLdBCNjqeM1AHakMgxF9drRYoPhrCMKAhudIZWh67qXO0vFQ4aSzp9IIY1XiPbziypSnSQsgj07SfZYc82dO5Qeqw9TsRNRDHyfdkg27L8pBMHwdbInYV9TxgJrG6J95XhE1IV5lyk7QXG6kUQwxJ3eQMFP%2Fw%2FPN6SM4JpCtkgpSlYx2w19Mut5rU%2BP47vloz3PrrIOrSL0xUbRYr&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q3HMVqoJhO0ed2F5eNYNNF8&wx_header=0#rd | Á†îÁ©∂ËÄÖÊÑèÂ§ñÂèëÁé∞DALL-E 2Âú®Áî®Ëá™ÂàõËØ≠Ë®ÄÁîüÊàêÂõæÂÉèÔºöÂÖ®ÊñáÈªëËØùÔºå‰∫∫Á±ªÈÉΩÁúã‰∏çÊáÇ
https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650851425&idx=1&sn=08a31c3e5de9b9ce18defa5a0fbf7f17&exportkey=n_ChQIAhIQOAX16xa8xJzLMg%2BaWr%2F5AxKWAgIE97dBBAEAAAAAAE51Lp%2BHhE4AAAAOpnltbLcz9gKNyK89dVj0%2Fk6Kixt9YrrWHJUmbyL1z2SEGKWIqt0GTkZaz96EQUcz5EXrggIGUkWZ4e%2BhTwS3T3guw6cWa52hyPth3uDN4IKvBJgl5sXZe95QDFWdMZrE%2FgFFEq2DjOHGGpzTQBPwgmQyG5SD1TMklJjigo0V%2F5CDFxBdpbTypfYJVlfmp43kzLucVUB4IFW3T0cr9BEmbABfMGcxp%2BLTon8juUb3jE8fp5Dfynn4RZGCFbok1NiaNhpjT62cTnzw7Jv5N%2F9Vm%2F2LqmhogQvYOH9hkgcq0wMDamCDUS%2BgPaGgTbgLuPgV%2ByIYflNbWOzjrTZDYICO&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q2sjJxlfCuB124tYFeXBNfR&wx_header=0 | ÈôàÂ§©Â•á„ÄÅÁéãÂ®ÅÂªâÁ≠â‰∫∫Êé®ËçêÔºöACLÊúÄ‰Ω≥ËÆ∫ÊñáÂ•ñÂæó‰∏ªÁªôÊñ∞ÂÖ•Ë°åÁ†îÁ©∂ËÄÖÁöÑ‰∏ÄÁÇπÂª∫ËÆÆ
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247528370&idx=1&sn=fb3dc251519540cbbb1a4337ed1cabd9&chksm=970f4764a078ce72ca9f641d8574385ebec3a34429d4903d73cf2c69cbeb43ec0492a6a51f40&mpshare=1&scene=1&srcid=1116Bz5j8Zu9768shmqxqoNl&sharer_sharetime=1668544702215&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQiRLPTpUyVBUdNGagRNkZ2BKWAgIE97dBBAEAAAAAANN%2FBBfKY7gAAAAOpnltbLcz9gKNyK89dVj0%2Bme3uCrCARgZru48GM2kVVN%2BmmrzqtzHdkgsPES8%2BVBg3ztOULhnIR4AEkpjnPnqTd5GJVaUqxwEnNA45HpzrmlPGATS7qgwLJVrsstcOAdTCgpB1OoKgq8Q1drxB7bTf7Ba0bW85T5Ngxp1rh74e1sE26Eu6YIzP9DywzFPtSWuErS4p%2BD%2FZlSEPQGvnOhVEO6Vxe62mvgTFJvQuZAc2Q2YdEp15HYmdwmUqjdMYPwmMo6cbA0s1TRo%2Fv0BRKP7mfCh79VcQPxZ2EtwAlpitZSmbbIj1DrCestXK8qvDfEJgeuAoqQchzpYqM73wqf2&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q3cbg2OEnvH0nsPWHeSymKa&wx_header=0#rd | CMU ÊèêÂá∫ÂÖ®Êñ∞ GAN ÁªìÊûÑÔºåGAN Ëá™Ê≠§ËøàÂÖ•È¢ÑËÆ≠ÁªÉÂ§ßÂÜõÔºÅ
https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247640819&idx=1&sn=34048077ddf16734df0163b13c9cba85&exportkey=n_ChQIAhIQC9F%2FCGF5Zbjo1aaRTtrDKBKWAgIE97dBBAEAAAAAADkHLhfbnzkAAAAOpnltbLcz9gKNyK89dVj0J4RiKmK9Fnh6di61v6F%2FnpVShnUEE7oe2psqbsCPSXTgxRKNTlel8C5BPbLEcAGv16KMWLwb5umhIlgLGfAYqH6x4os01TofKCCsUXedDKbv7%2FHhrW9TmfBwjystTn6hRTjy9ttearhs%2BRRacrjl2zjihY9SywrXqaEoCvNLxtTZuSI9hoPxZDhubR8QL9DSVJXxi5fhca7hby7j7%2F%2F%2FY1n9iFOfuTUklbZVQuf0bi7DcuTDfKqSEC98zcCnnLMYqvkh7SNu64OdwLDVGSoIgO0RRWRq7twsD22bDVElj%2FDI9LIzLlAlzcGGXFhVLszJ&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q3TLWhCWfCNW3Y07Q5r96i6&wx_header=0 | ‚ÄúÂú®ÊâÄÊúâ‰∫ãÊÉÖ‰∏äÊâìË¥•ÊâÄÊúâ‰∫∫‚ÄùÔºåÂæÆËΩØÂ§öÊ®°ÊÄÅÊñ∞‰ΩúÊ®™Êâ´12Á±ª‰ªªÂä°ÔºåËøûÁ∫ØËßÜËßâSOTAÈÉΩÂà∑Êñ∞‰∫Ü
https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650859714&idx=2&sn=e065043ceccffdcb67b6d7c7e92b2227&chksm=84e52cbcb392a5aa0c7e430f5449b78360a32e67a36034d447df66418734c9c347e1c6213ba3&mpshare=1&scene=1&srcid=1116AswOu1XHbqOUD3lNdo2l&sharer_sharetime=1668533196137&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQEBDsDAZQgtXrD4lMhY4wmBKWAgIE97dBBAEAAAAAAB%2FDIPux%2BNUAAAAOpnltbLcz9gKNyK89dVj0mWEhPKExecajXImJU%2Fd85dW12sQwfnjepX5ItsXoYj5uE%2Fwa604KdguCRkbTfqXJ0fwutIg5DWLztvPA%2BL7g9tpHjnXAtyJVnE5r1vWwS%2Ftg2KmB13tzGYLnhzn31lIdV8A74FuOzQAuA1Erco2uG0dRLsTwtndd1JwVGkXJ%2BDbwm7vyClZ3yHycbIbZABaV6C8zBNPPxfQKflO7eE1VoJbZJfWcqG8oXsXIE5OXSa34C3PyFTFXPKByKaeVjTv%2FdwExBtuKQcIAp%2FDWBe1CZQz1WDm4ZmvPsTy2WfkxbR6W2vKd3zfnQtn9DxI4H8Q4&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q2UiDB3yzuu6K7%2BWz0dKcU9&wx_header=0#rd | ‰∏ÄË°å‰ª£Á†Å12ÂÄçÂä†ÈÄüBertÊé®ÁêÜÔºåOpenAIÁºñÁ®ãËØ≠Ë®ÄÂä†ÊåÅÁöÑÂºïÊìéÁÅ´‰∫Ü
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247529906&idx=1&sn=f30387b45d8e882adb4044dbd58d5859&exportkey=n_ChQIAhIQCudZbohJE3oYB8H5QNBjwxKWAgIE97dBBAEAAAAAAO72DJxUd6UAAAAOpnltbLcz9gKNyK89dVj0NTjKh7iKftDMbuAfhghIvd2g6L1sR2AOMvgzGKNp25KnhhCgvny6rdPaHRSVwJaL0e5oOaYSRN2e6zOlbbhen4cb2ktdJJ2%2F0FcDNTWFj4ii3swRBcHFOa%2BZ0Fnv2jr%2F5nKsl2axU1P6bwlFmKOx%2BCj7H4sOnM884Fz2%2BU9eFqWTZulbBaEHj2DR1vHfYDYiog%2BDS6JlczxLQk48pmuuM3tpmvEFYHKKtlXNBQkyFHaw1ImRKFOFI%2FrSiyFV31xNpjMFqAcIhbt8JA5iXfObUTe9FPc88eYiEB9zL7FSYi49YMeI%2Fkyf49wSMDz5MMmE&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q3t4hBtrl2cOqzk9ACigs94&wx_header=0 | ACL‚Äò22Êù∞Âá∫ËÆ∫ÊñáÔºöPromptËåÉÂºèÊúâbugÔºÅ
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247531708&idx=1&sn=0dd6bd51624cf3aa21d6679e19ed1796&exportkey=n_ChQIAhIQjpP9RcEikrD%2BOC1NlSKjQhKWAgIE97dBBAEAAAAAAIosJJIPvbwAAAAOpnltbLcz9gKNyK89dVj0L9gs%2BiLUhfFIOzjJrx1LkphvqN7QoKby5pIK959gKUz81hOzNZM%2FvBezCKRCrWco2%2BcZVnSSSnT8EBjOhTztpnQR6T%2BeCDx1dmUiF4vtaPjx3TXs6EVBcA5c6JdkpcY2OBLoiv5ZoMC7yLUQmzUnU6XTM39%2FDU9VqEJAyIaijd1mE62ExjT1ZY76KCEZg%2FB3sPfKcCxhKgLKQI9R5OwmMgrvdPdnzd7xp6QZK8AzSPuaHPQemhZSx5nOFk5COLqJU6LNVo%2FPM7uyA1kozGz456Lf92DyPv%2BSQ%2B9O1kPqIVb%2BdHh74RAWsObEFmIf5oHm&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q3GNbsGIKh6jteJn0S6dXGk&wx_header=0 | MITÊåáÂá∫ÂÖ¨ÂºÄÈ¢ÑËÆ≠ÁªÉÊ®°Âûã‰∏çËÉΩ‰π±Áî®
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247533143&idx=1&sn=30b7725412f51fc6e049466402999c98&exportkey=n_ChQIAhIQA7UMun1XuKEKg8F0M%2B3%2FIRKWAgIE97dBBAEAAAAAAIXTINfW01YAAAAOpnltbLcz9gKNyK89dVj0G1LRwJ74nv%2FpZNyW%2BhDkaNr5MLC2sFUMKhPJAqesEGjcvxlE9DK4aUzoafqZKnfCFMJUMnHyapzNXBbuBpxSqRDLkS6hrUn4JLwCHe5VsRU5ZqAVGcX%2Fc6GhciMFwSjsAdHa4SM42%2FsOc%2FSBTR7%2BIW3tKKCZddvaxksYd8LGMi9ugEgSSj3VKDmAGWGldjnXUZ8QhRNv955l32Kru%2BTZyIAH7z64FlGkYY9Aony4bKcIl%2FAfucS2lVU6vAy%2BzkDk7fpn%2B%2FnoDZFJiCB513%2FZfiNjDw637PQB8dUDGkZOcpApXXvyt34XBkFhnsC2ccBz&acctmode=0&pass_ticket=dk1%2BYeI9KcxVieaewIrrrbKjqtemDbE%2BLsWThaEB6q3ZgExJjmmZ%2FP%2F%2FPYzQ1lNP&wx_header=0 | GPT-3 Ê≥ÑÈú≤‰∫ÜÊàëÁöÑÁúüÂÆûÂßìÂêç

https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247531708&idx=1&sn=0dd6bd51624cf3aa21d6679e19ed1796&exportkey=n_ChQIAhIQ7XraOwDDZUo9JAdAQEhp%2FRKWAgIE97dBBAEAAAAAAF0WLQWcV2cAAAAOpnltbLcz9gKNyK89dVj092%2F1BY5VluWiHtdbuuQR1zyTqn9bjzPlu9XYuZGQYSuMlNBySPzus0G3vWcuhC6iRl%2Bz4zBUsmUifs5V4ZJAft6945Mj3APCAeBiOGdRn9aSBhocNIG6uoHaVyLN3zlG4e%2BeKMIYoZRPiAoCdd%2FPu6kiGKmz%2BLbVAihSWzGfyvIzlCjYfXkrzNHZqaA%2BEkkU%2Bp%2BFNggF4h%2B1L25znq0IJpXGVHtgtWEdFypgPdfoD8AxMmkz705jCa%2BIhzH9OEIv%2Bp4ZehS2NhP9bdxViLXonv2ig9sW6wnpI1sk79T5xroqRk7AyFWKsS6dEp%2FgXPKs&acctmode=0&pass_ticket=zUvJ6Bt8HsTNAPBA6U4qajGZ5sn0Pt6WEGVtKUG1A6McGlT1Awcq8l9VBLNEv8Kt&wx_header=0 | MITÊåáÂá∫ÂÖ¨ÂºÄÈ¢ÑËÆ≠ÁªÉÊ®°Âûã‰∏çËÉΩ‰π±Áî®
https://mp.weixin.qq.com/s?__biz=MzIzNzU4OTAxMQ==&mid=2247493187&idx=1&sn=6e99d3b6e4eba8fec06380af8bcd2050&chksm=e8c4e13fdfb3682927f550094936768586650ab5b6a7d3bc71f1bd0ceccc31432e6775534f5f&mpshare=1&scene=1&srcid=11166sE47MSxzzvkUoTxmhmK&sharer_sharetime=1668532819886&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQpSZbFxroUyu57ilR3nLejBKWAgIE97dBBAEAAAAAAJkSIg4U%2ByAAAAAOpnltbLcz9gKNyK89dVj0HrbZO0aLCzga8hiKwGFWBE50CSIN2t99u%2FpFy80H5sk9k9JS3iIElMjg%2FTYkoV5%2FOo3BdHQX5fQECbw6nRMN16uBOrKwmITooNU5PpYqEWXpw4d9H7FAQvqetCcfWIKyklAh%2F%2BBKYR%2FG9uVytir1hThKxExGHyiorzn9XKgLx1NfYCVQUUbKNsLhmrHceZiiOvd%2BuvRQ%2BSXVCToJH5L%2BVXOO4yjCq2mcRbYBC1BR2phF%2Fpc9euzO1sOJBzCNEYfy9RXp1M4yXMk%2B3P2FEwyo6VDeCYwijhPkzPPu9g7P7FIQnDdlhjvr7FUuTL1nzGGN&acctmode=0&pass_ticket=zUvJ6Bt8HsTNAPBA6U4qajGZ5sn0Pt6WEGVtKUG1A6MYnG3J6GgrtlVrof5z3aOc&wx_header=0#rd | WACV2022 | ‰∏ÄÂº†ÂõæÁâáÂè™ÂÄº‰∫îÂè•ËØùÂêóÔºüUABÊèêÂá∫ÂõæÂÉè-ÊñáÊú¨ÂåπÈÖçËØ≠‰πâÁöÑÊñ∞ËßÜËßíÔºÅ
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247536935&idx=1&sn=3593e2b71af9d7142b03978a67052f30&exportkey=n_ChQIAhIQvRty0%2BARyeH%2FhQekVI5ZKRKWAgIE97dBBAEAAAAAAOcHODZCFiAAAAAOpnltbLcz9gKNyK89dVj0omvS8NMhsMo%2FVTJxMm0GS3ipPXJ%2BoIFSyt%2F79de4MUhq77Dl1nRIOonso0YHZZ%2FE6aUrw5nL5rnkZUGirFsxgzyesp2Hg2cAvsR8h8V7%2BqUuPCZPGIpqppbpsVqomdMy%2FzMFu3sXj7zM6mcV4qnre0v4UGKSz%2FJ0JW0ZN%2Bql0eN%2Fyu0oI%2FPBuYf%2BqxETmO5YfFjDv1Dfmx379hJ5qC3g2b%2BgGUMb6eFjgoiAV0lVZTL0ttHCPhwkCbAlSGY6gz6cgokzDxUKsRtbvhqlDbKGEFjpzN%2BsP4U8nm3dbbbVKzvB%2BH7IPGkoqJn03c6y80Rw&acctmode=0&pass_ticket=zUvJ6Bt8HsTNAPBA6U4qajGZ5sn0Pt6WEGVtKUG1A6N7LqcDy72qCLyIgoAc8OJ2&wx_header=0 | ICLR 2023 ÊúÄÈ´òÂàÜËÆ∫ÊñáË¢´Èî§ÊäÑË¢≠ÔºüÔºü
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247536565&idx=1&sn=bf381c5092de28dfc54326cefa7df264&chksm=970f6763a078ee751e41373fa8a34b10245f91a51619e92b156b43eed05080ff02d67825d771&mpshare=1&scene=1&srcid=1116fdPSMiJe3AqPttYaBva4&sharer_sharetime=1668532870713&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQc%2FP4q8UrlGQiZT8I6z4bvRKWAgIE97dBBAEAAAAAAJqvLN%2BmRH0AAAAOpnltbLcz9gKNyK89dVj0jgeUqtax0Pr4rAlLHk8dLiAgiX1O1QfQISlBU3567cdxvvo8pQckA8IQ%2FV3Y6vuabjUelVXrbt9w%2BEo4490j9%2FDxVYv4ahmbBDG87EsIhLWqnLND8eKqz1uclUw%2FD95ULcVpXZRE7KZokqj2aNwg3IAsMd0JcAVYid5dCp5E8LS8DfhxgPzFAYaEeysUpjIUMu5e4w8JZlgl0E61rR%2BiMwHUg3ghdcIpo%2F2rSdODPum03mAtr75yGbxO9DI6Q4TlFZv14ePqFQJ9objFGmgYEBnsygOl1Tz7vg%2BMaJdiaEW2kRyLxqGju8oZa3yGELdC&acctmode=0&pass_ticket=zUvJ6Bt8HsTNAPBA6U4qajGZ5sn0Pt6WEGVtKUG1A6M4VemB5CPEuPfthxbERKwf&wx_header=0#rd | AIÂèñ‰ª£‰∫∫Á±ªÔºåÂèØ‰ª•Ëá™Âä®ÁîüÊàêprompt‰∫Ü
https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247536935&idx=1&sn=3593e2b71af9d7142b03978a67052f30&exportkey=n_ChQIAhIQA3oIolGi7kKNEC4aELHqBRKWAgIE97dBBAEAAAAAAEGlA1xy%2FPkAAAAOpnltbLcz9gKNyK89dVj03sw89IITQ%2BLgHRDDPgTW%2BonXNlWcBRzZjU9AdffLwr1Fh46%2Fcv4NNi3Y82ZbnEjNUIyHhqykrQxf1%2BLtvuxb%2FdP9TP%2FE2x0P8RObdvvi4sFMjeDMBxZxBUJVde9q%2Bz6OJl7xBNeVORUqfYC2sQM4U1gBGvtItsN06N2%2BmE%2BHag0AFNj%2ByvbzCHbnVpfbEfds3lrcryisusuztsJjhHl2sgRgvUBU0vgU8p%2B7UWS9dq9gadVb4eVF3d94mTeNglg2s8GJyvpY77XpO%2FN3kzXtBBMnIroqXeUliEnv2ThyXdm4cX%2FDJvNtHeW5hiAey0PP&acctmode=0&pass_ticket=zUvJ6Bt8HsTNAPBA6U4qajGZ5sn0Pt6WEGVtKUG1A6M6WdYO36NrmwEtOS6I%2Fz3o&wx_header=0 | ICLR 2023 ÊúÄÈ´òÂàÜËÆ∫ÊñáË¢´Èî§ÊäÑË¢≠ÔºüÔºü
https://mp.weixin.qq.com/s?__biz=MzIzNzU4OTAxMQ==&mid=2247493187&idx=1&sn=6e99d3b6e4eba8fec06380af8bcd2050&chksm=e8c4e13fdfb3682927f550094936768586650ab5b6a7d3bc71f1bd0ceccc31432e6775534f5f&mpshare=1&scene=1&srcid=11166sE47MSxzzvkUoTxmhmK&sharer_sharetime=1668532819886&sharer_shareid=5d63aa4dfc13f490ea99a15b75d4f593&exportkey=n_ChQIAhIQpkdfsWvK0kwkcqM9sk0kZBKWAgIE97dBBAEAAAAAABzhFPUs8zgAAAAOpnltbLcz9gKNyK89dVj0UzVbJxU93yyIE%2BAaT2OOeRQVCmQW%2FTnB88ubleERdlfuX%2FZnbZhGu9W5IgYQIZ2CRpOngHXQBI6lphBBlWQXWy4wOsjwbo2c1YjvVTue4AObxbrKv70LWw1nuZNjNX8bjDZt15uZLf6zpbjNFKlm718M9tG1DfBmcXjZ%2B0i3S0GiQcM4QJQ9lQ9W42uDGM2lPyyGWXcAMhIlFfDOrWmJfpkv9ar5Nbfm4LZjP1v8JCkmZE9mKQQ4r9MTLYes3fTWuzFEO8IY5LB9ctRutip57J89AXT4zAzmfjG0nLNgC8JUv26pPAOGJCSVtEczoHD5&acctmode=0&pass_ticket=zUvJ6Bt8HsTNAPBA6U4qajGZ5sn0Pt6WEGVtKUG1A6ObPH4kDzfuf96tj9OMolxN&wx_header=0#rd | WACV2022 | ‰∏ÄÂº†ÂõæÁâáÂè™ÂÄº‰∫îÂè•ËØùÂêóÔºüUABÊèêÂá∫ÂõæÂÉè-ÊñáÊú¨ÂåπÈÖçËØ≠‰πâÁöÑÊñ∞ËßÜËßíÔºÅ
https://www.cs.princeton.edu/courses/archive/fall22/cos597G/ | COS 597G: Understanding Large Language Models

https://www.douban.com/note/790069051/?_i=8488771KLQjbnS | „ÄêÊ∞ëÁøª„ÄëÁôΩ‰∫ïÊô∫‰πã È£ü‰πã‰ø°Êù°
https://www.google.com/search?q=%E9%82%AA%E6%95%99%E3%81%AE%E7%A5%9E | ÈÇ™Êïô„ÅÆÁ•û - Google Search
https://book.douban.com/subject/5602441/ | Ë´ãÂãøÂú®Ê≠§‰∏üÊ£ÑÂ±çÈ´î (Ë±ÜÁì£)
https://www.douban.com/note/831114616/?_i=8488680KLQjbnS | „ÄêÂ∞èËØ¥ÁøªËØë„ÄëÂ•óÂ®É‰πãÂ§ú
https://www.99csw.com/article/4835.htm | Spring Has Come_Ê¢ìÂ¥é‰ºò_Âú®Á∫øÈòÖËØª_‰πù‰πùËóè‰π¶ÁΩë
https://book.douban.com/subject/26653983/ | ‰πåÈ∏¶Á§æ (Ë±ÜÁì£)
https://book.douban.com/subject/5395151/ | ÂâØÊú¨ (Ë±ÜÁì£)
https://book.douban.com/subject/11597370/ | Èô∂ÂÅ∂ (Ë±ÜÁì£)
https://book.douban.com/subject/26900294/ | Á∫¢ÊòüËìùË∞É (Ë±ÜÁì£)
https://book.douban.com/subject/34464618/ | Âú£Â§©Áß§Êòü (Ë±ÜÁì£)
https://book.douban.com/subject/27001144/ | ÂçÅ‰∏âÂ±ÇÁ©∫Èó¥ (Ë±ÜÁì£)
https://www.google.com/search?q=And%20Then%20There%20Were%20(N-One) | And Then There Were (N-One) - Google Search
https://www.google.com/search?q=%E8%8F%9C%E8%8A%B1%E8%9B%87%E7%9A%84%E6%9C%AB%E6%97%A5 | ËèúËä±ËõáÁöÑÊú´Êó• - Google Search
https://www.google.com/search?q=%E6%94%AF%E7%A6%BB%E7%A0%B4%E7%A2%8E%E7%9A%84%E8%94%B7%E8%96%87 | ÊîØÁ¶ªÁ†¥Á¢éÁöÑËî∑Ëñá - Google Search
https://www.google.com/search?q=%20%E6%96%A9%E9%A6%96T%E5%AD%97%E4%B9%8B%E8%B0%9C | Êñ©È¶ñTÂ≠ó‰πãË∞ú - Google Search
https://www.google.com/search?q=%E7%9C%9F%E5%AE%9F%E3%82%92%E8%A6%86%E3%81%84%E9%9A%A0%E3%81%99%E9%9B%A8 | ÁúüÂÆü„ÇíË¶Ü„ÅÑÈö†„ÅôÈõ® - Google Search
https://www.google.com/search?q=%E6%B6%88%E5%A4%B1%E7%9A%84%E7%A0%82%E8%9B%BE%E5%AE%B6 | Ê∂àÂ§±ÁöÑÁ†ÇËõæÂÆ∂ - Google Search
https://www.google.com/search?q=%E5%9B%AD%E7%94%B0%E4%BF%AE%E4%B8%80%E9%83%8E%E3%80%8A%20%E4%BD%9C%E8%80%85%E3%82%88%E6%AC%BA%E3%81%8B%E3%82%8B%E3%82%8B%E3%81%AA%E3%81%8B%E3%82%8C%20%E3%80%8B | Âõ≠Áî∞‰øÆ‰∏ÄÈÉé„Ää ‰ΩúËÄÖ„ÇàÊ¨∫„Åã„Çã„Çã„Å™„Åã„Çå „Äã - Google Search
https://www.douban.com/note/728081430/?_i=8490432KLQjbnS | „ÄêÊ∏£ËØë„ÄëÊàëÁöÑÁ•ûÁßòÂÆ´ÊÆøbyÊ≥ΩÊùë‰ºäÊô∫
https://www.google.com/search?q=%E4%BA%95%E4%B8%8A%E7%9C%9F%E4%BC%AA%20%E3%80%8A%E5%9B%9A%E4%BA%BA%E9%A6%86%E7%9A%84%E6%83%A8%E5%89%A7%E3%80%8B | ‰∫ï‰∏äÁúü‰º™ „ÄäÂõö‰∫∫È¶ÜÁöÑÊÉ®Ââß„Äã - Google Search
https://www.google.com/search?q=%E3%82%AE%E3%82%AC%E3%81%8F%E3%82%89%E3%82%8A%E3%81%AE%E6%AE%BA%E4%BA%BA | „ÇÆ„Ç¨„Åè„Çâ„Çä„ÅÆÊÆ∫‰∫∫ - Google Search
https://www.google.com/search?q=%0A%E7%99%BD%E4%BA%95%E6%99%BA%E4%B9%8B%20%E3%80%8A%E9%9D%92%E5%B1%81%E8%82%A1%E7%9A%84%E5%B0%B8%E4%BD%93%E3%80%8B | ÁôΩ‰∫ïÊô∫‰πã „ÄäÈùíÂ±ÅËÇ°ÁöÑÂ∞∏‰Ωì„Äã - Google Search
https://www.google.com/search?q=%E6%9C%89%E6%A0%96%E5%B7%9D%E6%9C%89%E6%A0%96+%E6%AF%94%E6%B5%B7%E6%9B%B4%E6%B7%B1%E7%9A%84%E6%B2%B3%E5%B7%9D&oq=%E6%9C%89%E6%A0%96%E5%B7%9D%E6%9C%89%E6%A0%96+%E6%AF%94%E6%B5%B7%E6%9B%B4%E6%B7%B1%E7%9A%84%E6%B2%B3%E5%B7%9D&aqs=chrome..69i57j33i160l4.2154j0j1&sourceid=chrome&ie=UTF-8 | ÊúâÊ†ñÂ∑ùÊúâÊ†ñ ÊØîÊµ∑Êõ¥Ê∑±ÁöÑÊ≤≥Â∑ù - Google Search
https://www.google.com/search?q=%0A%E3%80%8A%E8%99%BD%E6%96%AD%E5%A4%B4%E8%80%8C%E4%B8%8D%E6%AD%BB%E7%9A%84%E6%88%91%E4%BB%AC%E7%9A%84%E6%97%A0%E5%A4%B4%E6%9D%80%E4%BA%BA%E4%BA%8B%E4%BB%B6%E3%80%8B | „ÄäËôΩÊñ≠Â§¥ËÄå‰∏çÊ≠ªÁöÑÊàë‰ª¨ÁöÑÊó†Â§¥ÊùÄ‰∫∫‰∫ã‰ª∂„Äã - Google Search
https://book.douban.com/subject/26809566/ | Áõ∏È£ü (Ë±ÜÁì£)
https://www.google.com/search?q=%E3%82%A6%E3%82%A3%E3%83%B3%E3%83%81%E3%82%A7%E3%82%B9%E3%82%BF%E3%83%BC%E3%83%BB%E3%83%9E%E3%83%BC%E3%83%80%E3%83%BC%E3%83%BB%E3%83%9F%E3%82%B9%E3%83%86%E3%83%AA%E3%83%BC%E3%83%BB%E3%83%8F%E3%82%A6%E3%82%B9%E3%81%AE%E6%AE%BA%E4%BA%BA+%E6%96%9C%E7%BA%BF%E5%A0%82%E6%9C%89%E7%BA%AA&newwindow=1&sxsrf=ALiCzsY22DCy0gy8dhM0p5w9_C_IdngQ8A%3A1668490859025&ei=ayZzY_WSAfXQ5NoP9POHkAc&ved=0ahUKEwi19NnnvK_7AhV1KFkFHfT5AXIQ4dUDCBA&uact=5&oq=%E3%82%A6%E3%82%A3%E3%83%B3%E3%83%81%E3%82%A7%E3%82%B9%E3%82%BF%E3%83%BC%E3%83%BB%E3%83%9E%E3%83%BC%E3%83%80%E3%83%BC%E3%83%BB%E3%83%9F%E3%82%B9%E3%83%86%E3%83%AA%E3%83%BC%E3%83%BB%E3%83%8F%E3%82%A6%E3%82%B9%E3%81%AE%E6%AE%BA%E4%BA%BA+%E6%96%9C%E7%BA%BF%E5%A0%82%E6%9C%89%E7%BA%AA&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIHCAAQHhCiBDIFCAAQogQyBQgAEKIEMgcIABAeEKIESgQIQRgBSgQIRhgAUL0QWMsbYOMiaANwAHgAgAGNBIgB4gSSAQUxLjUtMZgBAKABAcABAQ&sclient=gws-wiz-serp | „Ç¶„Ç£„É≥„ÉÅ„Çß„Çπ„Çø„Éº„Éª„Éû„Éº„ÉÄ„Éº„Éª„Éü„Çπ„ÉÜ„É™„Éº„Éª„Éè„Ç¶„Çπ„ÅÆÊÆ∫‰∫∫ ÊñúÁ∫øÂ†ÇÊúâÁ∫™ - Google Search
https://www.google.com/search?q=%0A%E3%80%8AQJKJQ%E3%80%8B%E4%BD%90%E8%97%A4%E7%A9%B6 | „ÄäQJKJQ„Äã‰ΩêËó§Á©∂ - Google Search
https://www.google.com/search?q=%E5%A4%A7%E5%B1%B1%E8%AA%A0%E4%B8%80%E9%83%8E+%E4%B8%8D%E9%81%8B%E3%81%AA%E7%8A%AF%E4%BA%BA&newwindow=1&sxsrf=ALiCzsbOKvKLVmUhT1HXupK1bsImheH9cQ%3A1668490953653&ei=ySZzY_qlJ9jn5NoPt5iasA4&ved=0ahUKEwi6rumUva_7AhXYM1kFHTeMBuYQ4dUDCBA&uact=5&oq=%E5%A4%A7%E5%B1%B1%E8%AA%A0%E4%B8%80%E9%83%8E+%E4%B8%8D%E9%81%8B%E3%81%AA%E7%8A%AF%E4%BA%BA&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIECCMQJ0oECEEYAUoECEYYAFCeBFi7DWDtEWgCcAB4AIABVogB6QGSAQEzmAEAoAEBwAEB&sclient=gws-wiz-serp | Â§ßÂ±±Ë™†‰∏ÄÈÉé ‰∏çÈÅã„Å™ÁäØ‰∫∫ - Google Search
https://www.douban.com/note/699540049/?_i=8490981KLQjbnS | ÁßãÈú≤ÂÆ´Ë∂ÖË±™ÂçéÂà´Â¢ÖË°ÄÊ¥ó‰∫ã‰ª∂
https://www.99csw.com/book/10418/index.htm | ÈÖíÂæí_Âàò‰ª•È¨Ø_Âú®Á∫øÈòÖËØª_‰πù‰πùËóè‰π¶ÁΩë
https://www.google.com/search?q=%0A%E3%80%8A%E3%82%B4%E3%83%BC%E3%82%B9%E3%83%88%E2%89%A0%E3%83%8E%E3%82%A4%E3%82%BA%E3%80%8B | „Ää„Ç¥„Éº„Çπ„Éà‚â†„Éé„Ç§„Ç∫„Äã - Google Search
https://www.google.com/search?q=%0A%E3%80%8A%E6%9C%80%E5%90%8E%E4%B9%9F%E6%98%AF%E6%9C%80%E5%88%9D%E7%9A%84%E5%81%B6%E5%83%8F%E3%80%8B%0A | „ÄäÊúÄÂêé‰πüÊòØÊúÄÂàùÁöÑÂÅ∂ÂÉè„Äã - Google Search
https://www.google.com/search?q=%0A%E3%80%8A%E7%8E%BB%E7%92%83%E4%B9%8B%E5%A1%94%E6%9D%80%E4%BA%BA%E4%BA%8B%E4%BB%B6%E3%80%8B | „ÄäÁéªÁíÉ‰πãÂ°îÊùÄ‰∫∫‰∫ã‰ª∂„Äã - Google Search
https://www.google.com/search?q=%0A%E3%80%8A%E5%A4%9A%E7%B1%B3%E8%AF%BA%E5%B0%91%E5%A5%B3%E3%80%8B | „ÄäÂ§öÁ±≥ËØ∫Â∞ëÂ•≥„Äã - Google Search

https://book.douban.com/subject/35585182/ | ‰∏çÊ≠ªÈ≥• (Ë±ÜÁì£)
https://book.douban.com/subject/35515868/ | Â°îÁ¥çÊâòÊñØÁöÑÂ§¢Â¢É (Ë±ÜÁì£)
https://book.douban.com/subject/27035205/ | Ê¢ÖÊùúËéé,ÁúãÈè°Â≠ê (Ë±ÜÁì£)
https://movie.douban.com/subject/26252157/ | Èæô‰∏âÂíå‰ªñÁöÑ‰∏É‰∫∫ÂÖö (Ë±ÜÁì£)
https://book.douban.com/subject/35606758/ | Âä®Áâ©Âüé2333 (Ë±ÜÁì£)
https://book.douban.com/subject/26394917/ | ÊñúÁúºÂ∞ëÂπ¥ (Ë±ÜÁì£)
https://book.douban.com/subject/35544979/ | ‰æ¶Êé¢ÂæÄ‰∫ã (Ë±ÜÁì£)
https://book.douban.com/subject/10344079// | Â´åÁñë (Ë±ÜÁì£)
https://book.douban.com/subject/27104036/ | Ëé´ÊØî‰πåÊñØÁöÑÂúàÂ•ó (Ë±ÜÁì£)
https://www.99csw.com/book/7137/index.htm | Â∞∏‰ΩìÈïøÂèë‰πãË∞ú¬∑ÊùÄ‰∫∫ÊñπÁ®ãÂºè2_Áª´ËæªË°å‰∫∫_Âú®Á∫øÈòÖËØª_‰πù‰πùËóè‰π¶ÁΩë
https://book.douban.com/subject/27165658/ | ‰∏âÁôæÂπ¥„ÅÆË¨éÂå£ (Ë±ÜÁì£)
https://book.douban.com/subject/26728812/ | Ê∂ôÈ¶ôËø∑ÂÆÆ (Ë±ÜÁì£)
https://book.douban.com/subject/32567077/ | ÊöóÈªëÂ•≥Â≠ê (Ë±ÜÁì£)
https://www.52shuku.vip/tuili/hsv.html | Áõ≤‰∫∫‰∏éÁãó_Ê∞¥Â§©‰∏ÄËâ≤„ÄêÂÆåÁªì„ÄëÂú®Á∫øÈòÖËØª_52‰π¶Â∫ì
https://www.sto.cx/book-27321-1.html | Ê∞¥Â§©‰∏ÄËâ≤„ÄäÊ†°Âõ≠ÊÉ®Ââß„Äã_ÂÖ®ÊñáÂú®Á∑öÈñ±ËÆÄ_ÊÄùÂÖî
https://book.douban.com/subject/25805272/ | ÂØÜÂÆ§Êé®ÁêÜÊù∞‰ΩúÈÄâ‚Äî‚ÄîÊó•Êú¨Âç∑2 (Ë±ÜÁì£)
https://book.douban.com/subject/4282164/ | Âè´„Å≥„Å®Á•à„Çä (Ë±ÜÁì£)
https://book.douban.com/subject/26277997/ | Â≤õÂíåÊàë‰ª¨ (Ë±ÜÁì£)
https://book.douban.com/subject/5348099/ | Â•áËÅåÊÄ™‰∏ö‰ø±‰πêÈÉ® (Ë±ÜÁì£)
https://book.douban.com/subject/35563806/comments/?start=20&limit=20&status=P&sort=new_score | Èö®Ê©üÊ≠ª‰∫° Áü≠ËØÑ
https://book.douban.com/subject/30458315/ | Êú¨Â∫óÊãõÁâåËèú (Ë±ÜÁì£)
https://www.google.com/search?q=%E4%BA%BA%E6%B2%B9%E8%9C%A1%E7%83%9B%20%E5%B0%8F%E9%85%92%E4%BA%95%E4%B8%8D%E6%9C%A8 | ‰∫∫Ê≤πËú°ÁÉõ Â∞èÈÖí‰∫ï‰∏çÊú® - Google Search
https://www.99csw.com/book/7743/index.htm | ÊÅãÁà±Êõ≤Á∫ø_Â∞èÈÖí‰∫ï‰∏çÊú®_Âú®Á∫øÈòÖËØª_‰πù‰πùËóè‰π¶ÁΩë
https://www.99csw.com/book/8043/index.htm | Ëø∑Â§±ÁöÑÈõ™Â§ú_ÂçÉÂ±±_Âú®Á∫øÈòÖËØª_‰πù‰πùËóè‰π¶ÁΩë

https://github.com/dvlab-research/LBGAT | dvlab-research/LBGAT: Learnable Boundary Guided Adversarial Training (ICCV2021)
https://zhuanlan.zhihu.com/p/558286175 | ÊâìÂºÄÊ®°ÂûãZero-ShotÊñ∞ËåÉÂºèÔºöInstruction Tuning - Áü•‰πé
https://zhuanlan.zhihu.com/p/408166011 | Instruction TuningÔΩúË∞∑Ê≠åQuoc V.LeÂõ¢ÈòüÊèêÂá∫Âèà‰∏ÄÁ≤æË∞ÉËåÉÂºè - Áü•‰πé
https://zhuanlan.zhihu.com/p/422713214 | Fine-tuneÁöÑÊõø‰ª£ÂìÅÔºüÊ∏ÖÂçéP-Tuning v2Â§ßÂπÖÊèêÂçáÂ∞èÊ®°ÂûãÊÄßËÉΩÔºåNER‰πüÂèØpromp tuning‰∫ÜÔºÅ - Áü•‰πé
https://arxiv.org/pdf/2010.15980.pdf | AutoPrompt Eliciting Knowledge from Language Models with Automatically Generated Prompts - Arxiv-2010.15980
https://arxiv.org/pdf/2206.14858.pdf | Solving Quantitative Reasoning Problems with Language Models - Arxiv-2206.14858
https://zhuanlan.zhihu.com/p/440169921 | ‰∏ÄÊñáËΩªÊùæÂÖ•Èó®Prompt(ÈôÑ‰ª£Á†Å) - Áü•‰πé
https://github.com/THUDM/P-tuning-v2 | THUDM/P-tuning-v2: An optimized deep prompt tuning strategy comparable to fine-tuning across scales and tasks
https://github.com/thunlp/OpenPrompt | thunlp/OpenPrompt: An Open-Source Framework for Prompt-Learning.
https://arxiv.org/pdf/2103.08493.pdf | How Many Data Points is a Prompt Worth? - Arxiv-2103.08493
https://huggingface.co/blog/how_many_data_points/ | How many data points is a prompt worth ?
https://zhuanlan.zhihu.com/p/419215591 | NLPÁöÑ‚ÄúÁ¨¨ÂõõËåÉÂºè‚Äù‰πãprompt learningÊÄªÁªì - Áü•‰πé
https://zhuanlan.zhihu.com/p/395115779 | Ëøë‰ª£Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊäÄÊúØÂèëÂ±ïÁöÑ‚ÄúÁ¨¨ÂõõËåÉÂºè‚Äù - Áü•‰πé
https://arxiv.org/pdf/2210.01848.pdf | Explaining Patterns in Data with Language Models via Interpretable Autoprompting - Arxiv-2210.01848
https://github.com/csinva/iprompt | csinva/iprompt: Finding semantically meaningful and accurate prompts
https://github.com/csinva/imodelsX | csinva/imodelsX: Library to explain *a dataset* using natural language and neural networks.
https://github.com/ucinlp/autoprompt | ucinlp/autoprompt: AutoPrompt: Automatic Prompt Construction for Masked Language Models.

https://aclanthology.org/2022.findings-acl.318.pdf | GCPG A General Framework for Controllable Paraphrase Generation - ACL-ACL| Findings-2022_2022.findings-acl.318
https://aclanthology.org/2022.findings-naacl.160.pdf | Learning Structural Information for Syntax-Controlled Paraphrase Generation - ACL-Findings| NAACL-2022_2022.findings-naacl.160
https://arxiv.org/pdf/1903.08855.pdf | Linguistic Knowledge and Transferability of Contextual Representations - Arxiv-1903.08855
https://arxiv.org/pdf/2010.01737.pdf | Transformer-Based Neural Text Generation with Syntactic Guidance - Arxiv-2010.01737
https://arxiv.org/pdf/2108.00104.pdf | Structural Guidance for Transformer Language Models - Arxiv-2108.00104
https://blog.evjang.com/2018/01/nf1.html | Eric Jang: Normalizing Flows Tutorial, Part 1: Distributions and Determinants
https://arxiv.org/pdf/2205.14217.pdf | Diffusion-LM Improves Controllable Text Generation - Arxiv-2205.14217
https://github.com/XiangLi1999/Diffusion-LM | XiangLi1999/Diffusion-LM: Diffusion-LM
https://arxiv.org/abs/2209.11799 | Emb-GAM an Interpretable and Efficient Predictor using Pre-trained Language Models - Arxiv-2209.11799

https://stats.stackexchange.com/questions/390437/variance-of-reparameterization-trick-and-score-function | gradient descent - Variance of reparameterization trick and score function - Cross Validated
https://arxiv.org/pdf/1802.05098.pdf | DiCE The Infinitely Differentiable Monte-Carlo Estimator - Arxiv-1802.05098
https://github.com/alexis-jacq/LOLA_DiCE | alexis-jacq/LOLA_DiCE: Pytorch implementation of LOLA (https://arxiv.org/abs/1709.04326) using DiCE (https://arxiv.org/abs/1802.05098)
https://paperswithcode.com/paper/dice-the-infinitely-differentiable-monte | DiCE: The Infinitely Differentiable Monte-Carlo Estimator | Papers With Code

https://github.com/Verified-Intelligence/auto_LiRPA/blob/master/README.md | auto_LiRPA/README.md at master ¬∑ Verified-Intelligence/auto_LiRPA
https://arxiv.org/pdf/2002.12920.pdf | Automatic Perturbation Analysis for Scalable Certified Robustness and Beyond - Arxiv-2002.12920
https://aclanthology.org/2022.naacl-main.321.pdf | Informativeness and Invariance Two Perspectives on Spurious Correlations in Natural Language - ACL-NAACL-2022_2022.naacl-main.321
https://arxiv.org/pdf/2104.08735.pdf | Learning with Instance Bundles for Reading Comprehension - Arxiv-2104.08735
https://aclanthology.org/2022.naacl-main.238.pdf | Global Entity Disambiguation with BERT - ACL-NAACL-2022_2022.naacl-main.238
https://arxiv.org/pdf/2109.07022.pdf | How Does Counterfactually Augmented Data Impact Models for Social Computing Constructs? - Arxiv-2109.07022
https://arxiv.org/pdf/2203.12942.pdf | Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets - ACL-ACL-2022_2022.acl-long.190
https://arxiv.org/pdf/2010.01057.pdf | LUKE Deep Contextualized Entity Representations with Entity-aware Self-attention - Arxiv-2010.01057
https://arxiv.org/pdf/2204.13902.pdf | Fast Sampling of Diffusion Models with Exponential Integrator - Arxiv-2204.13902

https://www.google.com/search?q=McNema&sourceid=chrome&ie=UTF-8 | McNema - Google Search
https://www.wikiwand.com/en/McNemar%27s_test | McNemar's test - Wikiwand
https://www.google.com/search?q=mcnemar%27s+test+vs+chi+square&newwindow=1&ei=dnpVY6S_EKim5NoPovyOuAE&oq=McNemar%27s+test+vs&gs_lcp=Cgdnd3Mtd2l6EAEYADIFCAAQgAQyBQgAEIAEMgUIABCABDIGCAAQFhAeMgYIABAWEB4yBggAEBYQHjIGCAAQFhAeMgYIABAWEB4yBQgAEIYDMgUIABCGAzoECAAQRzoECAAQQ0oECE0YAUoECEEYAEoECEYYAFDiBVjyCWDRFWgAcAJ4AIABhwGIAd8CkgEDMC4zmAEAoAEByAEIwAEB&sclient=gws-wiz | mcnemar's test vs chi square - Google Search
https://stats.stackexchange.com/questions/76875/what-is-the-difference-between-mcnemars-test-and-the-chi-squared-test-and-how | r - What is the difference between McNemar's test and the chi-squared test, and how do you know when to use each? - Cross Validated

https://www.wikiwand.com/en/Dirichlet_distribution | Dirichlet distribution - Wikiwand
https://www.cs.cmu.edu/~epxing/Class/10701-08s/recitation/dirichlet.pdf | Dirichlet Distribution, Dirichlet Process and Dirichlet Process Mixture

https://book.douban.com/subject/26436007/ | ÊÑèËØÜÂΩ¢ÊÄÅÁöÑÁªàÁªì (Ë±ÜÁì£)
https://www.douban.com/search?q=%E6%B0%91%E4%B8%BB%E7%9A%84%E6%A8%A1%E5%BC%8F | ÊêúÁ¥¢: Ê∞ë‰∏ªÁöÑÊ®°Âºè
https://book.douban.com/review/12918946/ | ‚ÄúÊãøËµ∑Á¨îÔºåÊàëÊòØËá™Â∑±ÁöÑÁ•ûÔºåÊîæ‰∏ãÁ¨îÔºåÊàë‰ªçÊòØÂ∞òÂüÉÔºåÊòØÈáéËçâÔºåÊòØÁÇÆÁÅ∞‚ÄùÔºà‰ªôÁóáÔºâ‰π¶ËØÑ
https://book.douban.com/review/12762218/ | ‚Äú‰∏úÂåóÊñáËâ∫Â§çÂÖ¥‰∏âÊù∞‚ÄùË°•ÂÆåËÆ°ÂàíÔºàÁîüÂêûÔºâ‰π¶ËØÑ
https://reproducedpapers.org/ | Reproduced Papers
https://www.google.com/search?q=%E5%96%84%E7%9A%84%E7%A0%94%E7%A9%B6&oq=%E5%96%84%E7%9A%84%E7%A0%94%E7%A9%B6&aqs=chrome.0.0i355i512j46i512j0i30l2j0i15i30j0i30l2j0i15i30.4192j0j1&sourceid=chrome&ie=UTF-8 | ÂñÑÁöÑÁ†îÁ©∂ - Google Search

https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb#scrollTo=Ylc3AIdZkFhl | Stable Diffusion with üß® diffusers - Colaboratory
https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ | What are Diffusion Models? | Lil'Log
https://yang-song.net/blog/2021/score/ | Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song

https://book.douban.com/subject/35687505/ | ÂÖ≠ËóèÂõæ (Ë±ÜÁì£)
https://book.douban.com/subject/35501248/ | ÊàòÂõΩ¬∑ÁôΩ‰∫ëË∞£ (Ë±ÜÁì£)

https://arxiv.org/pdf/2204.06340.pdf | Distributionally Robust Models with Parametric Likelihood Ratios - Arxiv-2204.06340
https://arxiv.org/pdf/2104.13478.pdf | Geometric Deep Learning Grids, Groups, Graphs, Geodesics, and Gauges - Arxiv-2104.13478

https://www.bilibili.com/video/BV1ds411B7fR/?p=5&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | ÂûíÁêÉÁ§æ‚ÄúÊöñÊöñ‚Äù_ÂìîÂì©ÂìîÂì©_bilibili
https://github.com/THUYimingLi/awesome_lists/blob/main/advice.md | awesome_lists/advice.md at main ¬∑ THUYimingLi/awesome_lists
https://medium.com/@marcotcr/coming-up-with-research-ideas-3032682e5852 | Coming up with research ideas. ‚ÄúWhat project should I do next?‚Äù is a‚Ä¶ | by Marco Tulio Ribeiro | Medium
https://github.com/THUYimingLi/machine_unlearning | THUYimingLi/machine_unlearning: Existing Literature about Machine Unlearning
https://arxiv.org/abs/2205.12331 | Certified Robustness Against Natural Language Attacks by Causal Intervention - Arxiv-2205.12331

https://phillipi.github.io/6.s898/materials/notes/05_cnns.pdf | 05_cnns.pdf
https://geometricdeeplearning.com/blogs/ | GDL Blogs
https://geometricdeeplearning.com/lectures/ | GDL Course

https://movie.douban.com/subject/34915462/?from=subject-page | SSSS.ÁîµÂÖâÊú∫Áéã (Ë±ÜÁì£)
https://www.olehdtv.com/index.php/vod/play/id/25917/sid/1/nid/2.html | SSSS.ÁîµÂÖâÊú∫Áéã_Á¨¨02ÈõÜ - Ê¨ß‰πêÂΩ±Èô¢ÔºçÈù¢ÂêëÊµ∑Â§ñÂçé‰∫∫ÁöÑÂú®Á∫øËßÜÈ¢ëÂ™í‰ΩìÂπ≥Âè∞,Êµ∑ÈáèÈ´òÊ∏ÖËßÜÈ¢ëÂú®Á∫øËßÇÁúã
https://movie.douban.com/subject/26935251/ | Êò•ÂÆµËã¶Áü≠ÔºåÂ∞ëÂ•≥ÂâçËøõÂêßÔºÅ (Ë±ÜÁì£)

https://www.maofly.com/manga/24804.html | Êù•Ëá™Ê∑±Ê∏äÁöÑÈòøÊùú_Êù•Ëá™Ê∑±Ê∏äÁöÑÈòøÊùúÊº´ÁîªÂú®Á∫øÈòÖËØª-Êº´ÁîªÁå´
https://www.manhuagui.com/comic/6808/ | JKÂ•≥Â≠êÊîªÂÖµÊº´Áîª_Â•≥Â≠êÊîªÂÖµÊº´Áîª_ÊùæÊú¨Ê¨°ÈÉé - ÁúãÊº´Áîª
https://www.manhuagui.com/comic/35833/ | area51Êº´Áîª_‰πÖÊ≠£‰∫∫ - ÁúãÊº´Áîª
https://www.manhuagui.com/comic/17254/ | ÊÉ©ÂΩπ339Âπ¥Êº´Áîª_Êá≤ÂΩπ339Âπ¥Êº´Áîª_‰ºäÂäø„Å®„ÇÇ„Åã - ÁúãÊº´Áîª
https://www.zhihu.com/search?q=%E5%AD%A4%E9%AB%98%E4%B9%8B%E4%BA%BA&type=content | Â≠§È´ò‰πã‰∫∫ - ÊêúÁ¥¢ÁªìÊûú - Áü•‰πé
https://www.zhihu.com/question/25274459 | Â¶Ç‰ΩïËØÑ‰ª∑„ÄäÊôöÂÆâÂ∏ÉÂ∏É„ÄãÔºü - Áü•‰πé

https://www.zhihu.com/question/453567336/answer/1825995877 | Á¨îÁªô‰Ω†Ôºå‰Ω†‰ºöÊÄé‰πàÂÜô„ÄäËøõÂáªÁöÑÂ∑®‰∫∫„ÄãÁªìÂ±ÄÔºü - Áü•‰πé

https://www.google.com/search?q=%E5%B0%BC%E9%87%87%E5%B1%B1%E4%B8%8B%E7%9A%84%E6%A0%91&oq=%E5%B0%BC%E9%87%87%E5%B1%B1%E4%B8%8B%E7%9A%84%E6%A0%91&aqs=chrome..69i57j0i546l2.5274j0j1&sourceid=chrome&ie=UTF-8 | Â∞ºÈááÂ±±‰∏ãÁöÑÊ†ë - Google Search
https://www.google.com/search?q=%E6%9F%A5%E6%8B%89%E5%9B%BE%E6%96%AF%E7%89%B9%E6%8B%89%E5%A6%82%E6%98%AF%E8%AF%B4&oq=%E6%9F%A5%E6%8B%89%E5%9B%BE%E6%96%AF%E7%89%B9%E6%8B%89%E5%A6%82%E6%98%AF%E8%AF%B4&aqs=chrome..69i57.1404j0j1&sourceid=chrome&ie=UTF-8 | Êü•ÊãâÂõæÊñØÁâπÊãâÂ¶ÇÊòØËØ¥ - Google Search
https://www.99csw.com/book/2744/83221.htm | Êü•ÊãâÊñØÂõæÊãâÂ¶ÇÊòØËØ¥ : Á¨¨‰∏ÄÈÉ® Â±±‰∏äÁöÑÊ†ë_Â∞ºÈáá_Âú®Á∫øÈòÖËØª_‰πù‰πùËóè‰π¶ÁΩë

https://www.bilibili.com/video/BV194411R7UF?spm_id_from=333.999.0.0&vd_source=302fb6f7a4f9835f875c33acdb0a1572 | „ÄêÊ∏∏ÊàèÈÄöÈâ¥Vol.10„Äë‰ªÄ‰πàËÉΩÊîπÂèò‰∏Ä‰∏™‰∫∫ÁöÑÊú¨Ë¥®ÔºüCRPGÂêç‰Ωú„ÄäÂºÇÂüüÈïáÈ≠ÇÊõ≤„ÄãÈÄöËßà_ÂìîÂì©ÂìîÂì©_bilibili
https://www.youtube.com/watch?v=NuTf6SZul20&list=PLzw_r3FRBpcOPmDhUkdf9YRE9PYpzaWz0&index=4 | (313) Let's Play Planescape Torment - 02 Mortuary Level 2 - YouTube

https://book.douban.com/series/11463 | Âç°Â∞îÁª¥ËØ∫ÁªèÂÖ∏
https://book.douban.com/subject/10555509/discussion/637203803/ | ËøôÊú¨‰π¶ÁÆóÊòØÊï£ÊñáÂêóÔºüËÇØÂÆö‰∏çÊòØÂ∞èËØ¥
https://book.douban.com/subject/10555550/ | ‰∏∫‰ªÄ‰πàËØªÁªèÂÖ∏ (Ë±ÜÁì£)
https://www.douban.com/search?q=%E7%BE%8E%E5%9B%BD%E8%AE%B2%E7%A8%BF | ÊêúÁ¥¢: ÁæéÂõΩËÆ≤Á®ø
https://book.douban.com/subject/10555538/ | ÁæéÂõΩËÆ≤Á®ø (Ë±ÜÁì£)
https://book.douban.com/review/9648748/ | „ÄäÁæéÂõΩËÆ≤Á®ø„ÄãÊèêÂà∞ÁöÑÊâÄÊúâ‰∫∫Âêç‰∏é‰π¶ÂêçÔºàÁæéÂõΩËÆ≤Á®øÔºâ‰π¶ËØÑ
https://zhuanlan.zhihu.com/p/159241206 | ÁæéÂõΩËÆ≤Á®øÁ¨îËÆ∞-Â∫èË®Ä - Áü•‰πé
https://www.wikiwand.com/en/Charles_Eliot_Norton_Lectures | Charles Eliot Norton Lectures - Wikiwand
https://www.zhihu.com/search?q=%E6%9C%AA%E6%9D%A5%E5%8D%83%E5%B9%B4%E6%96%87%E5%AD%A6%E5%A4%87%E5%BF%98%E5%BD%95&type=content | Êú™Êù•ÂçÉÂπ¥ÊñáÂ≠¶Â§áÂøòÂΩï - ÊêúÁ¥¢ÁªìÊûú - Áü•‰πé
http://www.ruanyifeng.com/calvino/nonfiction/cat-76/ | Êú™Êù•ÂçÉÂπ¥ÊñáÂ≠¶Â§áÂøòÂΩïÔºàËØ∫È°øËÆ≤Á®øÔºâ - Âç°Â∞îÁª¥ËØ∫‰∏≠ÊñáÁ´ô

https://aclanthology.org/2021.acl-long.346.pdf | Evaluation Examples are not Equally Informative How should that change NLP Leaderboards? - ACL-ACL| IJCNLP-2021_2021.acl-long.346
https://www.reddit.com/r/MachineLearning/comments/vbwe8k/d_yet_another_case_of_plagiarism_in_iccv_the_iccv/ | [D] Yet another case of plagiarism in ICCV. The ICCV 2021 paper "Learnable Boundary Guided Adversarial Training"(arxiv 2011.11164) with the BMVC 2020 paper "Adversarial Concurrent Training: Optimizing Robustness and Accuracy Trade-off of Deep Neural Networks" (arxiv 2008.07015) : MachineLearning
https://arxiv.org/abs/2008.07015 | Adversarial Concurrent Training Optimizing Robustness and Accuracy Trade-off of Deep Neural Networks - Arxiv-2008.07015
https://medium.com/@fahad.sarfraz/plagiarism-by-iccv-2021-paper-learnable-boundary-guided-adversarial-training-404d2ff5ed4e | Plagiarism by ICCV 2021 paper ‚ÄúLearnable Boundary Guided Adversarial Training‚Äù? | by Fahad Sarfraz | Jun, 2022 | Medium
https://arxiv.org/pdf/2008.07015.pdf | Adversarial Concurrent Training Optimizing Robustness and Accuracy Trade-off of Deep Neural Networks - Arxiv-2008.07015
https://arxiv.org/pdf/2011.11164.pdf | Learnable Boundary Guided Adversarial Training - Arxiv-2011.11164
https://aclanthology.org/2021.naacl-main.335.pdf | TaxoClass: Hierarchical Multi-Label Text Classification Using Only Class Names
https://github.com/benedekrozemberczki/pytorch_geometric_temporal | benedekrozemberczki/pytorch_geometric_temporal: PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models (CIKM 2021)
https://arxiv.org/pdf/2009.08205.pdf | Generating Label Cohesive and Well-Formed Adversarial Claims - Arxiv-2009.08205
https://arxiv.org/pdf/2106.00872.pdf | On the Efficacy of Adversarial Data Collection for Question Answering Results from a Large-Scale Randomized Study - Arxiv-2106.00872
https://arxiv.org/pdf/2110.08514.pdf | Analyzing Dynamic Adversarial Training Data in the Limit - Arxiv-2110.08514
https://www.google.com/search?q=generalized+inner+loop+meta-learning&oq=generalized+inner+loop+&aqs=chrome.0.0i512j69i57.2930j0j1&sourceid=chrome&ie=UTF-8 | generalized inner loop meta-learning - Google Search

https://arxiv.org/pdf/1707.06347.pdf | Proximal Policy Optimization Algorithms - Arxiv-1707.06347
https://huggingface.co/blog/constrained-beam-search | Guiding Text Generation with Constrained Beam Search in ü§ó Transformers

https://arxiv.org/pdf/1905.05301.pdf | Hierarchically Structured Meta-learning - Arxiv-1905.05301
https://book.douban.com/subject/35192665/ | Âπ≥Âéü‰∏äÁöÑÊë©Ë•ø (Ë±ÜÁì£)
https://book.douban.com/subject_search?search_text=%E5%8F%8C%E9%9B%AA%E6%B6%9B | ÂèåÈõ™Ê∂õ - ËØª‰π¶ - Ë±ÜÁì£ÊêúÁ¥¢
https://book.douban.com/subject/26881768/ | ÊàëÁöÑÊúãÂèãÂÆâÂæ∑ÁÉà (Ë±ÜÁì£)

https://philosophy.stackexchange.com/questions/90518/how-is-pure-intuition-possible-according-to-kant | philosophy of mathematics - How is 'Pure Intuition' possible according to Kant? - Philosophy Stack Exchange
https://www.reddit.com/r/askphilosophy/comments/5g95ue/kants_prolegomena_to_any_future_metaphysics_that/ | Kant's Prolegomena to Any Future Metaphysics That Will Be Able to Present Itself as a Science, ch. "Main transcendental problem 2: How is pure natural science possible?", Note III - What is he trying to say? : askphilosophy
https://www.wikiwand.com/en/Prolegomena_to_Any_Future_Metaphysics | Prolegomena to Any Future Metaphysics - Wikiwand
http://people.wku.edu/jan.garrett/303/kantprop.htm | Notes on Kant's Prolegomena
https://hume.ucdavis.edu/phi023/kantLEC.HTM | Lectures on Immanuel Kant

https://movie.douban.com/subject/1301021/ | Áõ≤ÂÖΩ (Ë±ÜÁì£)
https://www.zhihu.com/question/269926545 | ÂèîÊú¨ÂçéÂú®„Ää‰Ωú‰∏∫ÊÑèÂøóÂíåË°®Ë±°ÁöÑ‰∏ñÁïå„Äã‰∏Ä‰π¶‰∏≠ÁªèÂ∏∏Ë∞àÂà∞ÁöÑÂÖÖË∂≥ÁêÜÁî±ÂæãÊòØ‰ªÄ‰πà? - Áü•‰πé
https://book.douban.com/subject/35703665/ | ‰Ωú‰∏∫ÊÑèÊ¨≤ÂíåË°®Ë±°ÁöÑ‰∏ñÁïåÔºàÁ¨¨2Âç∑Ôºâ (Ë±ÜÁì£)
https://www.reddit.com/r/LearnJapanese/comments/98dxbv/what_does_this_subreddit_think_of_bunprojp/ | What does this subreddit think of bunpro.jp? : LearnJapanese

https://github.com/allegro/allRank | allegro/allRank: allRank is a framework for training learning-to-rank neural models based on PyTorch.
https://zhuanlan.zhihu.com/p/111636490 | Learning to RankÔºö pointwise „ÄÅ pairwise „ÄÅ listwise - Áü•‰πé
https://zhuanlan.zhihu.com/p/214242589 | SIGIR20|ÊúÄ‰Ω≥ËÆ∫ÊñáÔºöÈÄöÂæÄÂÖ¨Âπ≥„ÄÅÂÖ¨Ê≠£ÁöÑLearning to RankÔºÅ - Áü•‰πé
https://www.zhihu.com/question/389068269 | (24 Â∞ÅÁßÅ‰ø° / 81 Êù°Ê∂àÊÅØ) Learning To RankÁöÑpair wiseÊñπÊ≥ïÂ¶Ç‰ΩïÂæóÂà∞ÂÖ®Â±ÄÊéíÂ∫èÁªìÊûúÂë¢Ôºü - Áü•‰πé
https://zhuanlan.zhihu.com/p/64952093 | Learning to RankËØª‰π¶Á¨îËÆ∞--ÊéíÂ∫èËØÑ‰ª∑ÊåáÊ†á - Áü•‰πé
https://zhuanlan.zhihu.com/p/149310341 | „ÄêËêùÂçúÊó•ËÆ∞Á¨¨32Êúü„ÄëÂçÅÂπ¥Âêé‰ªçÊòØËôöÊûÑ‰ΩúÂìÅ‚ÄîÊú∫Âä®Ë≠¶ÂØüÂâßÂú∫Áâà&Áúü‰∫∫Áâà - Áü•‰πé
https://zhuanlan.zhihu.com/p/149309480 | „ÄêËêùÂçúÊó•ËÆ∞Á¨¨31Êúü„ÄëÊâÄÂà∞‰πãÂ§ÑÔºåÂØ∏Ëçâ‰∏çÁîü‚Äî‚ÄîÊú∫Âä®Ë≠¶ÂØüPATLABOR - Áü•‰πé
https://www.zhihu.com/question/305055900/answer/1177727168 | (24 Â∞ÅÁßÅ‰ø° / 81 Êù°Ê∂àÊÅØ) Â¶Ç‰ΩïËØÑ‰ª∑2020Âπ¥4Êúà23Êó•NetflixÂéüÂàõÂä®Áîª„ÄäÊîªÂ£≥Êú∫Âä®ÈòüSAC_2045„ÄãÔºü - Áü•‰πé

https://zhuanlan.zhihu.com/p/172121380 | Ê∑±ÂÖ•ÊµÖÂá∫YoloÁ≥ªÂàó‰πãYolov5Ê†∏ÂøÉÂü∫Á°ÄÁü•ËØÜÂÆåÊï¥ËÆ≤Ëß£ - Áü•‰πé
https://zhuanlan.zhihu.com/p/143747206 | Ê∑±ÂÖ•ÊµÖÂá∫YoloÁ≥ªÂàó‰πãYolov3&Yolov4&Yolov5&YoloxÊ†∏ÂøÉÂü∫Á°ÄÁü•ËØÜÂÆåÊï¥ËÆ≤Ëß£ - Áü•‰πé
https://docs.ultralytics.com/ | YOLOv5 Documentation
https://github.com/ultralytics/yolov5#pretrained-checkpoints | ultralytics/yolov5: YOLOv5 üöÄ in PyTorch > ONNX > CoreML > TFLite
https://zhuanlan.zhihu.com/p/94986199 | ÂÜôÁªôÂ∞èÁôΩÁöÑYOLO‰ªãÁªç - Áü•‰πé
https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab | Understanding SSD MultiBox ‚Äî Real-Time Object Detection In Deep Learning | by Eddie Forson | Towards Data Science
https://developers.arcgis.com/python/guide/how-ssd-works/ | How single-shot detector (SSD) works? | ArcGIS Developer
https://www.zhihu.com/search?q=ssd%20algorithm&type=content | ssd algorithm - ÊêúÁ¥¢ÁªìÊûú - Áü•‰πé
https://zhuanlan.zhihu.com/p/33544892 | ÁõÆÊ†áÊ£ÄÊµã|SSDÂéüÁêÜ‰∏éÂÆûÁé∞ - Áü•‰πé
https://arxiv.org/abs/1708.02002 | Focal Loss for Dense Object Detection - Arxiv-1708.02002
https://link.zhihu.com/?target=https%3A//www.cnblogs.com/xuanyuyt/p/7222867.html | Áü•‰πé - ÂÆâÂÖ®‰∏≠ÂøÉ
https://www.zhihu.com/search?q=ssd%20%E6%A3%80%E6%B5%8B&type=content | ssd Ê£ÄÊµã - ÊêúÁ¥¢ÁªìÊûú - Áü•‰πé
https://gullayeshwantkumarruler.medium.com/single-shot-detector-ssd-a299f437f6ef | Single Shot Detector (SSD). What is SSD? | by Yeshwant Kumar | Medium
https://blog.actorsfit.com/a?ID=00001-38d70c78-3466-4c1b-bbff-26805f5a4ea4 | Detailed SSD object detection algorithm - actorsfit
https://zhuanlan.zhihu.com/p/183261974 | ‰Ω†‰∏ÄÂÆö‰ªéÊú™ÁúãËøáÂ¶ÇÊ≠§ÈÄö‰øóÊòìÊáÇÁöÑYOLOÁ≥ªÂàó(‰ªév1Âà∞v5)Ê®°ÂûãËß£ËØª (‰∏ä) - Áü•‰πé
https://movie.douban.com/subject/3041806/ | Âè∂ÈóÆ (Ë±ÜÁì£)

https://www.google.com/search?q=%E4%B8%9C%E9%82%AA%E8%A5%BF%E6%AF%92&sourceid=chrome&ie=UTF-8 | ‰∏úÈÇ™Ë•øÊØí - Google Search

https://www.google.com/search?q=%E5%91%BC%E5%90%B8+%E7%89%B9%E5%BE%B7%E5%A7%9C&oq=%E5%91%BC%E5%90%B8te+de+jiang+%7C&aqs=chrome.1.69i57j0i333.8110j0j1&sourceid=chrome&ie=UTF-8 | ÂëºÂê∏ ÁâπÂæ∑Âßú - Google Search
https://book.douban.com/review/12163001/ | „ÄäÂëºÂê∏„ÄãÂêÑÁØáÁü≠ËØÑÔºàÂëºÂê∏Ôºâ‰π¶ËØÑ
https://www.google.com/search?q=learn2learn+kronecker&oq=learn2learn+kronecker&aqs=chrome..69i57j69i60.4601j0j1&sourceid=chrome&ie=UTF-8 | learn2learn kronecker - Google Search
https://joeddav.github.io/blog/2020/05/29/ZSL.html | Zero-Shot Learning in Modern NLP | Joe Davison Blog
https://www.google.com/search?q=%E7%AA%84%E9%97%A8&oq=%E7%AA%84%E9%97%A8&aqs=chrome..69i57j69i59l2.2012j0j1&sourceid=chrome&ie=UTF-8 | Á™ÑÈó® - Google Search

https://kexue.fm/tag/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/5/ | Ê†áÁ≠æ ÁîüÊàêÊ®°Âûã ‰∏ãÁöÑÊñáÁ´† - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://kexue.fm/archives/5776/comment-page-3#comments | ÁªÜÊ∞¥Èïøflow‰πãNICEÔºöÊµÅÊ®°ÂûãÁöÑÂü∫Êú¨Ê¶ÇÂøµ‰∏éÂÆûÁé∞ - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
http://www.cs.toronto.edu/~duvenaud/courses/csc2541/ | index

https://www.99csw.com/book/10305/371569.htm | ÂëºÂê∏ : ÁÑ¶ËôëÊòØËá™Áî±ÂºïËµ∑ÁöÑÁú©Êôï_ÁâπÂæ∑¬∑Âßú_Âú®Á∫øÈòÖËØª_‰πù‰πùËóè‰π¶ÁΩë
https://book.douban.com/review/13890377/ | ÂàÜËäÇÊÑüÊÉ≥Ôºå‰∏•ÈáçÂâßÈÄèÔºàÂëºÂê∏Ôºâ‰π¶ËØÑ
https://book.douban.com/review/12245641/ | ÁÑ¶ËôëÊòØËá™Áî±ÂºïËµ∑ÁöÑÁú©ÊôïÔºàÂëºÂê∏Ôºâ‰π¶ËØÑ
https://www.zhihu.com/people/ai-yu-16-30/posts | Liewschild - Áü•‰πé
https://zhuanlan.zhihu.com/p/263554045 | StyleGAN Âíå StyleGAN2 ÁöÑÊ∑±Â∫¶ÁêÜËß£ - Áü•‰πé
https://www.zhihu.com/people/kong-gu-91/posts | ÂúÜÂúÜË¶ÅÂ≠¶‰π† - Áü•‰πé

https://kexue.fm/archives/6549 | ‰ªéDCGANÂà∞SELF-MODÔºöGANÁöÑÊ®°ÂûãÊû∂ÊûÑÂèëÂ±ï‰∏ÄËßà - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://kexue.fm/archives/8757 | WGANÊñ∞ÊñπÊ°àÔºöÈÄöËøáÊ¢ØÂ∫¶ÂΩí‰∏ÄÂåñÊù•ÂÆûÁé∞LÁ∫¶Êùü - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://kexue.fm/archives/8244 | WGANÁöÑÊàêÂäüÔºåÂèØËÉΩË∑üWassersteinË∑ùÁ¶ªÊ≤°Âï•ÂÖ≥Á≥ª - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://kexue.fm/archives/7466 | Ê≥õÂåñÊÄß‰π±ÂºπÔºö‰ªéÈöèÊú∫Âô™Â£∞„ÄÅÊ¢ØÂ∫¶ÊÉ©ÁΩöÂà∞ËôöÊãüÂØπÊäóËÆ≠ÁªÉ - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://kexue.fm/archives/7234 | ÂØπÊäóËÆ≠ÁªÉÊµÖË∞àÔºöÊÑè‰πâ„ÄÅÊñπÊ≥ïÂíåÊÄùËÄÉÔºàÈôÑKerasÂÆûÁé∞Ôºâ - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://kexue.fm/archives/7105 | Á∫ßËÅîÊäëÂà∂ÔºöÊèêÂçáGANË°®Áé∞ÁöÑ‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑÊñπÊ≥ï - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://kexue.fm/archives/6139 | WGAN-divÔºö‰∏Ä‰∏™ÈªòÈªòÊó†ÈóªÁöÑWGANÂ°´ÂùëËÄÖ - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://kexue.fm/archives/7210 | Designing GANsÔºöÂèà‰∏Ä‰∏™GANÁîü‰∫ßËΩ¶Èó¥ - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://kexue.fm/archives/4439 | ‰∫íÊÄºÁöÑËâ∫ÊúØÔºö‰ªéÈõ∂Áõ¥ËææWGAN-GP - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://kexue.fm/archives/6016 | f-GANÁÆÄ‰ªãÔºöGANÊ®°ÂûãÁöÑÁîü‰∫ßËΩ¶Èó¥ - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://kexue.fm/archives/6214 | BiGAN-QPÔºöÁÆÄÂçïÊ∏ÖÊô∞ÁöÑÁºñÁ†Å&ÁîüÊàêÊ®°Âûã - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://kexue.fm/archives/6110 | RSGANÔºöÂØπÊäóÊ®°Âûã‰∏≠ÁöÑ‚ÄúÂõæÁÅµÊµãËØï‚ÄùÊÄùÊÉ≥ - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://www.zhihu.com/search?q=Snorkel&type=content | Snorkel - ÊêúÁ¥¢ÁªìÊûú - Áü•‰πé

https://www.wikiwand.com/en/Hungarian_algorithm | Hungarian algorithm - Wikiwand
https://www.zhihu.com/people/marisa.moe | ÂúÜËßíÈ™ëÂ£´È≠îÁêÜÊ≤ô - Áü•‰πé
https://github.com/pyg-team/pytorch_geometric/issues/1365 | What is the relationship between DGL and PyG? ¬∑ Issue #1365 ¬∑ pyg-team/pytorch_geometric
https://posts.careerengine.us/p/6196f6ecae2a98248ad5e6c0 | ÂõæÁ•ûÁªèÁΩëÁªúÊ°ÜÊû∂-PyTorch GeometricÔºàPyGÔºâÁöÑ‰ΩøÁî®ÂèäË∏©Âùë
https://www.spaces.ac.cn/archives/6620 | ÂáΩÊï∞ÂÖâÊªëÂåñÊùÇË∞àÔºö‰∏çÂèØÂØºÂáΩÊï∞ÁöÑÂèØÂØºÈÄºËøë - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces

https://zhuanlan.zhihu.com/p/239929601 | [Meta-Learning]ÂØπReptileÁöÑÊ∑±Â∫¶Ëß£Êûê - Áü•‰πé
https://github.com/bamos/HowToTrainYourMAMLPytorch | bamos/HowToTrainYourMAMLPytorch: The original code for the paper "How to train your MAML" along with a replication of the original "Model Agnostic Meta Learning" (MAML) paper in Pytorch.
https://github.com/facebookresearch/higher/issues/56 | Meaning of stop-gradient ¬∑ Issue #56 ¬∑ facebookresearch/higher
https://github.com/google-research/sam | google-research/sam
http://learn2learn.net/tutorials/anil_tutorial/ANIL_tutorial/ | Feature Reuse with ANIL - learn2learn
http://learn2learn.net/docs/learn2learn.nn/ | learn2learn.nn - learn2learn
https://arxiv.org/pdf/1910.13603.pdf | When MAML Can Adapt Fast and How to Assist When It Cannot | PDF
https://github.com/Sha-Lab/kfo | Sha-Lab/kfo: Code release for "When MAML Can Adapt Fast and How to Assist When It Cannot", AISTATS 2021.
http://seba1511.net/posters/ | Posters
https://www.google.com/search?q=maml+%2B%2B&oq=maml+%2B%2B&aqs=chrome..69i57j69i65j69i61.1197j0j1&sourceid=chrome&ie=UTF-8 | maml ++ - Google Search
https://arxiv.org/abs/1810.09502 | How to train your MAML | Abstract

https://xingyuzhou.org/blog/notes/strong-convexity#mjx-eqn-eqdef | Strong convexity ¬∑ Xingyu Zhou's blog
https://www.zhihu.com/question/22426561 | Â¶Ç‰ΩïÁêÜËß£Bregman divergenceÔºü - Áü•‰πé
http://mark.reid.name/blog/meet-the-bregman-divergences.html | Meet the Bregman Divergences ‚Üê Inductio Ex Machina ‚Üê Mark Reid
https://www2.cs.uic.edu/~zhangx/teaching/bregman.pdf | bregman.pdf
http://users.cecs.anu.edu.au/~xzhang/teaching/bregman.pdf | bregman.pdf

https://arxiv.org/pdf/1410.8516.pdf | NICE: Non-linear Independent Components Estimation | PDF
https://github.com/bojone/flow/blob/master/nice.py | flow/nice.py at master ¬∑ bojone/flow
https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650745032&idx=1&sn=a889433dd4c4d9f62bfab347909d9d28&chksm=871aecb6b06d65a02625abdf4b21a2116251e311a49508db587b76ae8f76d7a9e03d4a6ab80a&scene=27#wechat_redirect | ‰∏ã‰∏Ä‰∏™GANÔºüOpenAIÊèêÂá∫ÂèØÈÄÜÁîüÊàêÊ®°ÂûãGlow
https://github.com/paultsw/nice_pytorch | paultsw/nice_pytorch: Nonlinear Independent Components Estimation (Dinh et al, 2014) in PyTorch.
https://opensourcelibs.com/lib/pytorch-nice | Pytorch Nice - Implementation of non-linear independent components estimation (NICE) in pytorch - (pytorch-nice)
https://spaces.ac.cn/archives/5776/comment-page-1#comments | ÁªÜÊ∞¥Èïøflow‰πãNICEÔºöÊµÅÊ®°ÂûãÁöÑÂü∫Êú¨Ê¶ÇÂøµ‰∏éÂÆûÁé∞ - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://github.com/fmu2/NICE | fmu2/NICE: PyTorch implementation of NICE
https://github.com/DakshIdnani/pytorch-nice | DakshIdnani/pytorch-nice: Implementation of non-linear independent components estimation (NICE) in pytorch
https://kexue.fm/archives/5807 | ÁªÜÊ∞¥Èïøflow‰πãRealNVP‰∏éGlowÔºöÊµÅÊ®°ÂûãÁöÑ‰º†Êâø‰∏éÂçáÂçé - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://kexue.fm/tag/flow/ | Ê†áÁ≠æ flow ‰∏ãÁöÑÊñáÁ´† - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://www.zhihu.com/question/444045409 | Â¶Ç‰ΩïËØÑ‰ª∑ÁéãÂõΩ‰πãÂøÉÁ≥ªÂàó‰ªäÂ§©ÂÆ£Â∏ÉÁôªÂΩïEPICÔºü - Áü•‰πé
https://www.91m.cc/vodplay/15679-3-11.html | Âú®Á∫øÊí≠ÊîæÂ∑¥ÂÖã¬∑‰∫öÁΩó_BACK ARROWÁ¨¨11ÈõÜ-ÊâãÊú∫È´òÊ∏ÖÂÖçË¥πÊµÅÁïÖËßÇÁúã-Ê®±Ëä±Âä®Êº´

https://spaces.ac.cn/archives/7180/comment-page-1 | ‰ªéÂá†‰ΩïËßÜËßíÊù•ÁêÜËß£Ê®°ÂûãÂèÇÊï∞ÁöÑÂàùÂßãÂåñÁ≠ñÁï• - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/8620 | ÊµÖË∞àTransformerÁöÑÂàùÂßãÂåñ„ÄÅÂèÇÊï∞Âåñ‰∏éÊ†áÂáÜÂåñ - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/8069 | ‰Ω†ÂèØËÉΩ‰∏çÈúÄË¶ÅBERT-flowÔºö‰∏Ä‰∏™Á∫øÊÄßÂèòÊç¢Â™≤ÁæéBERT-flow - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/7681 | L2Ê≠£ÂàôÊ≤°ÊúâÊÉ≥Ë±°ÈÇ£‰πàÂ•ΩÔºüÂèØËÉΩÊòØ‚ÄúÊùÉÈáçÂ∞∫Â∫¶ÂÅèÁßª‚ÄùÊÉπÁöÑÁ•∏ - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/category/Mathematics/2/ | ÂàÜÁ±ª Êï∞Â≠¶Á†îÁ©∂ ‰∏ãÁöÑÊñáÁ´† - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://spaces.ac.cn/archives/8444 | Êàë‰ª¨ÂèØ‰ª•Êó†ÊçüÊîæÂ§ß‰∏Ä‰∏™TransformerÊ®°ÂûãÂêóÔºà‰∏ÄÔºâ - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces

https://berkeleytime.com/catalog | Berkeleytime
https://edusalsa.com/ | Edusalsa - Discover Your Stanford

https://www.99csw.com/book/1166/33767.htm | Á™ÑÈó® : Á¨¨‰∫åÁ´†_ÂÆâÂæ∑ÁÉà¬∑Á∫™Âæ∑_Âú®Á∫øÈòÖËØª_‰πù‰πùËóè‰π¶ÁΩë
https://www.zhihu.com/question/28961306 | Â¶Ç‰ΩïËØÑ‰ª∑ÁéãÂ∞èÊ≥¢„ÄäÁªøÊØõÊ∞¥ÊÄ™„ÄãËøôÈÉ®‰ΩúÂìÅ? - Áü•‰πé
https://www.google.com/search?q=%E5%9C%B0%E4%B9%85%E5%A4%A9%E9%95%BF+%E7%8E%8B%E5%B0%8F%E6%B3%A2&newwindow=1&sxsrf=AOaemvLKO6u5iqifyQu5bh3oX-R_CRls-g%3A1635914834852&ei=UhSCYZa8M9zI0PEPz4mpsAk&oq=%E5%9C%B0%E4%B9%85%E5%A4%A9%E9%95%BF+%E7%8E%8B%E5%B0%8F%E6%B3%A2&gs_lcp=Cgdnd3Mtd2l6EAM6BwgAEEcQsAM6BQgAEIAEOgUILhCABDoHCAAQgAQQDDoFCAAQzQJKBAhBGABQoAlY-BlgtxpoAXACeACAAcUDiAGmFpIBCTAuMi41LjMuMZgBAKABAcgBCMABAQ&sclient=gws-wiz&ved=0ahUKEwiWrfTUsfvzAhVcJDQIHc9ECpYQ4dUDCA4&uact=5 | Âú∞‰πÖÂ§©Èïø ÁéãÂ∞èÊ≥¢ - Google Search
https://www.99csw.com/book/1970/index.htm | Âú∞‰πÖÂ§©Èïø_ÁéãÂ∞èÊ≥¢_Âú®Á∫øÈòÖËØª_‰πù‰πùËóè‰π¶ÁΩë

https://lilianweng.github.io/lil-log/2020/01/29/curriculum-for-reinforcement-learning.html | Curriculum for Reinforcement Learning

https://perso.telecom-paristech.fr/tchamker/SI221/2020f/ | Index of /tchamker/SI221/2020f
https://www.manhuacat.com/manga/5565/464372.html | ÊîæÂ≠¶ÂêéÂ§±Áú†ÁöÑ‰Ω†Êº´ÁîªÁ¨¨20ËØùÂú®Á∫øÈòÖËØª-„Ç™„Ç∏„É≠„Éû„Ç≥„Éà - Êº´ÁîªÁå´
https://www.zhihu.com/question/309212728 | Â¶Ç‰ΩïËØÑ‰ª∑„ÄäÊàëÊÉ≥ÂêÉÊéâ‰Ω†ÁöÑËÉ∞ËÑè„ÄãÁöÑÂéü‰ΩúÂ∞èËØ¥‰ª•ÂèäË°çÁîü‰ΩúÂìÅÔºàÊº´Áîª„ÄÅÁúü‰∫∫ÁîµÂΩ±„ÄÅÂä®ÁîªÁîµÂΩ±ÔºâÔºü - Áü•‰πé
https://zhuanlan.zhihu.com/p/56166459 | „ÄêËΩªÂêêÊßΩ„ÄëÂõûÂ∫îÂÖàÁü•ÔºåÂÖ≥‰∫é„ÄäËÉ∞ËÑè„Äã‰∏≠ÁöÑÊ†°Âõ≠Èò∂Á∫ßÈÇ£‰∫õ‰∫ã - Áü•‰πé
https://zhuanlan.zhihu.com/p/55581506 | ÂØπ„ÄäÊàëÊÉ≥ÂêÉÊéâ‰Ω†ÁöÑËÉ∞ËÑè„ÄãÁöÑÁ∫ØÁ≤πÈùûÁêÜÊÄßÊâπÂà§ - Áü•‰πé

https://spaces.ac.cn/archives/6409/comment-page-1 | O-GANÔºöÁÆÄÂçï‰øÆÊîπÔºåËÆ©GANÁöÑÂà§Âà´Âô®ÂèòÊàê‰∏Ä‰∏™ÁºñÁ†ÅÂô®ÔºÅ - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://github.com/eriklindernoren/PyTorch-GAN | eriklindernoren/PyTorch-GAN: PyTorch implementations of Generative Adversarial Networks.
https://spaces.ac.cn/archives/6280/comment-page-1#mjx-eqn-eq%3Astrong-dual | ‰ªéWassersteinË∑ùÁ¶ª„ÄÅÂØπÂÅ∂ÁêÜËÆ∫Âà∞WGAN - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces
https://vincentherrmann.github.io/blog/wasserstein/ | Wasserstein GAN and the Kantorovich-Rubinstein Duality - Vincent Herrmann
https://spaces.ac.cn/archives/6051 | Ê∑±Â∫¶Â≠¶‰π†‰∏≠ÁöÑLipschitzÁ∫¶ÊùüÔºöÊ≥õÂåñ‰∏éÁîüÊàêÊ®°Âûã - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces

https://www.zhihu.com/column/c_171450570 | ÁîüÊàêÂØπÊäóÁΩëÁªú - Áü•‰πé
https://zhuanlan.zhihu.com/p/34635690 | ÁîüÊàêÂØπÊäóÁΩëÁªúÁ≥ªÂàó(2)‚Äî‚ÄîGANÊèêÈ´ò - Áü•‰πé
https://github.com/nocotan/pytorch-lightning-gans | nocotan/pytorch-lightning-gans: Collection of PyTorch Lightning implementations of Generative Adversarial Network varieties presented in research papers.

https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#noise-contrastive-estimation-nce | Learning Word Embedding

https://blog.csdn.net/a1015553840/article/details/51043019 | Ê©üÂô®Â≠∏ÁøíÂü∫Áü≥(Machine Learning Foundations) Êú∫Âô®Â≠¶‰π†Âü∫Áü≥ ‰Ωú‰∏ö‰∫å ËØæÂêé‰π†È¢òËß£Á≠î_Mac JiangÁöÑÂçöÂÆ¢-CSDNÂçöÂÆ¢
https://github.com/Doraemonzzz/ML-Foundation-and-ML-Techniques | Doraemonzzz/ML-Foundation-and-ML-Techniques: Âè∞Â§ßÊú∫Âô®Â≠¶‰π†ËØæÁ®ã‰Ωú‰∏öËØ¶Ëß£
https://github.com/ppaquay/Learning-from-Data-Solutions/blob/master/Problems_Chap2.pdf | Learning-from-Data-Solutions/Problems_Chap2.pdf at master ¬∑ ppaquay/Learning-from-Data-Solutions
https://github.com/Doraemonzzz/Learning-from-data/blob/master/Chapter2/Chapter%202%20Training%20versus%20Testing.pdf | Learning-from-data/Chapter 2 Training versus Testing.pdf at master ¬∑ Doraemonzzz/Learning-from-data
https://github.com/niuers/Learning-From-Data-A-Short-Course/blob/master/Solutions%20to%20Chapter%202%20Training%20versus%20Testing.ipynb | Learning-From-Data-A-Short-Course/Solutions to Chapter 2 Training versus Testing.ipynb at master ¬∑ niuers/Learning-From-Data-A-Short-Course

https://www.google.com/search?q=%E6%8B%BE%E5%8F%88%E4%B9%8B%E5%9B%BD&oq=%E6%8B%BE%E5%8F%88%E4%B9%8B%E5%9B%BD&aqs=chrome..69i57&sourceid=chrome&ie=UTF-8 | ÊãæÂèà‰πãÂõΩ - Google Search
https://www.google.com/search?q=%E6%97%A5%E6%9C%88%E5%90%8C%E9%94%99&oq=%E6%97%A5%E6%9C%88%E5%90%8C%E9%94%99&aqs=chrome..69i57&sourceid=chrome&ie=UTF-8 | Êó•ÊúàÂêåÈîô - Google Search
https://www.zhihu.com/search?q=%E9%95%BF%E5%AE%89%E7%9D%A3%E6%AD%A6%E5%8F%B8&type=content | ÈïøÂÆâÁù£Ê≠¶Âè∏ - ÊêúÁ¥¢ÁªìÊûú - Áü•‰πé

https://www.zhihu.com/search?q=%E5%AD%98%E5%9C%A8%E4%B8%8E%E8%99%9A%E6%97%A0&type=content | Â≠òÂú®‰∏éËôöÊó† - ÊêúÁ¥¢ÁªìÊûú - Áü•‰πé
https://book.douban.com/subject/25939476/ | Â≠òÂú®‰∏éÊó∂Èó¥ (Ë±ÜÁì£)

https://www.zxzj.me/video/775-1-4.html | „ÄäÁÅ´Ëä±„ÄãÁ¨¨4ÈõÜÂú®Á∫øËßÇÁúã- Âú®Á∫ø‰πãÂÆ∂
https://www.kanunu8.com/book3/6631/116245.html | Ëµ°ÂÖª‰∫∫Á±ª_ÂàòÊÖàÊ¨£‰∏≠Áü≠ÁØáÁßëÂπª‰ΩúÂìÅ_ÂàòÊÖàÊ¨£ Â∞èËØ¥Âú®Á∫øÈòÖËØª
https://search.cn-ki.net/search?keyword=%E8%BF%9C%E5%B1%B1%E6%B7%A1%E5%BD%B1&db=CDMD | ËøúÂ±±Ê∑°ÂΩ±-iDataÁü•ËØÜÊêúÁ¥¢

https://project.hupili.net/tutorial/hu2012-matrix-calculus/hu2012matrix-calculus.pdf | hu2012matrix-calculus.pdf
https://cdn-uploads.piazza.com/paste/kstd6rn7gfj2xa/d141b3471443145c7413a7dd8ddea576fd8c0710a292de74c429364ed60ac129/Vector_and_Matrix_Derivatives-1.pdf | Vector_and_Matrix_Derivatives-1.pdf
https://www.bilibili.com/video/BV1F54y197GW?p=2 | „Äê‰ªä Êïè„ÄëÂ¶ÑÊÉ≥‰ª£ÁêÜ‰∫∫Âà∞Â∫ïËÆ≤‰∫Ü‰ªÄ‰πà | ‰∏™‰∫∫Ëß£Êûê | ÂêàÈõÜ_ÂìîÂì©ÂìîÂì©_bilibili

https://wzyboy.im/post/1317.html | ‰ΩøÁî® Beancount ËÆ∞ÂΩïËØÅÂà∏ÊäïËµÑ | wzyboy‚Äôs blog
https://www.zhihu.com/question/265457580 | Â¶Ç‰ΩïËØÑ‰ª∑Êº´Áîª„ÄäÂ∞ëÂ•≥ÁªàÊú´ÊóÖË°å„Äã42ËØùÔºü - Áü•‰πé

https://www.zhihu.com/column/c_1387802014982762496 | ËßâÈÜíÂπ¥‰ª£‰∏≠ÁöÑÂ•ΩÊñáÁ´†Êï¥ÁêÜ - Áü•‰πé
https://www.doc88.com/p-0062457643161.html?r=1 | Â•áÊñáËµèÊûê Ê∞ë‰∏ªËá≥‰∏ä‚Äî‚ÄîËØÑÈôàÁã¨ÁßÄÁöÑ„ÄäÁà±ÂõΩÂøÉ‰∏éËá™ËßâÂøÉ„Äã - ÈÅìÂÆ¢Â∑¥Â∑¥
https://www.zhihu.com/search?q=%E4%B8%AD%E5%9B%BD%E8%BF%91%E4%BB%A3%E5%8F%B2%20%E8%92%8B%E5%BB%B7%E9%BB%BB&type=content | ‰∏≠ÂõΩËøë‰ª£Âè≤ ËíãÂª∑Èªª - ÊêúÁ¥¢ÁªìÊûú - Áü•‰πé

https://socket.io/get-started/ | Get started | Socket.IO
https://www.zhihu.com/question/22198714 | Â¶Ç‰ΩïËß£ËØªËä•Â∑ùÈæô‰πã‰ªãÁöÑ„ÄäÂú∞Áã±Âèò„Äã‰∏ÄÊñáÔºü - Áü•‰πé

https://www.reddit.com/r/reinforcementlearning/comments/a4qwva/metalearning_learning_to_learn_fast_lilian_weng/ | "Meta-Learning: Learning to Learn Fast", Lilian Weng [metric learning, MANN & meta networks, MAML/REPTILE] : reinforcementlearning
https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html | Meta Reinforcement Learning
https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html | Meta-Learning: Learning to Learn Fast

https://zhuanlan.zhihu.com/p/149725307 | „ÄêËêùÂçúÊó•ËÆ∞Á¨¨38Êúü„ÄëFollow me,follow you‚Äî‚ÄîËãçÁ©πÁöÑÊ≥ïËäôÂ®úEXODUS - Áü•‰πé
https://zhuanlan.zhihu.com/p/149724367 | „ÄêËêùÂçúÊó•ËÆ∞Á¨¨37Êúü„ÄëÊàëÂ≠òÂú®‰∫éÊ≠§‚Äî‚ÄîËãçÁ©πÁöÑÊ≥ïËäôÂ®úÔºàOVA&ÂâßÂú∫ÁâàÔºâ - Áü•‰πé

https://zhuanlan.zhihu.com/p/110955275 | ÊùéËà™ÁªüËÆ°Â≠¶‰π†ÊñπÊ≥ïÔºàÁ¨¨ÂõõÁ´†Ôºâ - Áü•‰πé
https://zhuanlan.zhihu.com/p/149890569 | "Linformer" Êãç‰∫ÜÊãç "Ë¢´ÂêäÊâìÁöÑTransformers ÂêéÊµ™‰ª¨" - Áü•‰πé
https://www.zhihu.com/question/349958732/answer/945349902 | ÊúâÂì™‰∫õ‰ª§‰Ω†Âç∞Ë±°Ê∑±ÂàªÁöÑÈ≠îÊîπtransformerÔºü - Áü•‰πé
https://zhuanlan.zhihu.com/p/259765593 | È´òÊïàTransformerÂ±ÇÂá∫‰∏çÁ©∑ÔºåË∞∑Ê≠åÂõ¢ÈòüÁªºËø∞ÊñáÁ´†‰∏ÄÁΩëÊâìÂ∞Ω - Áü•‰πé
https://www.zhihu.com/question/319339652 | transformer‰∏≠‰∏∫‰ªÄ‰πà‰ΩøÁî®‰∏çÂêåÁöÑK Âíå QÔºå ‰∏∫‰ªÄ‰πà‰∏çËÉΩ‰ΩøÁî®Âêå‰∏Ä‰∏™ÂÄºÔºü - Áü•‰πé
https://zhuanlan.zhihu.com/p/223430086 | Longformer: Â±ÄÈÉ®AttentionÂíåÂÖ®Â±ÄattentionÁöÑÊ∑∑Êê≠ - Áü•‰πé
https://zhuanlan.zhihu.com/p/208134502 | Reformer: ÊêûÁ¨ëÔºàÈ´òÊïàÔºâÁöÑtransformerÁªìÊûÑ(2020Âπ¥2ÊúàGoogle) - Áü•‰πé
https://zhuanlan.zhihu.com/c_1213397558586257408 | ÊùéËà™ÁªüËÆ°Â≠¶‰π†ÊñπÊ≥ï - Áü•‰πé

https://zhuanlan.zhihu.com/p/107944440 | ÊùéËà™ÁªüËÆ°Â≠¶‰π†ÊñπÊ≥ïÔºàÁ¨¨‰∏ÄÁ´†Ôºâ - Áü•‰πé
https://www.zhihu.com/column/c_1213397558586257408 | ÊùéËà™ÁªüËÆ°Â≠¶‰π†ÊñπÊ≥ï - Áü•‰πé
https://zhuanlan.zhihu.com/p/114284754 | ÊùéËà™ÁªüËÆ°Â≠¶‰π†ÊñπÊ≥ïÔºàÁ¨¨ÂÖ≠Á´†Ôºâ - Áü•‰πé
https://zhuanlan.zhihu.com/p/115631923 | Âπø‰πâÁ∫øÊÄßÊ®°ÂûãÔºàÁ¨¨ÂÖ≠Á´†Ë°•ÂÖÖÔºâ - Áü•‰πé
